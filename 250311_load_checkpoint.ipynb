{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msclap\n",
      "  Downloading msclap-1.3.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: librosa<0.11.0,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from msclap) (0.10.2.post1)\n",
      "Collecting numba<0.59.0,>=0.58.0 (from msclap)\n",
      "  Downloading numba-0.58.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.0 (from msclap)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from msclap) (2.2.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from msclap) (6.0.2)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from msclap) (1.6.1)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from msclap) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchaudio<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from msclap) (2.6.0+cu124)\n",
      "Collecting torchlibrosa<0.2.0,>=0.1.0 (from msclap)\n",
      "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting torchvision<0.17.0,>=0.16.0 (from msclap)\n",
      "  Downloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from msclap) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.11/dist-packages (from msclap) (4.48.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (1.14.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa<0.11.0,>=0.10.1->msclap) (1.1.0)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba<0.59.0,>=0.58.0->msclap)\n",
      "  Downloading llvmlite-0.41.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.0->msclap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.0->msclap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.0->msclap) (2025.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.3.1->msclap) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (3.17.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.1.0->msclap) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.1.0->msclap) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision<0.17.0,>=0.16.0->msclap) (2.32.3)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision<0.17.0,>=0.16.0 (from msclap)\n",
      "  Downloading torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting triton==3.1.0 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.5.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.0->msclap) (12.5.82)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.3.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting triton==2.3.1 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.3.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting triton==2.3.0 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==2.2.0 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torchaudio<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torchaudio-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting torch<3.0.0,>=2.1.0 (from msclap)\n",
      "  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting triton==2.1.0 (from torch<3.0.0,>=2.1.0->msclap)\n",
      "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision<0.17.0,>=0.16.0->msclap) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->msclap) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->msclap) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->msclap) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->msclap) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->msclap) (0.5.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa<0.11.0,>=0.10.1->msclap) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.0.0->msclap) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision<0.17.0,>=0.16.0->msclap) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision<0.17.0,>=0.16.0->msclap) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision<0.17.0,>=0.16.0->msclap) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision<0.17.0,>=0.16.0->msclap) (2025.1.31)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa<0.11.0,>=0.10.1->msclap) (1.17.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.1.0->msclap) (3.0.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa<0.11.0,>=0.10.1->msclap) (2.22)\n",
      "Downloading msclap-1.3.3-py3-none-any.whl (31 kB)\n",
      "Downloading numba-0.58.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.1.2-cp311-cp311-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading llvmlite-0.41.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, llvmlite, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nvidia-cusolver-cu12, torch, torchvision, torchlibrosa, torchaudio, msclap\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.43.0\n",
      "    Uninstalling llvmlite-0.43.0:\n",
      "      Successfully uninstalled llvmlite-0.43.0\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.60.0\n",
      "    Uninstalling numba-0.60.0:\n",
      "      Successfully uninstalled numba-0.60.0\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba-cuda 0.2.0 requires numba>=0.59.1, but you have numba 0.58.1 which is incompatible.\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.58.1 which is incompatible.\n",
      "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.58.1 which is incompatible.\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.58.1 which is incompatible.\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.58.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed llvmlite-0.41.1 msclap-1.3.3 numba-0.58.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 torchaudio-2.1.2 torchlibrosa-0.1.0 torchvision-0.16.2 triton-2.1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "9a7b663a765441619ae722bbe11500e2",
       "pip_warning": {
        "packages": [
         "nvidia",
         "torch",
         "torchgen",
         "torchvision",
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install msclap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from msclap import CLAP\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0054e+03,  3.8947e+00, -8.1710e+00,  1.5679e+01,  2.4611e+01,\n",
       "         -8.3082e+00,  2.1421e+00, -5.8280e+01, -1.4073e+01, -1.8947e+01,\n",
       "          2.4188e+01,  1.5536e+00,  1.4976e+01, -1.8848e+01, -3.4659e+00,\n",
       "         -1.0776e+01],\n",
       "        [ 3.8947e+00,  1.0112e+03, -2.1039e+01, -1.9784e+01,  9.7389e+00,\n",
       "          7.0783e+00,  2.8886e+01,  3.7974e+01,  2.9314e+01, -1.1577e+00,\n",
       "          5.7733e+01, -1.5454e+01, -1.3698e+01, -6.2852e+00,  6.0602e+01,\n",
       "         -3.2429e+01],\n",
       "        [-8.1710e+00, -2.1039e+01,  9.8494e+02,  3.5804e+01, -5.2367e+01,\n",
       "         -1.4041e+01, -3.7153e+01, -3.0501e+01,  1.4157e+01, -4.3488e+01,\n",
       "         -2.3286e+01, -6.4315e+00, -2.7586e+00,  9.3741e+00,  5.4416e-01,\n",
       "          4.0171e-01],\n",
       "        [ 1.5679e+01, -1.9784e+01,  3.5804e+01,  1.0110e+03,  4.1728e+01,\n",
       "         -5.1955e+01,  3.9012e+01,  3.0137e+01, -3.3048e+00,  2.5583e+01,\n",
       "         -9.9429e+00, -6.0531e+01,  2.9718e+01,  5.8247e+01, -2.7040e+01,\n",
       "          1.8320e+01],\n",
       "        [ 2.4611e+01,  9.7389e+00, -5.2367e+01,  4.1728e+01,  9.9578e+02,\n",
       "          1.2397e+01, -4.6030e+01,  1.2342e+01, -1.2439e+01, -3.0825e+01,\n",
       "         -5.0356e+00, -4.8524e+01, -3.3711e+01, -2.4483e+01, -6.3853e+00,\n",
       "         -3.3572e+01],\n",
       "        [-8.3082e+00,  7.0783e+00, -1.4041e+01, -5.1955e+01,  1.2397e+01,\n",
       "          9.9768e+02, -4.4754e+01, -1.8677e+01,  7.1270e+01, -2.0772e+01,\n",
       "         -2.0722e+01,  3.8043e+01, -1.1846e+00,  2.0507e+01,  4.5544e+01,\n",
       "         -8.1497e+00],\n",
       "        [ 2.1421e+00,  2.8886e+01, -3.7153e+01,  3.9012e+01, -4.6030e+01,\n",
       "         -4.4754e+01,  9.8039e+02, -3.5983e+00,  2.5260e+01,  8.0801e+00,\n",
       "         -8.3126e+00, -9.0908e+01,  1.4315e+01,  4.1645e+01, -1.9696e+00,\n",
       "         -2.8001e+00],\n",
       "        [-5.8280e+01,  3.7974e+01, -3.0501e+01,  3.0137e+01,  1.2342e+01,\n",
       "         -1.8677e+01, -3.5983e+00,  1.0199e+03,  3.9667e+01,  1.8510e+01,\n",
       "          1.6187e+01, -4.0630e+01, -2.3760e+01, -4.4399e+01, -3.0931e+00,\n",
       "          6.2528e+00],\n",
       "        [-1.4073e+01,  2.9314e+01,  1.4157e+01, -3.3048e+00, -1.2439e+01,\n",
       "          7.1270e+01,  2.5260e+01,  3.9667e+01,  1.0463e+03, -2.5668e+01,\n",
       "          3.6241e+01, -3.8751e+01,  4.6189e+01, -1.6996e+01, -7.5677e+00,\n",
       "         -3.8146e+00],\n",
       "        [-1.8947e+01, -1.1577e+00, -4.3488e+01,  2.5583e+01, -3.0825e+01,\n",
       "         -2.0772e+01,  8.0801e+00,  1.8510e+01, -2.5668e+01,  9.0252e+02,\n",
       "         -1.2810e+01,  2.2525e+01, -2.4284e+01, -3.4358e+01,  3.9017e+01,\n",
       "         -1.9244e+01],\n",
       "        [ 2.4188e+01,  5.7733e+01, -2.3286e+01, -9.9429e+00, -5.0356e+00,\n",
       "         -2.0722e+01, -8.3126e+00,  1.6187e+01,  3.6241e+01, -1.2810e+01,\n",
       "          9.9124e+02,  3.6161e+01,  4.8090e+01, -1.3308e+01, -1.2949e+01,\n",
       "         -3.8623e+01],\n",
       "        [ 1.5536e+00, -1.5454e+01, -6.4315e+00, -6.0531e+01, -4.8524e+01,\n",
       "          3.8043e+01, -9.0908e+01, -4.0630e+01, -3.8751e+01,  2.2525e+01,\n",
       "          3.6161e+01,  1.0354e+03, -1.6041e+01, -1.9598e+01,  5.3812e+00,\n",
       "          1.7763e+01],\n",
       "        [ 1.4976e+01, -1.3698e+01, -2.7586e+00,  2.9718e+01, -3.3711e+01,\n",
       "         -1.1846e+00,  1.4315e+01, -2.3760e+01,  4.6189e+01, -2.4284e+01,\n",
       "          4.8090e+01, -1.6041e+01,  1.0364e+03,  8.0649e+00, -1.2023e+00,\n",
       "         -6.7973e+01],\n",
       "        [-1.8848e+01, -6.2852e+00,  9.3741e+00,  5.8247e+01, -2.4483e+01,\n",
       "          2.0507e+01,  4.1645e+01, -4.4399e+01, -1.6996e+01, -3.4358e+01,\n",
       "         -1.3308e+01, -1.9598e+01,  8.0649e+00,  9.8820e+02, -1.0903e+01,\n",
       "         -8.5208e+01],\n",
       "        [-3.4659e+00,  6.0602e+01,  5.4416e-01, -2.7040e+01, -6.3853e+00,\n",
       "          4.5544e+01, -1.9696e+00, -3.0931e+00, -7.5677e+00,  3.9017e+01,\n",
       "         -1.2949e+01,  5.3812e+00, -1.2023e+00, -1.0903e+01,  9.8396e+02,\n",
       "          6.5484e+00],\n",
       "        [-1.0776e+01, -3.2429e+01,  4.0171e-01,  1.8320e+01, -3.3572e+01,\n",
       "         -8.1497e+00, -2.8001e+00,  6.2528e+00, -3.8146e+00, -1.9244e+01,\n",
       "         -3.8623e+01,  1.7763e+01, -6.7973e+01, -8.5208e+01,  6.5484e+00,\n",
       "          9.9435e+02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_caption_embed = torch.randn(16,1024)\n",
    "captions_similarity = sample_caption_embed @ sample_caption_embed.T\n",
    "captions_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_audio_embed = torch.randn(16,1024)\n",
    "audios_similarity = sample_audio_embed @ sample_audio_embed.T\n",
    "audios_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# train_csv_path = '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/csv/train.csv'\n",
    "# val_csv_path = '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/csv/val.csv'\n",
    "\n",
    "# origin_train = pd.read_csv(train_csv_path)\n",
    "# origin_val = pd.read_csv(val_csv_path)\n",
    "# origin_train\n",
    "\n",
    "# train_loader = get_data_loader(csv_path=train_csv_path, batch_size=batch_size, mode='train')\n",
    "# val_loader = get_data_loader(csv_path=val_csv_path, batch_size=batch_size, mode='val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/65121052_Use.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/135001000_Hit.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/400001064_Hit.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/63141012_Hit.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/31211003_Use.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/80003538_Use.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/35141002_Hit.mp3', '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill_44100/22110023_Use.mp3']\n",
      "['The sharp, forceful sound of a magical weapon, like a wand or staff, conjuring the rushing essence of wind as it strikes with a piercing, arcane impact.', 'The sharp, high-pitched clash of a slender weapon, like a thin club, landing a powerful strike, infused with the piercing chill of ice.', 'The low-pitched, muffled thud of a weak strike from a blunt weapon, like a club, landing with a soft and subdued impact.', 'The powerful impact begins with a dull thud that sharpens into a piercing slice, embodying the decisive strike of a bladed weapon.', 'The sound of magic unleashing a gentle explosion begins with a sharp, high-pitched burst before softening into a low, muffled resonance, evoking a subtle and fleeting arcane impact.', 'The sound begins with a sharp, high-pitched burst like a crashing wave of magical water, then transitions into a deep, dull thud as if the force of a powerful aquatic spell strikes its target with unrelenting strength.', 'The powerful sound of a blunt weapon, like a club, landing a heavy impact resonates with a transition from a deep, muffled thud to a sharp, cutting edge.', 'The powerful sound of a bladed weapon slicing through the air begins with a dull resonance before sharpening into a keen, cutting edge, evoking the swift and fierce force of the wind.']\n"
     ]
    }
   ],
   "source": [
    "for audio_files, class_labels in train_loader:\n",
    "    print(audio_files)\n",
    "    print(class_labels)\n",
    "    break\n",
    "\n",
    "# from IPython.display import display, Audio\n",
    "# audio_files = [\n",
    "#     '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000097_Hit.mp3',\n",
    "#     '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000097_Use.mp3',\n",
    "#     '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000099_Hit.mp3',\n",
    "#     '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000099_Use.mp3',\n",
    "#     '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000100_Hit.mp3',\n",
    "# ]\n",
    "# display(Audio(audio_files[2]))\n",
    "\n",
    "# class_labels = [\n",
    "#     'This Sound is class of sharp blade',\n",
    "#     'This Sound is class of blunt blade',\n",
    "#     'fire',\n",
    "#     'blunt weapon',\n",
    "#     'ax',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (Choose between versions '2022' or '2023')\n",
    "# The model weight will be downloaded automatically if `model_fp` is not specified\n",
    "clap_model = CLAP(version = '2023', use_cuda=True)\n",
    "\n",
    "# # Extract text embeddings\n",
    "# text_embeddings = clap_model.get_text_embeddings(class_labels = class_labels)\n",
    "\n",
    "# # Extract audio embeddings\n",
    "# audio_embeddings = clap_model.get_audio_embeddings(audio_files=audio_files[:1])\n",
    "\n",
    "# # Compute similarity between audio and text embeddings\n",
    "# similarities = clap_model.compute_similarity(audio_embeddings, text_embeddings)\n",
    "# print(f\"class_labels : {class_labels}\")\n",
    "# similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder_trainable = False\n",
    "caption_encoder_trainable = False\n",
    "projection_layer_trainable = True\n",
    "\n",
    "if audio_encoder_trainable:\n",
    "    clap_model.clap.audio_encoder.train()\n",
    "    for n, p in clap_model.clap.audio_encoder.named_parameters():\n",
    "        p.requires_grad = True\n",
    "else:\n",
    "    clap_model.clap.audio_encoder.eval()\n",
    "    for n, p in clap_model.clap.audio_encoder.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "\n",
    "if caption_encoder_trainable:\n",
    "    clap_model.clap.caption_encoder.train()\n",
    "    for n, p in clap_model.clap.caption_encoder.named_parameters():\n",
    "        p.requires_grad = True\n",
    "else:\n",
    "    clap_model.clap.caption_encoder.eval()\n",
    "    for n, p in clap_model.clap.caption_encoder.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clap_model.clap.eval()\n",
    "# clap_model.clap.audio_encoder.eval()\n",
    "# clap_model.clap.caption_encoder.eval()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 20160] at entry 0 and [1, 16128] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-80ce81e73f85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m audio_loaded_tensor = torch.stack(\n\u001b[0m\u001b[1;32m     26\u001b[0m     [\n\u001b[1;32m     27\u001b[0m         \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maudio_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 20160] at entry 0 and [1, 16128] at entry 1"
     ]
    }
   ],
   "source": [
    "######################\n",
    "# dummy - rand tensor\n",
    "audio_sample_tensor = torch.randn(16,600)\n",
    "audio_sample_tensor = audio_sample_tensor.to(0)\n",
    "\n",
    "audio_embed, _ = clap_model.clap.audio_encoder(audio_sample_tensor)\n",
    "audio_embed.shape\n",
    "\n",
    "# print(clap_model.clap.audio_encoder(sample_tensor)[0].shape) # (B, embed_size)\n",
    "# print(clap_model.clap.audio_encoder(sample_tensor)[1].shape) # (B, class_num)\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "# loaded - audio tensor\n",
    "\n",
    "audio_files = [\n",
    "    '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000097_Hit.mp3',\n",
    "    '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000097_Use.mp3',\n",
    "    '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000099_Hit.mp3',\n",
    "    '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000099_Use.mp3',\n",
    "    '/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/dataset_tta/maple_svn/audio/Skill/0000100_Hit.mp3',\n",
    "]\n",
    "\n",
    "audio_loaded_tensor = torch.stack(\n",
    "    [\n",
    "        torchaudio.load(audio_path)[0] for audio_path in audio_files\n",
    "    ]\n",
    ")\n",
    "print(audio_loaded_tensor.shape)\n",
    "\n",
    "audio_loaded_tensor = audio_loaded_tensor.to(0)\n",
    "\n",
    "audio_embed, _ = clap_model.clap.audio_encoder(audio_loaded_tensor)\n",
    "audio_embed.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca534400c014e02b1827fcb2abfd8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec99c7498cdf44f9b5d574bb4da49033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7f71c5b21a465796eabc03460a1f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403e39d77f5148529f351cb84ad170af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "base_model_text = AutoModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_text.eval()\n",
    "for p in base_model_text.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-910b91d8a7b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5661,  2128,   318,   286,   262,   698,  5286,  4282],\n",
      "        [ 5661,  2128,   318,   286,   262, 19861,  4282,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0071, -0.0280, -0.3840,  ..., -0.0697, -0.0614, -0.0583],\n",
       "         [-0.4223, -0.4521, -0.9903,  ...,  0.1927, -0.4034,  0.1673],\n",
       "         [-0.2657,  0.0442, -1.2647,  ...,  0.1127,  0.1206,  0.0787],\n",
       "         ...,\n",
       "         [-0.7840, -0.4060,  0.5020,  ...,  0.5002,  0.1349, -0.0720],\n",
       "         [-0.5229, -0.1960,  0.0340,  ...,  0.0186, -0.5641, -0.5585],\n",
       "         [-0.4101, -0.3288, -1.5413,  ..., -0.1082,  0.3160, -0.3535]],\n",
       "\n",
       "        [[-0.0071, -0.0280, -0.3840,  ..., -0.0697, -0.0614, -0.0583],\n",
       "         [-0.4223, -0.4521, -0.9903,  ...,  0.1927, -0.4034,  0.1673],\n",
       "         [-0.2657,  0.0442, -1.2647,  ...,  0.1127,  0.1206,  0.0787],\n",
       "         ...,\n",
       "         [-0.6850, -1.1129, -0.8607,  ...,  0.2148, -0.1444, -0.4192],\n",
       "         [-0.5675, -0.4037, -1.2908,  ...,  0.0138,  0.3149, -0.3602],\n",
       "         [-0.3531, -0.5458, -0.4048,  ...,  0.1816,  0.1543, -0.2494]]]), past_key_values=((tensor([[[[-1.1436e+00,  2.0865e+00,  1.0915e+00,  ..., -1.2483e+00,\n",
       "           -6.9566e-01,  1.6006e+00],\n",
       "          [-2.0655e+00,  2.4186e+00,  3.8522e-01,  ..., -1.5499e+00,\n",
       "           -1.6540e+00,  2.0035e+00],\n",
       "          [-1.9516e+00,  2.4045e+00,  1.9535e+00,  ..., -1.6259e+00,\n",
       "           -2.7266e+00,  2.5379e+00],\n",
       "          ...,\n",
       "          [-2.0669e+00,  2.6467e+00,  7.4096e-01,  ..., -1.1156e+00,\n",
       "           -1.2602e+00,  3.1319e+00],\n",
       "          [-1.5324e+00,  2.7718e+00,  1.5009e+00,  ..., -1.2409e+00,\n",
       "           -2.2114e+00,  1.3979e+00],\n",
       "          [-1.7877e+00,  3.0435e+00,  3.0038e+00,  ..., -5.7709e-01,\n",
       "           -2.7736e+00,  2.8418e+00]],\n",
       "\n",
       "         [[ 3.6071e-01,  2.0304e-01, -8.6156e-01,  ...,  6.4923e-01,\n",
       "            2.1252e+00, -1.5015e-01],\n",
       "          [ 1.0914e+00, -9.0908e-01, -4.8810e-01,  ..., -1.9603e+00,\n",
       "            4.7904e+00,  1.8706e+00],\n",
       "          [ 6.3825e-01, -1.3322e+00, -1.9821e-01,  ..., -3.9342e+00,\n",
       "            3.4697e+00,  9.5961e-01],\n",
       "          ...,\n",
       "          [-4.3070e-01, -1.2241e+00, -1.0348e-01,  ..., -7.4933e-01,\n",
       "            3.9746e+00,  6.3782e-01],\n",
       "          [ 4.5431e-01, -1.7561e+00,  1.1997e+00,  ...,  1.3062e+00,\n",
       "            3.1233e+00,  9.8018e-01],\n",
       "          [ 2.5612e-01,  5.8893e-01,  2.1544e+00,  ..., -6.8823e-01,\n",
       "            5.8948e+00,  4.6789e-01]],\n",
       "\n",
       "         [[-1.9523e-01, -1.1913e-01,  1.0264e+00,  ..., -1.4734e+00,\n",
       "           -1.9029e+00,  9.1331e-01],\n",
       "          [ 1.0870e-01, -9.0690e-01,  1.6444e-01,  ..., -3.4544e+00,\n",
       "            1.1274e-01,  1.3554e+00],\n",
       "          [ 5.5029e-01,  1.7884e-01,  1.9406e-01,  ..., -2.7556e+00,\n",
       "            5.5401e-01,  1.9818e+00],\n",
       "          ...,\n",
       "          [ 5.1193e-01,  2.4552e-01, -2.6986e-01,  ..., -2.9752e+00,\n",
       "            1.7634e+00,  2.1651e+00],\n",
       "          [-2.1258e-01, -4.1693e-02, -5.9678e-01,  ..., -3.0227e+00,\n",
       "           -8.4866e-01,  9.4948e-01],\n",
       "          [-1.6915e-01,  9.0164e-01,  4.2270e-01,  ..., -3.8062e+00,\n",
       "            1.4554e+00,  1.3340e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.4446e-01, -1.7246e-01, -3.7272e-01,  ...,  3.5627e-01,\n",
       "            6.1325e-01,  4.2078e-01],\n",
       "          [ 3.5454e-01, -5.6379e-01, -1.8009e-01,  ...,  1.8514e+00,\n",
       "           -5.1220e-01,  1.0723e+00],\n",
       "          [ 2.1149e-01,  8.0715e-02,  1.9130e-02,  ...,  9.8345e-01,\n",
       "            2.3441e-01,  6.3648e-01],\n",
       "          ...,\n",
       "          [-6.1883e-02, -5.5401e-01, -4.3929e-02,  ...,  1.0052e+00,\n",
       "            5.6205e-01,  2.8620e-01],\n",
       "          [-7.6097e-01, -1.8013e-01,  3.8734e-01,  ...,  1.2247e+00,\n",
       "            8.9189e-01,  7.3691e-01],\n",
       "          [ 4.0523e-01,  3.8098e-02,  7.1087e-02,  ...,  1.0614e+00,\n",
       "           -9.0158e-01,  3.6321e-01]],\n",
       "\n",
       "         [[ 1.4932e+00,  1.1508e+00, -2.4704e-01,  ..., -7.6321e-02,\n",
       "            9.8715e-01, -1.3522e+00],\n",
       "          [ 6.4036e-01,  3.2189e-01,  1.4936e-01,  ..., -9.4182e-01,\n",
       "            1.1981e+00, -8.6206e-01],\n",
       "          [ 8.2104e-01,  4.1073e-01, -6.9295e-01,  ..., -1.0822e+00,\n",
       "            6.2705e-01, -2.3890e-01],\n",
       "          ...,\n",
       "          [-5.8946e-02,  4.5260e-01,  2.0343e-01,  ..., -1.0219e+00,\n",
       "            1.3716e+00, -2.0546e-01],\n",
       "          [ 1.3030e+00,  4.5461e-01, -3.6263e-01,  ..., -9.8425e-01,\n",
       "            5.4776e-01, -1.6461e-01],\n",
       "          [ 9.3570e-01, -3.4898e-02, -2.3460e-01,  ..., -5.6974e-01,\n",
       "            3.6569e-01,  2.9914e-01]],\n",
       "\n",
       "         [[ 8.1262e-01,  4.0266e-02,  5.6926e-01,  ..., -6.9822e-01,\n",
       "            4.0799e-01,  1.7316e+00],\n",
       "          [ 4.0775e-01,  3.2727e-01,  7.3920e-01,  ...,  3.7179e-02,\n",
       "           -8.0199e-02,  2.2406e+00],\n",
       "          [ 1.1558e+00, -3.4333e-01, -1.5896e-02,  ...,  4.3309e-01,\n",
       "            5.2857e-01,  1.7632e+00],\n",
       "          ...,\n",
       "          [ 2.4452e-01,  6.8595e-01, -2.2206e-01,  ...,  2.3729e-02,\n",
       "           -4.9185e-01,  1.3619e+00],\n",
       "          [ 3.5643e-01,  1.8868e-01,  4.9100e-01,  ..., -7.8005e-01,\n",
       "            1.3682e+00,  1.4396e+00],\n",
       "          [ 7.0839e-01, -1.5154e-02, -2.3896e-01,  ...,  1.0247e-01,\n",
       "           -2.9410e-02,  5.2750e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1436e+00,  2.0865e+00,  1.0915e+00,  ..., -1.2483e+00,\n",
       "           -6.9566e-01,  1.6006e+00],\n",
       "          [-2.0655e+00,  2.4186e+00,  3.8522e-01,  ..., -1.5499e+00,\n",
       "           -1.6540e+00,  2.0035e+00],\n",
       "          [-1.9516e+00,  2.4045e+00,  1.9535e+00,  ..., -1.6259e+00,\n",
       "           -2.7266e+00,  2.5379e+00],\n",
       "          ...,\n",
       "          [-1.0751e+00,  2.3585e+00,  9.3718e-01,  ..., -1.5649e+00,\n",
       "           -1.9986e+00,  1.3773e+00],\n",
       "          [-1.7962e+00,  3.1090e+00,  2.9890e+00,  ..., -5.6568e-01,\n",
       "           -2.7457e+00,  2.7835e+00],\n",
       "          [-2.1328e+00,  2.4912e+00,  2.0465e+00,  ..., -1.5112e+00,\n",
       "           -2.0199e+00,  2.2787e+00]],\n",
       "\n",
       "         [[ 3.6071e-01,  2.0304e-01, -8.6156e-01,  ...,  6.4923e-01,\n",
       "            2.1252e+00, -1.5015e-01],\n",
       "          [ 1.0914e+00, -9.0908e-01, -4.8810e-01,  ..., -1.9603e+00,\n",
       "            4.7904e+00,  1.8706e+00],\n",
       "          [ 6.3825e-01, -1.3322e+00, -1.9821e-01,  ..., -3.9342e+00,\n",
       "            3.4697e+00,  9.5961e-01],\n",
       "          ...,\n",
       "          [-1.1880e-01, -1.6549e+00, -2.2336e-01,  ...,  1.2495e-01,\n",
       "            4.9733e+00,  2.0045e+00],\n",
       "          [ 2.4161e-01,  5.8999e-01,  2.1002e+00,  ..., -6.9353e-01,\n",
       "            5.8823e+00,  4.8194e-01],\n",
       "          [ 7.0565e-02, -7.9252e-01, -1.6158e+00,  ..., -1.8459e+00,\n",
       "            1.8494e+00,  1.0810e+00]],\n",
       "\n",
       "         [[-1.9523e-01, -1.1913e-01,  1.0264e+00,  ..., -1.4734e+00,\n",
       "           -1.9029e+00,  9.1331e-01],\n",
       "          [ 1.0870e-01, -9.0690e-01,  1.6444e-01,  ..., -3.4544e+00,\n",
       "            1.1274e-01,  1.3554e+00],\n",
       "          [ 5.5029e-01,  1.7884e-01,  1.9406e-01,  ..., -2.7556e+00,\n",
       "            5.5401e-01,  1.9818e+00],\n",
       "          ...,\n",
       "          [ 1.8531e-01,  3.6216e-01, -5.1507e-01,  ..., -2.4011e+00,\n",
       "            1.3668e+00,  1.9277e+00],\n",
       "          [-1.9419e-01,  8.9976e-01,  4.4144e-01,  ..., -3.7694e+00,\n",
       "            1.3884e+00,  1.5775e-01],\n",
       "          [ 1.1088e+00, -3.6380e-03,  9.0587e-01,  ..., -2.9971e+00,\n",
       "            1.3816e+00,  1.6253e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.4446e-01, -1.7246e-01, -3.7272e-01,  ...,  3.5627e-01,\n",
       "            6.1325e-01,  4.2078e-01],\n",
       "          [ 3.5454e-01, -5.6379e-01, -1.8009e-01,  ...,  1.8514e+00,\n",
       "           -5.1220e-01,  1.0723e+00],\n",
       "          [ 2.1149e-01,  8.0715e-02,  1.9130e-02,  ...,  9.8345e-01,\n",
       "            2.3441e-01,  6.3648e-01],\n",
       "          ...,\n",
       "          [-1.0771e-01, -4.6019e-01,  1.1907e+00,  ...,  1.2736e+00,\n",
       "           -2.7426e-03,  6.7364e-01],\n",
       "          [ 4.2386e-01,  2.4888e-02,  6.3602e-02,  ...,  1.0571e+00,\n",
       "           -8.9131e-01,  3.3694e-01],\n",
       "          [-5.7941e-03,  3.1794e-01,  3.0419e-01,  ...,  9.6798e-01,\n",
       "            9.8636e-02,  3.2731e-01]],\n",
       "\n",
       "         [[ 1.4932e+00,  1.1508e+00, -2.4704e-01,  ..., -7.6321e-02,\n",
       "            9.8715e-01, -1.3522e+00],\n",
       "          [ 6.4036e-01,  3.2189e-01,  1.4936e-01,  ..., -9.4182e-01,\n",
       "            1.1981e+00, -8.6206e-01],\n",
       "          [ 8.2104e-01,  4.1073e-01, -6.9295e-01,  ..., -1.0822e+00,\n",
       "            6.2705e-01, -2.3890e-01],\n",
       "          ...,\n",
       "          [ 1.6776e-01, -3.0269e-01,  3.4790e-02,  ..., -8.6789e-01,\n",
       "            8.0555e-01,  3.6608e-01],\n",
       "          [ 9.5439e-01,  2.0187e-02, -2.3235e-01,  ..., -5.8923e-01,\n",
       "            4.4350e-01,  2.1439e-01],\n",
       "          [ 8.9057e-01, -3.2458e-02,  6.2301e-02,  ..., -6.5437e-01,\n",
       "            6.2772e-01, -1.8856e-01]],\n",
       "\n",
       "         [[ 8.1262e-01,  4.0266e-02,  5.6926e-01,  ..., -6.9822e-01,\n",
       "            4.0799e-01,  1.7316e+00],\n",
       "          [ 4.0775e-01,  3.2727e-01,  7.3920e-01,  ...,  3.7179e-02,\n",
       "           -8.0199e-02,  2.2406e+00],\n",
       "          [ 1.1558e+00, -3.4333e-01, -1.5896e-02,  ...,  4.3309e-01,\n",
       "            5.2857e-01,  1.7632e+00],\n",
       "          ...,\n",
       "          [-8.3342e-01, -1.7787e-01,  1.1374e-01,  ...,  1.4810e-02,\n",
       "           -6.1456e-02,  1.5453e+00],\n",
       "          [ 7.0625e-01, -7.8147e-02, -2.2271e-01,  ...,  1.1499e-01,\n",
       "           -7.7431e-02,  6.3508e-01],\n",
       "          [-2.3906e-01,  2.2958e-01,  1.2456e+00,  ..., -5.9374e-03,\n",
       "            1.0656e+00, -2.3688e-02]]]]), tensor([[[[ 1.1442e-01,  8.9396e-02,  9.9032e-02,  ...,  4.8486e-02,\n",
       "            4.2911e-02, -5.0545e-02],\n",
       "          [ 2.2277e-01,  1.2626e-01,  2.9917e-01,  ...,  1.1860e-01,\n",
       "           -1.0991e-01,  8.3752e-03],\n",
       "          [ 2.5557e-01, -2.6768e-01,  3.0051e-01,  ..., -5.0784e-02,\n",
       "            3.6647e-02, -8.0069e-02],\n",
       "          ...,\n",
       "          [ 2.7606e-01, -2.6871e-01, -3.2054e-01,  ...,  9.1268e-02,\n",
       "            1.2855e-01, -1.5683e-01],\n",
       "          [-1.4198e-01, -1.1997e-01, -4.0351e-02,  ..., -8.7178e-02,\n",
       "            1.4381e-01, -7.0372e-02],\n",
       "          [ 6.4074e-02,  1.2930e-02,  3.2824e-02,  ..., -9.1538e-02,\n",
       "            1.2224e-01, -2.2013e-01]],\n",
       "\n",
       "         [[ 3.8719e-01,  1.8677e-01, -2.0963e-01,  ..., -6.3338e-01,\n",
       "           -2.9971e-01,  1.2311e-01],\n",
       "          [ 4.0936e-01, -4.8359e-03,  9.6994e-02,  ...,  4.3593e-01,\n",
       "            5.5198e-02,  3.4131e-01],\n",
       "          [ 6.0686e-01,  7.4280e-02,  5.7613e-02,  ..., -1.2159e-01,\n",
       "            1.8726e-01, -9.9788e-03],\n",
       "          ...,\n",
       "          [ 5.4073e-01,  4.2482e-01, -1.1091e-01,  ...,  2.8222e-01,\n",
       "           -3.4104e-02,  1.2348e-01],\n",
       "          [ 2.0847e-01, -3.3013e-02,  2.0976e-01,  ..., -1.4266e-02,\n",
       "            6.9635e-02, -3.2849e-03],\n",
       "          [ 1.5073e-01,  4.7993e-02, -3.2618e-01,  ...,  2.1369e-01,\n",
       "           -3.1188e-02, -1.2696e-02]],\n",
       "\n",
       "         [[-1.7204e-01, -2.4489e-02, -9.7592e-02,  ..., -7.4125e-02,\n",
       "            9.2519e-02,  1.4184e-02],\n",
       "          [-9.8446e-02, -2.8474e-01,  8.1231e-02,  ..., -3.1224e-02,\n",
       "           -3.7072e-02, -1.0234e-01],\n",
       "          [ 1.6537e-01,  1.1376e-01,  9.8718e-02,  ..., -4.3806e-02,\n",
       "            8.1981e-02,  6.5961e-02],\n",
       "          ...,\n",
       "          [ 1.3367e-01,  8.7955e-02,  2.9167e-01,  ..., -1.5191e-01,\n",
       "            3.4847e-02, -1.0077e-01],\n",
       "          [-4.0634e-01, -5.2417e-02, -7.4775e-02,  ...,  3.6416e-01,\n",
       "            8.0636e-02,  4.7332e-01],\n",
       "          [ 1.2723e-02, -2.5138e-01,  1.3888e-01,  ..., -3.2285e-01,\n",
       "            1.3741e-01,  1.0154e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.9614e-02, -7.5363e-02, -3.6195e-02,  ..., -2.7125e-02,\n",
       "            4.8007e-02,  9.0335e-02],\n",
       "          [-2.6441e-01,  5.4301e-01, -9.9170e-02,  ...,  1.1891e-01,\n",
       "           -4.4166e-01, -5.0849e-01],\n",
       "          [-3.3938e-01,  3.6873e-01,  2.0094e-01,  ...,  1.5382e-01,\n",
       "           -4.7017e-01,  3.8088e-02],\n",
       "          ...,\n",
       "          [ 2.5268e-02, -1.7401e-01,  8.6009e-02,  ...,  1.7898e-01,\n",
       "           -3.4558e-01,  4.4359e-02],\n",
       "          [ 1.1916e-01, -9.7112e-02,  1.4359e-01,  ..., -1.8024e-01,\n",
       "            4.7554e-02,  6.1438e-02],\n",
       "          [-1.4283e-01,  1.0222e-02, -3.4468e-02,  ...,  4.8908e-02,\n",
       "           -6.9350e-02,  1.8265e-01]],\n",
       "\n",
       "         [[-9.4529e-02, -2.4230e-01, -5.8970e-02,  ...,  1.9337e-01,\n",
       "           -7.8404e-03, -1.9090e-01],\n",
       "          [-4.5278e-01, -2.0414e-01,  3.9156e-01,  ..., -3.5523e-01,\n",
       "            1.9764e-01, -1.2587e-01],\n",
       "          [-1.8267e-01,  4.2323e-02,  7.5111e-03,  ..., -5.6668e-01,\n",
       "           -2.4113e-01,  1.6339e-02],\n",
       "          ...,\n",
       "          [-1.4720e-01,  3.5909e-01,  2.3374e-01,  ..., -4.5540e-02,\n",
       "           -6.2177e-02, -2.0512e-01],\n",
       "          [-2.1720e-01,  1.3155e-01,  4.3938e-02,  ..., -5.3281e-01,\n",
       "            1.6709e-01, -9.9300e-02],\n",
       "          [-2.4209e-02, -1.8188e-01, -1.4785e-01,  ..., -3.9648e-01,\n",
       "            1.6862e-02,  2.5793e-01]],\n",
       "\n",
       "         [[-1.4689e-01, -1.3003e-01,  1.6086e-02,  ...,  1.0566e-01,\n",
       "           -2.3890e-01, -2.5639e-02],\n",
       "          [ 1.5090e-02,  2.0426e-01,  7.9361e-02,  ...,  3.1745e-01,\n",
       "            4.2863e-02,  5.5282e-02],\n",
       "          [-8.2214e-02, -1.0899e-01, -7.7801e-02,  ...,  1.1518e-01,\n",
       "            3.3343e-01,  5.6382e-02],\n",
       "          ...,\n",
       "          [ 5.4364e-02, -2.3904e-02,  1.2283e-02,  ...,  3.0402e-01,\n",
       "            1.3538e-01,  7.4951e-02],\n",
       "          [-1.0712e-01,  1.1138e-01, -2.3696e-01,  ..., -2.6974e-01,\n",
       "            5.0453e-01, -2.6067e-04],\n",
       "          [ 9.9619e-02,  1.4564e-01, -2.6485e-01,  ...,  2.0283e-01,\n",
       "           -2.3024e-01,  2.4244e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1442e-01,  8.9396e-02,  9.9032e-02,  ...,  4.8486e-02,\n",
       "            4.2911e-02, -5.0545e-02],\n",
       "          [ 2.2277e-01,  1.2626e-01,  2.9917e-01,  ...,  1.1860e-01,\n",
       "           -1.0991e-01,  8.3752e-03],\n",
       "          [ 2.5557e-01, -2.6768e-01,  3.0051e-01,  ..., -5.0784e-02,\n",
       "            3.6647e-02, -8.0069e-02],\n",
       "          ...,\n",
       "          [ 2.1296e-01, -2.2992e-01,  1.3055e-01,  ..., -8.5587e-02,\n",
       "           -2.3256e-01, -2.4333e-01],\n",
       "          [ 6.5040e-02,  1.2938e-02,  3.0650e-02,  ..., -7.3916e-02,\n",
       "            1.2600e-01, -2.2237e-01],\n",
       "          [-1.4286e-01, -2.0097e-02, -1.7181e-01,  ...,  2.3279e-01,\n",
       "           -2.4560e-01, -2.0813e-01]],\n",
       "\n",
       "         [[ 3.8719e-01,  1.8677e-01, -2.0963e-01,  ..., -6.3338e-01,\n",
       "           -2.9971e-01,  1.2311e-01],\n",
       "          [ 4.0936e-01, -4.8359e-03,  9.6994e-02,  ...,  4.3593e-01,\n",
       "            5.5198e-02,  3.4131e-01],\n",
       "          [ 6.0686e-01,  7.4280e-02,  5.7613e-02,  ..., -1.2159e-01,\n",
       "            1.8726e-01, -9.9788e-03],\n",
       "          ...,\n",
       "          [ 3.1428e-01,  3.4570e-01, -1.4536e-01,  ...,  4.2866e-01,\n",
       "           -6.5285e-02,  2.8870e-01],\n",
       "          [ 1.7116e-01,  4.4717e-02, -3.2025e-01,  ...,  2.1511e-01,\n",
       "           -2.7443e-02, -1.5694e-02],\n",
       "          [ 2.4405e-01, -1.2673e-01, -4.7559e-02,  ...,  1.0960e-01,\n",
       "            5.4931e-01, -2.1825e-01]],\n",
       "\n",
       "         [[-1.7204e-01, -2.4489e-02, -9.7592e-02,  ..., -7.4125e-02,\n",
       "            9.2519e-02,  1.4184e-02],\n",
       "          [-9.8446e-02, -2.8474e-01,  8.1231e-02,  ..., -3.1224e-02,\n",
       "           -3.7072e-02, -1.0234e-01],\n",
       "          [ 1.6537e-01,  1.1376e-01,  9.8718e-02,  ..., -4.3806e-02,\n",
       "            8.1981e-02,  6.5961e-02],\n",
       "          ...,\n",
       "          [ 5.1184e-03, -1.6978e-01, -3.7770e-03,  ...,  5.2419e-02,\n",
       "            7.0398e-02,  1.7631e-01],\n",
       "          [ 9.0886e-03, -2.5053e-01,  1.4105e-01,  ..., -3.1872e-01,\n",
       "            1.3955e-01,  9.5182e-02],\n",
       "          [-2.3810e-02, -1.9155e-01,  2.3918e-01,  ..., -2.2956e-01,\n",
       "           -1.6703e-02, -7.0961e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.9614e-02, -7.5363e-02, -3.6195e-02,  ..., -2.7125e-02,\n",
       "            4.8007e-02,  9.0335e-02],\n",
       "          [-2.6441e-01,  5.4301e-01, -9.9170e-02,  ...,  1.1891e-01,\n",
       "           -4.4166e-01, -5.0849e-01],\n",
       "          [-3.3938e-01,  3.6873e-01,  2.0094e-01,  ...,  1.5382e-01,\n",
       "           -4.7017e-01,  3.8088e-02],\n",
       "          ...,\n",
       "          [ 8.4961e-02, -2.8402e-01, -3.2249e-01,  ...,  1.2330e-01,\n",
       "           -8.4095e-02,  4.3258e-02],\n",
       "          [-1.3479e-01,  1.2735e-02, -2.4126e-02,  ...,  5.6745e-02,\n",
       "           -8.1253e-02,  1.8282e-01],\n",
       "          [ 1.4707e-01,  1.5746e-01,  1.0678e-01,  ..., -8.4326e-02,\n",
       "           -1.9112e-01, -3.4682e-03]],\n",
       "\n",
       "         [[-9.4529e-02, -2.4230e-01, -5.8970e-02,  ...,  1.9337e-01,\n",
       "           -7.8404e-03, -1.9090e-01],\n",
       "          [-4.5278e-01, -2.0414e-01,  3.9156e-01,  ..., -3.5523e-01,\n",
       "            1.9764e-01, -1.2587e-01],\n",
       "          [-1.8267e-01,  4.2323e-02,  7.5111e-03,  ..., -5.6668e-01,\n",
       "           -2.4113e-01,  1.6339e-02],\n",
       "          ...,\n",
       "          [ 1.3348e-01,  2.3944e-01,  1.7042e-01,  ...,  1.9706e-01,\n",
       "           -7.3184e-02,  1.0679e-01],\n",
       "          [-3.4472e-02, -1.7786e-01, -1.4455e-01,  ..., -3.9577e-01,\n",
       "            2.5575e-02,  2.5523e-01],\n",
       "          [-2.0065e-01,  1.2806e-01, -2.1544e-01,  ...,  6.4791e-03,\n",
       "            2.7109e-01, -2.5112e-01]],\n",
       "\n",
       "         [[-1.4689e-01, -1.3003e-01,  1.6086e-02,  ...,  1.0566e-01,\n",
       "           -2.3890e-01, -2.5639e-02],\n",
       "          [ 1.5090e-02,  2.0426e-01,  7.9361e-02,  ...,  3.1745e-01,\n",
       "            4.2863e-02,  5.5282e-02],\n",
       "          [-8.2214e-02, -1.0899e-01, -7.7801e-02,  ...,  1.1518e-01,\n",
       "            3.3343e-01,  5.6382e-02],\n",
       "          ...,\n",
       "          [-1.0803e-01,  3.6273e-01, -1.2743e-01,  ...,  6.6010e-02,\n",
       "           -2.2722e-01,  1.5540e-01],\n",
       "          [ 9.0012e-02,  1.4436e-01, -2.7129e-01,  ...,  2.0291e-01,\n",
       "           -2.2089e-01,  2.3762e-02],\n",
       "          [ 1.1952e-01, -8.1128e-02,  1.6822e-01,  ..., -1.3012e-02,\n",
       "            4.2157e-01, -6.1377e-02]]]])), (tensor([[[[-0.1710,  1.3373, -1.3746,  ...,  1.5076, -1.5404,  0.8587],\n",
       "          [ 0.6525,  1.3203, -0.7687,  ..., -0.7196, -1.9337, -0.2516],\n",
       "          [ 0.4729,  2.1617, -1.5536,  ..., -0.3910, -2.3293,  0.2691],\n",
       "          ...,\n",
       "          [ 0.2318, -1.3478,  0.1698,  ...,  0.2116, -1.9545, -0.7563],\n",
       "          [-0.3902,  1.0655, -0.5599,  ..., -0.0392, -1.6715, -0.8253],\n",
       "          [-0.3960,  0.6426,  0.0554,  ..., -0.0424, -1.9905, -0.9958]],\n",
       "\n",
       "         [[-1.0522, -0.4839, -0.7675,  ..., -0.1920,  0.8452, -0.4005],\n",
       "          [-0.6950,  0.0514, -1.2295,  ..., -0.6964,  0.3227, -0.6352],\n",
       "          [-0.9413,  0.6076, -1.4602,  ..., -0.3719,  0.8974, -0.5858],\n",
       "          ...,\n",
       "          [-0.6254, -0.4795, -2.0623,  ..., -0.7288,  0.7119, -0.2763],\n",
       "          [-0.8372,  0.9092, -2.3479,  ...,  0.0251, -0.5857, -1.4251],\n",
       "          [-0.5671,  0.8463, -1.5903,  ..., -0.3994, -0.8761, -0.4422]],\n",
       "\n",
       "         [[ 0.3445,  0.1636, -0.0327,  ..., -1.3216,  0.0733, -0.1622],\n",
       "          [-0.0210,  0.5705, -0.0682,  ..., -1.3057, -0.1810,  0.3053],\n",
       "          [-0.2559, -0.1064, -0.2392,  ..., -1.0044, -0.1241,  0.3559],\n",
       "          ...,\n",
       "          [-0.3180,  0.0193, -0.0426,  ..., -0.7820, -0.0795,  0.6096],\n",
       "          [ 0.0959,  0.3560, -0.4042,  ..., -0.9147,  0.2072,  0.2580],\n",
       "          [-0.2278,  0.0711, -0.1605,  ..., -1.0611, -0.1792,  0.3905]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1352, -0.2650, -0.7097,  ..., -0.3652,  0.2232, -0.9971],\n",
       "          [-1.5467,  1.4972,  2.0581,  ...,  0.3801, -0.2263, -1.8097],\n",
       "          [-0.1045,  2.8256,  1.2897,  ..., -0.1192, -0.1497,  0.3597],\n",
       "          ...,\n",
       "          [-0.4080, -1.9079,  1.9843,  ..., -1.2036, -0.8876, -1.9239],\n",
       "          [-2.5899,  2.7984,  2.1985,  ..., -0.5189, -0.5381, -0.7671],\n",
       "          [ 0.4414, -0.1068,  1.3604,  ..., -1.3899, -0.2812, -0.4955]],\n",
       "\n",
       "         [[-1.1331, -2.7813,  0.1867,  ...,  1.8182,  1.6015, -1.6858],\n",
       "          [ 0.0528,  0.8610, -0.4327,  ..., -0.5439,  0.5729, -0.4381],\n",
       "          [ 0.1673,  0.6692, -0.5039,  ..., -0.6262,  0.4445, -0.3232],\n",
       "          ...,\n",
       "          [ 0.1168,  0.5400, -0.6954,  ..., -0.1549,  0.7692,  0.1600],\n",
       "          [-0.3336,  0.5457, -0.9568,  ..., -0.4736,  0.8394,  0.3672],\n",
       "          [-0.2269,  0.5008, -0.7164,  ..., -0.4207,  0.7021,  0.5035]],\n",
       "\n",
       "         [[ 1.0809,  2.2289,  0.3652,  ..., -0.4478, -0.3158,  0.4477],\n",
       "          [-0.7260,  3.1838,  0.9027,  ...,  0.5261, -0.7315, -0.4441],\n",
       "          [ 1.2258,  1.6608, -0.5399,  ...,  1.0069,  0.1088, -1.2776],\n",
       "          ...,\n",
       "          [ 1.2076,  2.2450,  2.0444,  ...,  0.9829, -2.4266, -2.1647],\n",
       "          [ 0.7207,  1.5253,  0.0760,  ...,  0.5189, -0.0145, -1.4511],\n",
       "          [-1.9067,  0.9235,  0.3856,  ...,  1.6522, -1.1752, -0.2001]]],\n",
       "\n",
       "\n",
       "        [[[-0.1710,  1.3373, -1.3746,  ...,  1.5076, -1.5404,  0.8587],\n",
       "          [ 0.6525,  1.3203, -0.7687,  ..., -0.7196, -1.9337, -0.2516],\n",
       "          [ 0.4729,  2.1617, -1.5536,  ..., -0.3910, -2.3293,  0.2691],\n",
       "          ...,\n",
       "          [ 1.2532,  0.6574,  0.9115,  ...,  0.2176, -0.8083, -0.9096],\n",
       "          [-0.2188,  0.7050,  0.1703,  ..., -0.2038, -2.2202, -0.7705],\n",
       "          [-0.2660,  0.7401, -0.5732,  ...,  0.2490, -0.3060, -0.2315]],\n",
       "\n",
       "         [[-1.0522, -0.4839, -0.7675,  ..., -0.1920,  0.8452, -0.4005],\n",
       "          [-0.6950,  0.0514, -1.2295,  ..., -0.6964,  0.3227, -0.6352],\n",
       "          [-0.9413,  0.6076, -1.4602,  ..., -0.3719,  0.8974, -0.5858],\n",
       "          ...,\n",
       "          [-0.7263,  0.3115, -1.9239,  ..., -1.3430, -0.1901,  0.0247],\n",
       "          [-0.5475,  0.6440, -1.6749,  ..., -0.4287, -0.7746, -0.3737],\n",
       "          [-0.2782,  1.1877, -1.4474,  ..., -0.4815, -0.1530, -0.3983]],\n",
       "\n",
       "         [[ 0.3445,  0.1636, -0.0327,  ..., -1.3216,  0.0733, -0.1622],\n",
       "          [-0.0210,  0.5705, -0.0682,  ..., -1.3057, -0.1810,  0.3053],\n",
       "          [-0.2559, -0.1064, -0.2392,  ..., -1.0044, -0.1241,  0.3559],\n",
       "          ...,\n",
       "          [ 0.1372,  0.2752,  0.0528,  ..., -0.8756, -0.1156,  0.6244],\n",
       "          [-0.2676,  0.0343, -0.2095,  ..., -1.0548, -0.1300,  0.4579],\n",
       "          [-0.1839,  0.1789, -0.3186,  ..., -1.2274,  0.0598,  0.2289]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1352, -0.2650, -0.7097,  ..., -0.3652,  0.2232, -0.9971],\n",
       "          [-1.5467,  1.4972,  2.0581,  ...,  0.3801, -0.2263, -1.8097],\n",
       "          [-0.1045,  2.8256,  1.2897,  ..., -0.1192, -0.1497,  0.3597],\n",
       "          ...,\n",
       "          [-2.5599, -0.1306,  2.9792,  ...,  0.2407, -0.7433,  1.0019],\n",
       "          [ 0.6477,  0.0317,  1.2373,  ..., -1.1788,  0.0850, -0.6531],\n",
       "          [-0.6146,  1.8615,  1.9272,  ...,  0.7596, -1.8452,  0.0559]],\n",
       "\n",
       "         [[-1.1331, -2.7813,  0.1867,  ...,  1.8182,  1.6015, -1.6858],\n",
       "          [ 0.0528,  0.8610, -0.4327,  ..., -0.5439,  0.5729, -0.4381],\n",
       "          [ 0.1673,  0.6692, -0.5039,  ..., -0.6262,  0.4445, -0.3232],\n",
       "          ...,\n",
       "          [-0.2187,  0.7304, -0.7075,  ..., -0.1688,  0.9543, -0.1440],\n",
       "          [-0.1868,  0.4769, -0.6914,  ..., -0.4699,  0.6498,  0.4680],\n",
       "          [-0.1558,  0.6080, -0.7164,  ..., -0.4306,  0.6172, -0.2722]],\n",
       "\n",
       "         [[ 1.0809,  2.2289,  0.3652,  ..., -0.4478, -0.3158,  0.4477],\n",
       "          [-0.7260,  3.1838,  0.9027,  ...,  0.5261, -0.7315, -0.4441],\n",
       "          [ 1.2258,  1.6608, -0.5399,  ...,  1.0069,  0.1088, -1.2776],\n",
       "          ...,\n",
       "          [-0.9652,  2.2504,  0.9451,  ...,  0.9752, -0.5069, -1.7585],\n",
       "          [-2.0715,  0.8920,  0.1242,  ...,  1.7391, -1.2918, -0.5149],\n",
       "          [-0.1659,  1.6897,  1.1170,  ...,  0.0151, -0.1980,  0.3060]]]]), tensor([[[[ 3.4461e-02,  5.3810e-02, -6.0039e-02,  ...,  1.6357e-01,\n",
       "            2.0356e-01,  7.5095e-02],\n",
       "          [ 4.7639e-01, -4.2484e-01, -1.1801e-01,  ..., -2.2742e-01,\n",
       "           -1.4417e-01, -7.2629e-01],\n",
       "          [ 2.9024e-01,  1.6836e-01, -1.2606e-01,  ..., -2.4216e-01,\n",
       "           -4.9762e-01, -2.6083e-01],\n",
       "          ...,\n",
       "          [ 5.9815e-01,  7.2458e-02,  1.4292e-01,  ..., -2.3083e-01,\n",
       "            1.1607e-01, -1.2186e+00],\n",
       "          [-6.9666e-01, -3.4039e-01,  3.3241e-01,  ...,  1.1703e+00,\n",
       "            1.7860e-01,  4.2674e-01],\n",
       "          [-1.3713e-01, -2.4302e-01,  2.5140e-01,  ..., -4.1765e-01,\n",
       "           -1.7054e-01, -6.7208e-01]],\n",
       "\n",
       "         [[ 2.4386e-01, -1.8171e-01, -2.0817e-01,  ..., -1.6709e-02,\n",
       "           -3.7344e-01,  1.5752e-01],\n",
       "          [ 5.3348e-01,  2.2551e-01,  3.8752e-01,  ..., -4.8409e-02,\n",
       "           -1.0466e-01, -2.4491e-01],\n",
       "          [ 3.0238e-01,  3.5969e-01,  8.6172e-01,  ..., -3.7493e-01,\n",
       "            1.1850e-01,  3.0808e-01],\n",
       "          ...,\n",
       "          [ 2.2987e-01, -5.0897e-01,  4.5901e-01,  ..., -2.6113e-01,\n",
       "           -3.3951e-01, -4.2201e-01],\n",
       "          [ 1.1119e-01, -5.8535e-01, -4.1176e-01,  ..., -6.4166e-01,\n",
       "           -2.0591e-01,  6.2512e-01],\n",
       "          [ 3.2078e-01,  7.8464e-01, -2.7525e-01,  ...,  3.5138e-02,\n",
       "            2.3323e-01,  2.9213e-01]],\n",
       "\n",
       "         [[ 7.6679e-02, -6.9301e-02,  9.3575e-02,  ..., -5.5150e-01,\n",
       "           -3.9165e-02, -5.7337e-02],\n",
       "          [ 6.2735e-01,  2.2765e-01,  1.4792e-01,  ..., -5.4754e-01,\n",
       "            1.0314e-01,  2.4758e-01],\n",
       "          [ 5.7838e-01,  9.1407e-02,  4.4511e-01,  ..., -7.2691e-01,\n",
       "           -3.1447e-04, -1.0178e-01],\n",
       "          ...,\n",
       "          [ 8.3255e-01, -1.7843e-01,  3.1084e-01,  ..., -5.5168e-01,\n",
       "           -2.3975e-01,  1.0507e-01],\n",
       "          [ 8.0760e-01,  3.3441e-01, -8.1942e-02,  ..., -5.3254e-01,\n",
       "           -1.3316e-01, -3.8733e-01],\n",
       "          [ 6.2830e-01,  2.2615e-01,  4.9421e-02,  ..., -5.4943e-01,\n",
       "            1.8377e-01,  1.9257e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.0752e-02,  5.8248e-01, -2.1535e-01,  ...,  7.5454e-02,\n",
       "           -9.8725e-01, -1.6092e-01],\n",
       "          [ 2.9684e-01, -6.6961e-01, -1.8555e-01,  ...,  8.6394e-02,\n",
       "           -1.1618e+00, -3.9947e-01],\n",
       "          [ 1.3921e-01, -9.7825e-02, -5.8024e-02,  ...,  8.0907e-02,\n",
       "           -2.5873e-01, -2.6898e-01],\n",
       "          ...,\n",
       "          [ 1.1466e+00,  4.2995e-01, -2.8859e-01,  ...,  2.0312e-01,\n",
       "           -3.7264e-01, -6.7453e-01],\n",
       "          [-4.7273e-01, -5.0794e-01, -1.0731e-01,  ...,  1.8992e-01,\n",
       "           -7.3916e-01,  1.2430e+00],\n",
       "          [ 4.4597e-01,  3.7747e-01,  7.3052e-02,  ...,  1.5312e-01,\n",
       "           -1.8900e-01, -7.1905e-01]],\n",
       "\n",
       "         [[ 2.3116e-01,  3.4742e-03, -2.0459e-01,  ...,  3.8037e-01,\n",
       "           -3.6032e+00, -5.5403e-02],\n",
       "          [-4.7849e-02, -1.1475e-01,  1.9707e-01,  ...,  1.0683e-01,\n",
       "            3.0446e-02, -1.9806e-01],\n",
       "          [-6.0624e-02, -1.9889e-02,  1.0197e-01,  ...,  1.3777e-01,\n",
       "            1.3713e-01,  1.9755e-01],\n",
       "          ...,\n",
       "          [-2.8184e-01, -2.2332e-01,  4.5234e-01,  ...,  5.8545e-02,\n",
       "            6.8447e-02, -1.9759e-01],\n",
       "          [-3.4638e-01,  6.8197e-01, -7.6146e-01,  ...,  3.7683e-01,\n",
       "            2.1188e-03,  9.1040e-02],\n",
       "          [-3.8375e-01,  2.9792e-02, -6.3674e-02,  ...,  2.7366e-01,\n",
       "           -3.1989e-02, -6.0449e-02]],\n",
       "\n",
       "         [[ 4.2244e-02, -2.2267e-01,  1.5192e-02,  ..., -2.3924e-01,\n",
       "            1.4157e-01, -9.0194e-02],\n",
       "          [ 2.1614e-01,  2.3538e-01,  2.5738e-01,  ..., -2.9293e-02,\n",
       "            1.6721e-01,  2.1699e-02],\n",
       "          [ 1.0632e-01, -2.5687e-02,  1.9695e-02,  ..., -1.0024e-01,\n",
       "            2.4507e-01,  9.9835e-02],\n",
       "          ...,\n",
       "          [-2.6332e-02,  2.4213e-01,  1.5920e-02,  ..., -1.9419e-01,\n",
       "            2.0380e-01,  7.7946e-02],\n",
       "          [ 2.7583e-01,  4.1572e-01,  1.3806e-01,  ..., -4.1584e-01,\n",
       "           -2.0434e-01,  7.4196e-02],\n",
       "          [-5.9554e-03,  5.1524e-01,  1.8955e-01,  ..., -9.1919e-02,\n",
       "            8.8073e-02, -2.3407e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4461e-02,  5.3810e-02, -6.0039e-02,  ...,  1.6357e-01,\n",
       "            2.0356e-01,  7.5095e-02],\n",
       "          [ 4.7639e-01, -4.2484e-01, -1.1801e-01,  ..., -2.2742e-01,\n",
       "           -1.4417e-01, -7.2629e-01],\n",
       "          [ 2.9024e-01,  1.6836e-01, -1.2606e-01,  ..., -2.4216e-01,\n",
       "           -4.9762e-01, -2.6083e-01],\n",
       "          ...,\n",
       "          [ 2.9543e-01,  8.5983e-03,  3.2052e-01,  ...,  6.4711e-02,\n",
       "            3.8953e-01,  5.9404e-02],\n",
       "          [-1.4954e-01, -1.8059e-01,  2.4620e-01,  ..., -4.0995e-01,\n",
       "           -1.4992e-01, -6.7906e-01],\n",
       "          [ 8.3768e-01,  2.3416e-01,  1.8125e-02,  ..., -1.4808e-01,\n",
       "           -7.4150e-02,  7.2894e-02]],\n",
       "\n",
       "         [[ 2.4386e-01, -1.8171e-01, -2.0817e-01,  ..., -1.6709e-02,\n",
       "           -3.7344e-01,  1.5752e-01],\n",
       "          [ 5.3348e-01,  2.2551e-01,  3.8752e-01,  ..., -4.8409e-02,\n",
       "           -1.0466e-01, -2.4491e-01],\n",
       "          [ 3.0238e-01,  3.5969e-01,  8.6172e-01,  ..., -3.7493e-01,\n",
       "            1.1850e-01,  3.0808e-01],\n",
       "          ...,\n",
       "          [-1.5951e-01, -6.4632e-01, -2.5658e-01,  ..., -5.0300e-01,\n",
       "           -5.6070e-01, -3.2948e-01],\n",
       "          [ 2.9922e-01,  9.2583e-01, -3.3668e-01,  ...,  8.2251e-02,\n",
       "            2.0112e-01,  3.6263e-01],\n",
       "          [-8.5585e-03,  8.8595e-02,  6.7651e-02,  ...,  3.4560e-01,\n",
       "            1.3241e-01, -1.2514e-01]],\n",
       "\n",
       "         [[ 7.6679e-02, -6.9301e-02,  9.3575e-02,  ..., -5.5150e-01,\n",
       "           -3.9165e-02, -5.7337e-02],\n",
       "          [ 6.2735e-01,  2.2765e-01,  1.4792e-01,  ..., -5.4754e-01,\n",
       "            1.0314e-01,  2.4758e-01],\n",
       "          [ 5.7838e-01,  9.1407e-02,  4.4511e-01,  ..., -7.2691e-01,\n",
       "           -3.1447e-04, -1.0178e-01],\n",
       "          ...,\n",
       "          [ 7.5738e-01,  2.9652e-01,  1.6056e-02,  ..., -4.7272e-01,\n",
       "            3.8851e-01,  1.9215e-01],\n",
       "          [ 6.2581e-01,  1.5750e-01,  2.4212e-03,  ..., -6.1796e-01,\n",
       "            1.1058e-01,  2.4456e-01],\n",
       "          [ 7.5301e-01,  3.0720e-01,  5.9731e-01,  ..., -3.4426e-01,\n",
       "           -2.5434e-01, -6.5990e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.0752e-02,  5.8248e-01, -2.1535e-01,  ...,  7.5454e-02,\n",
       "           -9.8725e-01, -1.6092e-01],\n",
       "          [ 2.9684e-01, -6.6961e-01, -1.8555e-01,  ...,  8.6394e-02,\n",
       "           -1.1618e+00, -3.9947e-01],\n",
       "          [ 1.3921e-01, -9.7825e-02, -5.8024e-02,  ...,  8.0907e-02,\n",
       "           -2.5873e-01, -2.6898e-01],\n",
       "          ...,\n",
       "          [ 3.8937e-01, -1.4247e-01, -3.7604e-01,  ...,  1.7078e-01,\n",
       "           -5.6688e-01,  3.1174e-02],\n",
       "          [ 3.5941e-01,  4.8111e-01,  9.8799e-02,  ...,  1.3314e-01,\n",
       "           -2.3901e-01, -7.6065e-01],\n",
       "          [-3.1445e-01, -9.4670e-01,  3.2312e-01,  ...,  1.4400e-01,\n",
       "           -7.1186e-01,  1.9949e-01]],\n",
       "\n",
       "         [[ 2.3116e-01,  3.4742e-03, -2.0459e-01,  ...,  3.8037e-01,\n",
       "           -3.6032e+00, -5.5403e-02],\n",
       "          [-4.7849e-02, -1.1475e-01,  1.9707e-01,  ...,  1.0683e-01,\n",
       "            3.0446e-02, -1.9806e-01],\n",
       "          [-6.0624e-02, -1.9889e-02,  1.0197e-01,  ...,  1.3777e-01,\n",
       "            1.3713e-01,  1.9755e-01],\n",
       "          ...,\n",
       "          [-1.0396e-01,  7.2717e-02, -1.7059e-01,  ..., -6.3563e-02,\n",
       "            4.5306e-02, -2.9182e-01],\n",
       "          [-4.2947e-01,  7.0422e-02, -4.3211e-02,  ...,  3.0379e-01,\n",
       "           -2.0710e-03, -7.5741e-02],\n",
       "          [ 1.9186e-01,  1.8518e-01, -7.5721e-02,  ...,  4.9312e-02,\n",
       "           -1.7093e-01,  2.2080e-01]],\n",
       "\n",
       "         [[ 4.2244e-02, -2.2267e-01,  1.5192e-02,  ..., -2.3924e-01,\n",
       "            1.4157e-01, -9.0194e-02],\n",
       "          [ 2.1614e-01,  2.3538e-01,  2.5738e-01,  ..., -2.9293e-02,\n",
       "            1.6721e-01,  2.1699e-02],\n",
       "          [ 1.0632e-01, -2.5687e-02,  1.9695e-02,  ..., -1.0024e-01,\n",
       "            2.4507e-01,  9.9835e-02],\n",
       "          ...,\n",
       "          [ 7.9596e-02,  3.5486e-01, -8.0276e-03,  ..., -2.4644e-01,\n",
       "            5.7827e-02, -2.3995e-02],\n",
       "          [ 1.0981e-01,  5.8845e-01,  1.6131e-01,  ..., -1.5949e-01,\n",
       "            7.4208e-02, -2.9385e-01],\n",
       "          [ 2.5863e-01, -7.9712e-02,  1.9989e-01,  ..., -3.1200e-02,\n",
       "            2.8838e-02,  8.8121e-02]]]])), (tensor([[[[-1.1186e-01, -1.1026e+00,  2.4767e-01,  ..., -6.6577e-01,\n",
       "           -1.4554e-01, -2.9611e-02],\n",
       "          [ 3.6502e-01, -3.9350e+00, -8.3364e-01,  ..., -1.5125e+00,\n",
       "            1.7236e+00, -1.2858e+00],\n",
       "          [ 1.2716e+00, -1.5069e+00, -1.4332e+00,  ..., -5.6635e-01,\n",
       "            6.2908e-01, -2.2412e+00],\n",
       "          ...,\n",
       "          [-1.4593e-02, -2.5611e+00, -3.0076e-01,  ...,  4.3972e-01,\n",
       "           -3.3506e-01, -5.9884e-01],\n",
       "          [-4.4339e-01, -2.9503e+00,  9.7771e-03,  ...,  1.0619e+00,\n",
       "           -2.0771e-01, -6.2025e-01],\n",
       "          [ 1.1601e-01, -3.7071e+00,  7.8659e-02,  ...,  3.4276e-03,\n",
       "            6.2479e-01,  1.7493e-01]],\n",
       "\n",
       "         [[-5.2483e-01,  3.5554e-01, -4.6093e-01,  ...,  1.2881e+00,\n",
       "           -5.8592e-01, -4.8328e-01],\n",
       "          [-1.9848e+00,  2.4538e-01, -1.3853e+00,  ...,  1.3782e+00,\n",
       "           -4.5367e-01, -2.6087e-01],\n",
       "          [-1.0996e+00, -5.0891e-01, -1.9521e+00,  ...,  3.5400e-01,\n",
       "            9.7279e-01, -2.7433e-01],\n",
       "          ...,\n",
       "          [-7.0706e-01,  8.1434e-01, -3.4323e+00,  ...,  3.5173e-01,\n",
       "            4.3881e-01,  8.3492e-01],\n",
       "          [-4.6209e-02, -1.5843e+00, -3.2147e+00,  ..., -3.7299e-01,\n",
       "            5.3597e-02,  1.3290e+00],\n",
       "          [-1.5097e+00, -6.6613e-01, -1.8301e+00,  ...,  2.2057e-01,\n",
       "           -4.1749e-01, -2.5358e-01]],\n",
       "\n",
       "         [[ 1.3445e+00,  2.9825e+00,  3.7641e+00,  ...,  6.1052e-01,\n",
       "            1.7436e+00, -7.5559e-01],\n",
       "          [-2.5664e+00,  1.8577e+00, -2.3088e+00,  ..., -2.6007e+00,\n",
       "            2.9176e+00,  4.7832e-01],\n",
       "          [-4.2860e+00,  1.8278e+00, -2.9421e+00,  ..., -3.2008e+00,\n",
       "            3.2367e+00,  1.6046e+00],\n",
       "          ...,\n",
       "          [-2.4628e+00,  2.1518e-01, -1.7090e+00,  ..., -3.7901e+00,\n",
       "            1.8673e+00,  4.2523e-01],\n",
       "          [-4.0157e+00, -3.5967e-01, -3.7284e+00,  ..., -3.8264e+00,\n",
       "            2.3923e+00,  4.9770e-01],\n",
       "          [-2.7920e+00, -1.4021e+00, -4.4219e+00,  ..., -3.2063e+00,\n",
       "            1.5021e+00,  1.3684e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3135e+00, -2.7654e+00, -2.6663e+00,  ...,  9.2731e-01,\n",
       "            4.4871e-01,  2.6998e+00],\n",
       "          [-1.9386e+00,  1.8491e+00,  5.1022e-01,  ..., -1.4189e+00,\n",
       "           -2.0752e+00,  4.3427e-01],\n",
       "          [-1.9669e+00,  3.0340e+00,  3.6986e-01,  ..., -9.3247e-01,\n",
       "           -3.8082e+00,  6.0508e-01],\n",
       "          ...,\n",
       "          [-2.8705e+00,  3.4904e+00,  2.2309e+00,  ..., -5.0503e-01,\n",
       "           -4.2167e+00, -1.3876e+00],\n",
       "          [-3.5152e+00,  3.8541e+00,  2.2027e+00,  ..., -8.9223e-01,\n",
       "           -4.3482e+00, -1.7080e+00],\n",
       "          [-3.4686e+00,  3.2266e+00,  1.8740e+00,  ..., -1.5761e+00,\n",
       "           -3.8769e+00, -1.5074e+00]],\n",
       "\n",
       "         [[ 1.7215e+00,  4.3022e-01,  9.1175e-01,  ...,  2.2430e-02,\n",
       "           -9.9444e-01, -2.9929e-01],\n",
       "          [ 2.4425e+00,  1.3172e+00,  1.3130e+00,  ...,  5.8109e-01,\n",
       "           -1.6885e+00, -1.6583e+00],\n",
       "          [ 2.2457e+00,  9.4304e-01,  7.6179e-01,  ...,  2.1877e-01,\n",
       "           -2.0103e+00, -1.3964e+00],\n",
       "          ...,\n",
       "          [ 2.5123e+00,  2.0031e-01,  1.6640e+00,  ...,  3.1602e-01,\n",
       "           -1.9190e+00, -1.1183e+00],\n",
       "          [ 2.6421e+00,  8.3195e-02,  1.7265e+00,  ...,  4.1845e-01,\n",
       "           -2.0631e+00, -1.4812e+00],\n",
       "          [ 2.5899e+00,  5.9947e-01,  1.3289e+00,  ...,  1.5182e-01,\n",
       "           -1.2031e+00, -1.2792e+00]],\n",
       "\n",
       "         [[-2.4389e-01,  1.4319e-01, -5.5898e-01,  ...,  2.8451e-01,\n",
       "            2.5435e-01,  1.7896e-01],\n",
       "          [-9.3461e-01,  1.2209e+00, -1.3204e-01,  ...,  5.2765e-01,\n",
       "            2.6827e-01, -4.7608e-01],\n",
       "          [-7.1074e-01, -2.0835e-01, -6.7116e-02,  ...,  1.9865e-01,\n",
       "            4.8088e-01,  4.1199e-02],\n",
       "          ...,\n",
       "          [-8.3446e-01,  7.2203e-01, -1.1147e+00,  ..., -3.4326e-01,\n",
       "            4.8997e-01, -1.0140e+00],\n",
       "          [ 9.9571e-02,  2.1726e-01, -5.0553e-01,  ...,  5.5056e-01,\n",
       "            1.1398e+00,  2.8536e-01],\n",
       "          [-4.9926e-01,  2.4715e-01, -4.6890e-01,  ..., -1.9506e-01,\n",
       "            1.6531e-01, -4.7122e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.1186e-01, -1.1026e+00,  2.4767e-01,  ..., -6.6577e-01,\n",
       "           -1.4554e-01, -2.9611e-02],\n",
       "          [ 3.6502e-01, -3.9350e+00, -8.3364e-01,  ..., -1.5125e+00,\n",
       "            1.7236e+00, -1.2858e+00],\n",
       "          [ 1.2716e+00, -1.5069e+00, -1.4332e+00,  ..., -5.6635e-01,\n",
       "            6.2908e-01, -2.2412e+00],\n",
       "          ...,\n",
       "          [ 3.5814e-02, -2.7768e+00,  1.1196e+00,  ...,  7.2733e-01,\n",
       "            2.5585e-01, -1.8188e-01],\n",
       "          [ 1.5349e-01, -3.9606e+00,  4.6309e-02,  ..., -1.9065e-01,\n",
       "            8.9684e-01,  2.5306e-01],\n",
       "          [-1.6467e-01, -2.4480e+00,  2.9922e-01,  ...,  1.2352e+00,\n",
       "            2.8337e-01, -5.3851e-01]],\n",
       "\n",
       "         [[-5.2483e-01,  3.5554e-01, -4.6093e-01,  ...,  1.2881e+00,\n",
       "           -5.8592e-01, -4.8328e-01],\n",
       "          [-1.9848e+00,  2.4538e-01, -1.3853e+00,  ...,  1.3782e+00,\n",
       "           -4.5367e-01, -2.6087e-01],\n",
       "          [-1.0996e+00, -5.0891e-01, -1.9521e+00,  ...,  3.5400e-01,\n",
       "            9.7279e-01, -2.7433e-01],\n",
       "          ...,\n",
       "          [-9.0436e-01, -6.2668e-01, -2.0395e+00,  ..., -1.1647e+00,\n",
       "           -1.3112e-01,  5.7468e-01],\n",
       "          [-1.3046e+00, -4.3964e-01, -1.8049e+00,  ...,  8.9961e-02,\n",
       "           -2.0669e-01, -4.5962e-01],\n",
       "          [-1.5648e+00, -1.5554e+00, -1.6605e+00,  ...,  3.6018e-01,\n",
       "            9.5031e-01, -5.7608e-01]],\n",
       "\n",
       "         [[ 1.3445e+00,  2.9825e+00,  3.7641e+00,  ...,  6.1052e-01,\n",
       "            1.7436e+00, -7.5559e-01],\n",
       "          [-2.5664e+00,  1.8577e+00, -2.3088e+00,  ..., -2.6007e+00,\n",
       "            2.9176e+00,  4.7832e-01],\n",
       "          [-4.2860e+00,  1.8278e+00, -2.9421e+00,  ..., -3.2008e+00,\n",
       "            3.2367e+00,  1.6046e+00],\n",
       "          ...,\n",
       "          [-3.4037e+00, -1.0437e+00, -3.1948e+00,  ..., -3.1602e+00,\n",
       "            2.5466e+00,  5.1676e-01],\n",
       "          [-2.8415e+00, -1.0753e+00, -4.3714e+00,  ..., -3.2017e+00,\n",
       "            1.7096e+00,  1.3516e+00],\n",
       "          [-2.7551e+00, -1.5137e+00, -3.9670e+00,  ..., -3.6225e+00,\n",
       "            3.4464e+00,  3.9746e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3135e+00, -2.7654e+00, -2.6663e+00,  ...,  9.2731e-01,\n",
       "            4.4871e-01,  2.6998e+00],\n",
       "          [-1.9386e+00,  1.8491e+00,  5.1022e-01,  ..., -1.4189e+00,\n",
       "           -2.0752e+00,  4.3427e-01],\n",
       "          [-1.9669e+00,  3.0340e+00,  3.6986e-01,  ..., -9.3247e-01,\n",
       "           -3.8082e+00,  6.0508e-01],\n",
       "          ...,\n",
       "          [-2.7395e+00,  3.0635e+00,  2.7172e+00,  ..., -1.1117e+00,\n",
       "           -3.2739e+00, -1.3495e+00],\n",
       "          [-3.3788e+00,  3.0254e+00,  1.7789e+00,  ..., -1.5607e+00,\n",
       "           -4.0038e+00, -1.3500e+00],\n",
       "          [-2.8389e+00,  3.7216e+00,  1.3423e+00,  ..., -7.0708e-01,\n",
       "           -2.4312e+00, -1.2573e+00]],\n",
       "\n",
       "         [[ 1.7215e+00,  4.3022e-01,  9.1175e-01,  ...,  2.2430e-02,\n",
       "           -9.9444e-01, -2.9929e-01],\n",
       "          [ 2.4425e+00,  1.3172e+00,  1.3130e+00,  ...,  5.8109e-01,\n",
       "           -1.6885e+00, -1.6583e+00],\n",
       "          [ 2.2457e+00,  9.4304e-01,  7.6179e-01,  ...,  2.1877e-01,\n",
       "           -2.0103e+00, -1.3964e+00],\n",
       "          ...,\n",
       "          [ 2.4665e+00,  2.4808e-01,  1.2611e+00,  ..., -1.1304e-01,\n",
       "           -1.8157e+00, -9.6785e-01],\n",
       "          [ 2.6662e+00,  5.5367e-01,  1.3640e+00,  ...,  1.2659e-01,\n",
       "           -1.2640e+00, -1.3149e+00],\n",
       "          [ 1.9419e+00,  5.8484e-01,  8.8025e-01,  ...,  2.7264e-01,\n",
       "           -1.7975e+00, -1.2135e+00]],\n",
       "\n",
       "         [[-2.4389e-01,  1.4319e-01, -5.5898e-01,  ...,  2.8451e-01,\n",
       "            2.5435e-01,  1.7896e-01],\n",
       "          [-9.3461e-01,  1.2209e+00, -1.3204e-01,  ...,  5.2765e-01,\n",
       "            2.6827e-01, -4.7608e-01],\n",
       "          [-7.1074e-01, -2.0835e-01, -6.7116e-02,  ...,  1.9865e-01,\n",
       "            4.8088e-01,  4.1199e-02],\n",
       "          ...,\n",
       "          [-5.1032e-01,  4.5944e-01, -1.1081e+00,  ..., -5.2010e-01,\n",
       "            2.1935e-01, -3.0665e-01],\n",
       "          [-3.9373e-01,  2.0237e-01, -4.6803e-01,  ..., -2.4497e-01,\n",
       "            2.7374e-01, -3.8049e-01],\n",
       "          [-7.0341e-01, -7.4597e-01, -6.1379e-01,  ...,  7.2450e-01,\n",
       "            2.5086e-01,  8.3876e-01]]]]), tensor([[[[ 5.2398e-03, -3.1343e-02, -1.3422e-01,  ...,  2.1442e-03,\n",
       "            2.7118e-02, -5.3534e-01],\n",
       "          [-4.0810e-02, -1.2100e+00,  5.2776e-01,  ..., -7.5919e-01,\n",
       "            2.4475e-01,  8.2021e-01],\n",
       "          [ 5.1140e-01, -4.2827e-01, -3.3415e-02,  ..., -1.0961e+00,\n",
       "           -2.0376e-01,  5.9054e-01],\n",
       "          ...,\n",
       "          [-1.2443e+00, -3.1607e-01,  7.0336e-01,  ...,  4.5961e-02,\n",
       "           -4.7119e-01,  6.2298e-01],\n",
       "          [-2.3397e-01, -1.1770e-01,  1.0794e+00,  ...,  3.9040e-02,\n",
       "            1.0732e-01, -3.6109e-01],\n",
       "          [ 4.3043e-02, -1.2481e+00,  4.8381e-01,  ..., -4.0585e-01,\n",
       "            3.9375e-01,  3.6422e-01]],\n",
       "\n",
       "         [[ 2.9948e-02, -3.5371e-02,  5.8954e-02,  ..., -1.8629e-03,\n",
       "           -1.7093e-02,  1.3551e-02],\n",
       "          [-7.7392e-01, -5.3718e-01, -1.4205e-01,  ...,  4.0756e-01,\n",
       "           -2.7808e-01, -1.7180e+00],\n",
       "          [-6.3408e-02, -1.9890e-02,  7.8905e-02,  ..., -1.9102e-01,\n",
       "            3.5655e-01, -2.1807e-01],\n",
       "          ...,\n",
       "          [-7.1351e-02, -8.4453e-01, -3.7348e-01,  ..., -2.6075e-01,\n",
       "           -2.3408e-01, -1.0860e-01],\n",
       "          [-1.4008e-01, -4.3831e-01, -2.0735e-01,  ...,  5.4838e-01,\n",
       "            4.7910e-01, -4.2644e-01],\n",
       "          [ 2.3338e-01,  2.7975e-01, -2.8733e-02,  ...,  3.1976e-01,\n",
       "            4.1236e-01, -1.7367e-01]],\n",
       "\n",
       "         [[-1.1676e-02, -8.0466e-01, -5.3552e-02,  ...,  7.6077e-02,\n",
       "            1.9866e-02, -6.1520e-03],\n",
       "          [ 2.0300e-01, -1.4765e+00,  2.9981e-02,  ...,  3.3079e-01,\n",
       "            1.2464e-01, -2.6189e-02],\n",
       "          [ 1.4605e-01, -1.2693e+00,  6.7732e-02,  ...,  6.2897e-01,\n",
       "           -3.4779e-01,  3.9673e-01],\n",
       "          ...,\n",
       "          [-1.2543e+00, -1.3099e+00,  7.8352e-02,  ...,  3.0362e-02,\n",
       "            2.7049e-01,  5.6244e-01],\n",
       "          [-6.2207e-02, -1.7366e+00,  2.8328e-01,  ...,  3.8364e-02,\n",
       "            2.9805e-01,  4.7023e-01],\n",
       "          [ 5.8234e-01, -2.3705e+00, -2.9501e-01,  ...,  5.7395e-02,\n",
       "            3.1108e-02, -3.8590e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.6072e-02, -4.9973e-02,  1.3338e+00,  ..., -6.4964e-02,\n",
       "            1.9488e-01, -2.5954e-02],\n",
       "          [-1.9627e-01, -5.6560e-01,  1.1216e+00,  ...,  2.4393e-01,\n",
       "           -7.5145e-02,  1.3394e-01],\n",
       "          [-4.3467e-02, -5.6885e-01,  2.2688e+00,  ...,  5.4806e-01,\n",
       "            2.0354e-01,  1.0379e-01],\n",
       "          ...,\n",
       "          [-2.2054e-01,  1.4413e-01,  1.5808e+00,  ...,  7.4410e-01,\n",
       "           -3.5720e-01, -3.4923e-01],\n",
       "          [ 7.3439e-02, -2.7247e-01,  9.1473e-01,  ..., -1.8255e-01,\n",
       "           -8.3869e-02,  2.2170e-01],\n",
       "          [ 9.5330e-02, -7.5723e-01,  1.0730e+00,  ..., -4.7344e-01,\n",
       "           -3.2684e-01,  2.9060e-02]],\n",
       "\n",
       "         [[ 3.0375e-02, -1.1782e-01, -1.7403e-01,  ...,  1.1441e-01,\n",
       "            1.2647e-01,  1.5940e-01],\n",
       "          [ 9.1918e-01,  2.9815e-01, -1.4740e-01,  ..., -1.4741e-01,\n",
       "            1.3747e-02, -9.4514e-01],\n",
       "          [ 4.1133e-01, -7.4231e-02,  1.2102e-01,  ...,  2.2930e-01,\n",
       "           -5.6079e-01, -1.3622e-01],\n",
       "          ...,\n",
       "          [ 5.9877e-01, -2.3583e-01, -1.0586e+00,  ...,  3.5421e-01,\n",
       "            2.6404e-01, -3.0795e-01],\n",
       "          [-6.8465e-02,  1.5378e-01, -6.9412e-01,  ...,  4.1921e-01,\n",
       "            1.3158e-01, -6.0454e-01],\n",
       "          [ 2.6811e-01,  1.0384e-01, -1.4063e-01,  ...,  4.0467e-01,\n",
       "           -2.8265e-01, -7.3787e-01]],\n",
       "\n",
       "         [[ 1.5524e-02,  3.4123e-02,  3.3676e-02,  ..., -3.9930e-02,\n",
       "            2.0190e-01,  2.0927e-02],\n",
       "          [-1.8322e-01, -1.9041e-01, -6.7871e-01,  ..., -4.1554e-01,\n",
       "           -2.3852e+00, -9.1285e-02],\n",
       "          [ 4.0676e-02, -5.5553e-01, -9.3332e-02,  ...,  9.1142e-02,\n",
       "           -1.7874e+00,  6.2312e-01],\n",
       "          ...,\n",
       "          [ 2.8678e-01,  2.3491e-02, -8.4057e-01,  ...,  4.8435e-01,\n",
       "           -1.7816e+00, -1.6313e-01],\n",
       "          [ 9.8396e-01,  4.6629e-01,  2.0335e-01,  ...,  1.3732e+00,\n",
       "           -2.3059e+00, -2.0849e-01],\n",
       "          [ 3.1638e-01, -1.9452e-01, -1.2212e-01,  ..., -2.3979e-02,\n",
       "           -2.0892e+00, -1.4688e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.2398e-03, -3.1343e-02, -1.3422e-01,  ...,  2.1442e-03,\n",
       "            2.7118e-02, -5.3534e-01],\n",
       "          [-4.0810e-02, -1.2100e+00,  5.2776e-01,  ..., -7.5919e-01,\n",
       "            2.4475e-01,  8.2021e-01],\n",
       "          [ 5.1140e-01, -4.2827e-01, -3.3415e-02,  ..., -1.0961e+00,\n",
       "           -2.0376e-01,  5.9054e-01],\n",
       "          ...,\n",
       "          [-4.4002e-01,  5.3140e-02,  1.3210e+00,  ...,  2.3207e-01,\n",
       "           -2.8019e-01,  4.7560e-01],\n",
       "          [ 6.2199e-03, -1.3043e+00,  5.4387e-01,  ..., -3.4609e-01,\n",
       "            4.8736e-01,  3.6964e-01],\n",
       "          [ 2.6230e-01,  5.9830e-01, -1.3188e+00,  ...,  4.9116e-01,\n",
       "           -8.0048e-02, -6.1241e-01]],\n",
       "\n",
       "         [[ 2.9948e-02, -3.5371e-02,  5.8954e-02,  ..., -1.8629e-03,\n",
       "           -1.7093e-02,  1.3551e-02],\n",
       "          [-7.7392e-01, -5.3718e-01, -1.4205e-01,  ...,  4.0756e-01,\n",
       "           -2.7808e-01, -1.7180e+00],\n",
       "          [-6.3408e-02, -1.9890e-02,  7.8905e-02,  ..., -1.9102e-01,\n",
       "            3.5655e-01, -2.1807e-01],\n",
       "          ...,\n",
       "          [ 2.6394e-01, -6.2961e-01,  3.3520e-01,  ..., -5.8260e-01,\n",
       "            1.0910e-01, -2.9273e-01],\n",
       "          [ 2.4038e-01,  3.3739e-01,  1.6931e-01,  ...,  2.1979e-01,\n",
       "            4.1917e-01, -3.5655e-01],\n",
       "          [ 4.6229e-01, -4.8364e-01,  6.3936e-01,  ...,  1.9964e-01,\n",
       "            8.9579e-01, -5.4244e-01]],\n",
       "\n",
       "         [[-1.1676e-02, -8.0466e-01, -5.3552e-02,  ...,  7.6077e-02,\n",
       "            1.9866e-02, -6.1520e-03],\n",
       "          [ 2.0300e-01, -1.4765e+00,  2.9981e-02,  ...,  3.3079e-01,\n",
       "            1.2464e-01, -2.6189e-02],\n",
       "          [ 1.4605e-01, -1.2693e+00,  6.7732e-02,  ...,  6.2897e-01,\n",
       "           -3.4779e-01,  3.9673e-01],\n",
       "          ...,\n",
       "          [-9.4692e-01, -2.2932e+00,  2.7034e-01,  ..., -2.3373e-01,\n",
       "            2.5472e-02,  5.5686e-01],\n",
       "          [ 6.6177e-01, -2.3674e+00, -3.1458e-01,  ...,  1.2943e-01,\n",
       "           -2.2924e-02, -3.2887e-01],\n",
       "          [ 3.9794e-01, -1.4228e+00, -1.2863e-01,  ..., -8.0167e-02,\n",
       "            2.2878e-01, -1.4653e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 8.6072e-02, -4.9973e-02,  1.3338e+00,  ..., -6.4964e-02,\n",
       "            1.9488e-01, -2.5954e-02],\n",
       "          [-1.9627e-01, -5.6560e-01,  1.1216e+00,  ...,  2.4393e-01,\n",
       "           -7.5145e-02,  1.3394e-01],\n",
       "          [-4.3467e-02, -5.6885e-01,  2.2688e+00,  ...,  5.4806e-01,\n",
       "            2.0354e-01,  1.0379e-01],\n",
       "          ...,\n",
       "          [-4.6580e-01,  2.9276e-02,  7.4739e-01,  ..., -8.7272e-04,\n",
       "            2.3361e-01, -4.0416e-02],\n",
       "          [ 7.5839e-02, -7.6596e-01,  1.0352e+00,  ..., -6.3650e-01,\n",
       "           -2.5624e-01,  1.5754e-01],\n",
       "          [ 2.7985e-01, -8.1924e-01,  2.3462e+00,  ..., -1.6723e-01,\n",
       "            1.7951e-01,  2.3704e-01]],\n",
       "\n",
       "         [[ 3.0375e-02, -1.1782e-01, -1.7403e-01,  ...,  1.1441e-01,\n",
       "            1.2647e-01,  1.5940e-01],\n",
       "          [ 9.1918e-01,  2.9815e-01, -1.4740e-01,  ..., -1.4741e-01,\n",
       "            1.3747e-02, -9.4514e-01],\n",
       "          [ 4.1133e-01, -7.4231e-02,  1.2102e-01,  ...,  2.2930e-01,\n",
       "           -5.6079e-01, -1.3622e-01],\n",
       "          ...,\n",
       "          [ 4.1421e-01,  3.0699e-01,  7.3340e-02,  ...,  4.4702e-01,\n",
       "           -1.5260e-02, -5.3836e-01],\n",
       "          [ 3.1891e-01,  2.4275e-01,  9.7067e-02,  ...,  4.1216e-01,\n",
       "           -3.2070e-01, -7.6211e-01],\n",
       "          [ 8.3264e-01,  8.3865e-02,  3.5411e-01,  ..., -4.0828e-01,\n",
       "           -1.8442e-02,  3.5842e-01]],\n",
       "\n",
       "         [[ 1.5524e-02,  3.4123e-02,  3.3676e-02,  ..., -3.9930e-02,\n",
       "            2.0190e-01,  2.0927e-02],\n",
       "          [-1.8322e-01, -1.9041e-01, -6.7871e-01,  ..., -4.1554e-01,\n",
       "           -2.3852e+00, -9.1285e-02],\n",
       "          [ 4.0676e-02, -5.5553e-01, -9.3332e-02,  ...,  9.1142e-02,\n",
       "           -1.7874e+00,  6.2312e-01],\n",
       "          ...,\n",
       "          [ 8.3766e-02, -2.6277e-01, -6.6492e-01,  ..., -1.8841e-01,\n",
       "           -1.9686e+00, -1.6745e-01],\n",
       "          [ 2.8128e-01, -3.2431e-01, -5.4162e-02,  ...,  5.2444e-02,\n",
       "           -2.0687e+00, -1.7532e-01],\n",
       "          [ 4.0628e-03, -1.6619e-01,  3.9660e-01,  ..., -1.6386e-01,\n",
       "           -1.5347e+00, -2.9505e-01]]]])), (tensor([[[[ 4.2366e-02, -2.0151e-01,  1.5138e-01,  ..., -9.0207e-01,\n",
       "            7.2227e-01, -1.2004e+00],\n",
       "          [-9.2878e-01, -5.7103e-01,  3.4135e-03,  ...,  1.0973e+00,\n",
       "           -1.1441e-01,  1.2101e-01],\n",
       "          [-1.0288e+00,  4.4386e-01, -1.1608e+00,  ..., -2.9931e-01,\n",
       "           -8.6297e-01,  7.8930e-01],\n",
       "          ...,\n",
       "          [ 2.2064e+00, -6.0815e-02, -8.7116e-02,  ...,  5.9154e-01,\n",
       "           -3.8169e-01,  9.4529e-01],\n",
       "          [-8.0746e-02,  4.4523e-01, -2.0555e+00,  ...,  6.2763e-01,\n",
       "           -1.4260e+00,  1.9036e+00],\n",
       "          [ 8.2293e-02,  2.0698e+00,  4.7141e-01,  ...,  6.9417e-01,\n",
       "           -8.8571e-01,  1.7038e+00]],\n",
       "\n",
       "         [[ 7.9740e-01,  2.0673e-01,  3.1165e-02,  ..., -1.6713e-01,\n",
       "           -1.0898e+00, -2.0697e-01],\n",
       "          [ 1.3408e-01, -2.3038e+00, -5.0009e-01,  ...,  1.6309e+00,\n",
       "            3.6002e+00,  1.1724e+00],\n",
       "          [ 6.2238e-02, -2.1347e+00,  1.6717e+00,  ...,  1.2597e+00,\n",
       "            5.0623e+00,  2.2852e+00],\n",
       "          ...,\n",
       "          [ 2.2006e+00, -7.2046e-01, -5.6879e-01,  ...,  7.8954e-01,\n",
       "            5.4914e+00,  1.2856e+00],\n",
       "          [ 2.6513e+00, -9.5895e-01, -1.2733e+00,  ..., -1.8833e-01,\n",
       "            6.2036e+00,  1.0396e+00],\n",
       "          [ 3.3781e-01, -1.0156e-01, -5.0633e-01,  ...,  9.3775e-01,\n",
       "            5.2112e+00,  1.3960e+00]],\n",
       "\n",
       "         [[ 3.4062e-01, -3.6801e-01, -3.3386e-01,  ...,  3.2479e-01,\n",
       "            1.4316e+00,  2.7098e-01],\n",
       "          [ 1.1170e-03, -4.9199e+00, -1.1398e+00,  ..., -4.1479e+00,\n",
       "           -3.1047e+00, -4.1743e+00],\n",
       "          [-3.3923e-01, -6.8985e+00, -2.2174e+00,  ..., -4.5991e+00,\n",
       "           -3.7263e+00, -5.3011e+00],\n",
       "          ...,\n",
       "          [-2.2016e+00, -6.6137e+00, -2.2216e+00,  ..., -3.8960e+00,\n",
       "           -3.2671e+00, -4.6976e+00],\n",
       "          [-2.7376e+00, -5.9489e+00, -3.0321e+00,  ..., -4.3443e+00,\n",
       "           -2.9791e+00, -6.9053e+00],\n",
       "          [-1.7347e+00, -5.3949e+00, -1.8640e+00,  ..., -4.3466e+00,\n",
       "           -4.2767e+00, -4.9603e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2028e-01,  1.7825e+00,  5.1902e-01,  ...,  2.5925e-01,\n",
       "            4.6020e-01, -1.6673e+00],\n",
       "          [ 1.4196e+00, -5.3747e+00,  1.3096e+00,  ..., -2.7075e+00,\n",
       "           -2.5569e+00,  7.0062e+00],\n",
       "          [ 5.0108e-01, -6.9101e+00,  2.4904e+00,  ..., -1.9095e+00,\n",
       "           -2.6589e+00,  5.7207e+00],\n",
       "          ...,\n",
       "          [-6.1131e-01, -5.3374e+00,  2.8775e-01,  ..., -7.6423e-01,\n",
       "           -2.8585e+00,  8.4309e+00],\n",
       "          [ 1.1880e+00, -5.8075e+00,  1.3343e+00,  ..., -1.3684e+00,\n",
       "           -2.6726e+00,  8.7848e+00],\n",
       "          [ 3.7553e-01, -5.7763e+00,  3.0415e-01,  ..., -1.7385e+00,\n",
       "           -4.0828e+00,  7.1318e+00]],\n",
       "\n",
       "         [[ 5.3913e-02, -3.1064e-02,  1.3285e-01,  ..., -1.0906e-01,\n",
       "           -1.0876e-01, -1.4594e-01],\n",
       "          [ 8.1850e-01, -1.8365e+00, -1.2974e+00,  ..., -1.4024e+00,\n",
       "            9.2996e-01, -9.5476e-01],\n",
       "          [-8.6066e-01, -9.7050e-01, -7.2446e-01,  ..., -1.2289e+00,\n",
       "            6.1451e-01, -7.9465e-01],\n",
       "          ...,\n",
       "          [ 1.6716e+00, -2.0115e+00,  3.6674e-02,  ..., -4.2263e-01,\n",
       "            4.0759e-01,  9.9626e-01],\n",
       "          [ 2.0114e+00, -6.1724e-01, -1.7124e+00,  ..., -2.3069e-02,\n",
       "           -1.3275e+00, -4.3615e-01],\n",
       "          [ 1.4786e+00, -5.6639e-01, -1.2480e+00,  ..., -4.3959e-01,\n",
       "           -4.8962e-01, -9.5941e-01]],\n",
       "\n",
       "         [[ 3.9027e-01, -5.9695e-02,  1.9163e+00,  ..., -2.2390e-01,\n",
       "           -2.0639e-01, -9.8867e-01],\n",
       "          [ 1.8556e+00,  9.0667e-01, -1.8904e+00,  ...,  1.2717e+00,\n",
       "           -2.4058e-01,  4.4461e+00],\n",
       "          [ 3.3789e+00,  2.6828e+00, -3.0208e+00,  ...,  1.1801e+00,\n",
       "            9.0049e-01,  3.6671e+00],\n",
       "          ...,\n",
       "          [ 1.7100e+00,  1.6064e+00, -1.9098e+00,  ...,  1.5038e+00,\n",
       "           -2.3042e+00,  4.5287e+00],\n",
       "          [ 3.5705e+00,  1.1678e+00, -3.7527e+00,  ...,  1.6213e+00,\n",
       "           -8.8310e-02,  6.5110e+00],\n",
       "          [ 2.6376e+00,  8.5070e-01, -1.8144e+00,  ...,  1.6581e+00,\n",
       "           -1.2111e-01,  5.5432e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2366e-02, -2.0151e-01,  1.5138e-01,  ..., -9.0207e-01,\n",
       "            7.2227e-01, -1.2004e+00],\n",
       "          [-9.2878e-01, -5.7103e-01,  3.4135e-03,  ...,  1.0973e+00,\n",
       "           -1.1441e-01,  1.2101e-01],\n",
       "          [-1.0288e+00,  4.4386e-01, -1.1608e+00,  ..., -2.9931e-01,\n",
       "           -8.6297e-01,  7.8930e-01],\n",
       "          ...,\n",
       "          [-4.1841e-01,  1.3069e+00,  1.1736e+00,  ...,  1.3575e+00,\n",
       "           -5.9396e-02,  2.0170e+00],\n",
       "          [ 1.2582e-02,  2.1857e+00,  4.9832e-01,  ...,  8.4497e-01,\n",
       "           -7.2099e-01,  1.7883e+00],\n",
       "          [-5.6066e-01, -2.7639e-01,  9.7431e-01,  ...,  1.3617e+00,\n",
       "           -7.9099e-02, -8.0843e-01]],\n",
       "\n",
       "         [[ 7.9740e-01,  2.0673e-01,  3.1165e-02,  ..., -1.6713e-01,\n",
       "           -1.0898e+00, -2.0697e-01],\n",
       "          [ 1.3408e-01, -2.3038e+00, -5.0009e-01,  ...,  1.6309e+00,\n",
       "            3.6002e+00,  1.1724e+00],\n",
       "          [ 6.2238e-02, -2.1347e+00,  1.6717e+00,  ...,  1.2597e+00,\n",
       "            5.0623e+00,  2.2852e+00],\n",
       "          ...,\n",
       "          [ 1.4783e+00, -8.4816e-01, -6.4978e-03,  ...,  4.9345e-01,\n",
       "            5.7267e+00,  1.5242e+00],\n",
       "          [ 4.6295e-01, -3.0940e-01, -3.4670e-01,  ...,  1.0632e+00,\n",
       "            5.3342e+00,  1.6970e+00],\n",
       "          [-6.5663e-01, -5.4220e-02, -9.8433e-03,  ..., -2.9808e-01,\n",
       "            3.8817e+00,  9.3570e-01]],\n",
       "\n",
       "         [[ 3.4062e-01, -3.6801e-01, -3.3386e-01,  ...,  3.2479e-01,\n",
       "            1.4316e+00,  2.7098e-01],\n",
       "          [ 1.1170e-03, -4.9199e+00, -1.1398e+00,  ..., -4.1479e+00,\n",
       "           -3.1047e+00, -4.1743e+00],\n",
       "          [-3.3923e-01, -6.8985e+00, -2.2174e+00,  ..., -4.5991e+00,\n",
       "           -3.7263e+00, -5.3011e+00],\n",
       "          ...,\n",
       "          [-1.8469e+00, -6.9368e+00, -1.6030e+00,  ..., -4.1142e+00,\n",
       "           -3.4556e+00, -4.6194e+00],\n",
       "          [-1.4823e+00, -5.4725e+00, -1.8095e+00,  ..., -4.0388e+00,\n",
       "           -4.1491e+00, -5.1437e+00],\n",
       "          [-2.2814e+00, -5.9001e+00, -2.0366e+00,  ..., -3.2270e+00,\n",
       "           -1.8784e+00, -5.0171e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2028e-01,  1.7825e+00,  5.1902e-01,  ...,  2.5925e-01,\n",
       "            4.6020e-01, -1.6673e+00],\n",
       "          [ 1.4196e+00, -5.3747e+00,  1.3096e+00,  ..., -2.7075e+00,\n",
       "           -2.5569e+00,  7.0062e+00],\n",
       "          [ 5.0108e-01, -6.9101e+00,  2.4904e+00,  ..., -1.9095e+00,\n",
       "           -2.6589e+00,  5.7207e+00],\n",
       "          ...,\n",
       "          [ 2.4664e-02, -5.8310e+00,  1.5096e+00,  ..., -1.5934e+00,\n",
       "           -1.9835e+00,  7.5221e+00],\n",
       "          [ 1.7639e-01, -5.6453e+00,  4.0293e-01,  ..., -1.7457e+00,\n",
       "           -3.4881e+00,  7.2578e+00],\n",
       "          [ 1.4905e+00, -6.3492e+00,  1.4569e+00,  ..., -2.4014e+00,\n",
       "           -1.5231e+00,  4.6704e+00]],\n",
       "\n",
       "         [[ 5.3913e-02, -3.1064e-02,  1.3285e-01,  ..., -1.0906e-01,\n",
       "           -1.0876e-01, -1.4594e-01],\n",
       "          [ 8.1850e-01, -1.8365e+00, -1.2974e+00,  ..., -1.4024e+00,\n",
       "            9.2996e-01, -9.5476e-01],\n",
       "          [-8.6066e-01, -9.7050e-01, -7.2446e-01,  ..., -1.2289e+00,\n",
       "            6.1451e-01, -7.9465e-01],\n",
       "          ...,\n",
       "          [ 1.2605e+00, -1.3663e+00, -5.8716e-01,  ..., -7.3056e-02,\n",
       "            2.6413e-01, -4.5595e-01],\n",
       "          [ 1.2284e+00, -7.8519e-01, -1.4231e+00,  ..., -1.1463e-01,\n",
       "           -5.8378e-01, -1.4225e+00],\n",
       "          [ 3.6634e-01, -5.9898e-01, -1.9543e-01,  ..., -9.0320e-01,\n",
       "           -1.5591e-01, -1.5509e-01]],\n",
       "\n",
       "         [[ 3.9027e-01, -5.9695e-02,  1.9163e+00,  ..., -2.2390e-01,\n",
       "           -2.0639e-01, -9.8867e-01],\n",
       "          [ 1.8556e+00,  9.0667e-01, -1.8904e+00,  ...,  1.2717e+00,\n",
       "           -2.4058e-01,  4.4461e+00],\n",
       "          [ 3.3789e+00,  2.6828e+00, -3.0208e+00,  ...,  1.1801e+00,\n",
       "            9.0049e-01,  3.6671e+00],\n",
       "          ...,\n",
       "          [ 2.8287e+00,  6.4308e-01, -3.8140e+00,  ...,  1.5412e+00,\n",
       "           -5.9330e-01,  6.0161e+00],\n",
       "          [ 2.8514e+00,  1.2059e+00, -1.9965e+00,  ...,  1.7957e+00,\n",
       "            6.1337e-03,  5.6684e+00],\n",
       "          [ 2.9732e+00,  2.2829e+00, -3.1000e+00,  ...,  3.2534e-01,\n",
       "            8.2795e-01,  3.8759e+00]]]]), tensor([[[[ 0.0462,  0.0606, -0.0118,  ...,  0.0164,  0.1049,  0.0236],\n",
       "          [ 0.5052, -0.8859,  0.2985,  ..., -0.1993, -0.3406, -0.6794],\n",
       "          [ 0.5621, -0.7286, -0.5456,  ...,  0.6548, -0.9246, -0.5041],\n",
       "          ...,\n",
       "          [ 0.0686, -0.9389,  0.0229,  ..., -0.3273,  0.0266, -0.4043],\n",
       "          [ 0.1362, -0.9394, -0.8256,  ...,  0.0923, -0.9081, -0.8596],\n",
       "          [ 0.7719, -1.1262,  0.2137,  ...,  0.5201, -1.0817, -0.5569]],\n",
       "\n",
       "         [[-0.0370, -0.0060,  0.0860,  ..., -0.0481, -0.0321, -0.0415],\n",
       "          [-0.0224, -0.0172,  0.1759,  ..., -0.5661,  0.2182,  0.1458],\n",
       "          [ 0.5752,  0.2640,  0.4476,  ...,  0.2551,  0.0408,  0.0593],\n",
       "          ...,\n",
       "          [-0.4165,  0.3412, -0.1929,  ..., -0.0209,  0.3477, -0.0054],\n",
       "          [ 0.2297, -0.3659,  0.1500,  ..., -0.3481,  1.0878,  0.3304],\n",
       "          [ 0.0649, -0.7847, -0.6071,  ..., -0.0392,  1.5853, -0.6524]],\n",
       "\n",
       "         [[ 0.0379, -0.1080, -0.0521,  ..., -0.0238,  0.0912, -0.1588],\n",
       "          [-0.6998, -0.8556, -0.1666,  ..., -0.2326, -0.4881, -0.2475],\n",
       "          [-0.3331, -0.2729, -0.7333,  ...,  0.3359,  0.1084,  0.2180],\n",
       "          ...,\n",
       "          [ 0.2392, -0.1930, -0.3489,  ...,  0.0059, -0.3887,  0.3934],\n",
       "          [-0.8686, -0.0324, -0.1476,  ..., -0.0815, -0.2722,  0.5383],\n",
       "          [ 0.3240, -0.0714, -0.1343,  ..., -0.1090, -0.3835,  0.1840]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0240,  0.1224, -0.0202,  ..., -0.0300,  0.0600, -0.0402],\n",
       "          [ 1.3650, -0.7501,  0.1402,  ...,  0.1321, -0.5979, -0.2270],\n",
       "          [ 0.2362,  0.0888, -0.0963,  ...,  0.0641, -0.0712,  0.0394],\n",
       "          ...,\n",
       "          [ 0.4038, -0.7945,  0.1278,  ...,  0.5567, -0.3896,  0.2101],\n",
       "          [-0.1617, -0.6945, -0.0284,  ..., -0.0714, -0.2543, -0.0688],\n",
       "          [ 0.4191, -0.5167, -0.1785,  ...,  0.0852,  0.4338,  0.5481]],\n",
       "\n",
       "         [[-0.1720, -0.1402, -0.0733,  ..., -0.2602, -0.0069, -0.0317],\n",
       "          [ 0.3568, -1.1721,  1.3779,  ...,  0.1305, -0.0797,  1.4384],\n",
       "          [ 0.5992, -0.6912,  0.8325,  ...,  0.1693,  0.0202,  0.9524],\n",
       "          ...,\n",
       "          [ 0.4679,  0.3428,  1.9814,  ...,  0.5290, -0.2773,  0.6568],\n",
       "          [ 1.0661,  1.1537,  0.2630,  ...,  0.4063,  0.4897, -0.4554],\n",
       "          [ 0.3185,  0.0737,  0.8240,  ...,  0.0568,  0.9372,  0.6445]],\n",
       "\n",
       "         [[ 0.1171, -0.0691, -0.0401,  ..., -0.0101, -0.0916, -0.1016],\n",
       "          [-0.0882,  0.4114, -0.6870,  ...,  0.5879, -0.4463, -0.5986],\n",
       "          [ 0.0721,  0.0632,  0.0686,  ..., -0.4381, -0.6824,  0.1972],\n",
       "          ...,\n",
       "          [-0.4168,  0.0365,  0.2205,  ..., -0.1574,  0.6350, -1.0080],\n",
       "          [-0.3358,  0.0623,  0.6005,  ...,  0.6748,  0.3817, -0.3028],\n",
       "          [-0.1548,  0.0526,  0.0687,  ...,  0.4152, -0.2725, -0.0806]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0462,  0.0606, -0.0118,  ...,  0.0164,  0.1049,  0.0236],\n",
       "          [ 0.5052, -0.8859,  0.2985,  ..., -0.1993, -0.3406, -0.6794],\n",
       "          [ 0.5621, -0.7286, -0.5456,  ...,  0.6548, -0.9246, -0.5041],\n",
       "          ...,\n",
       "          [ 0.2357, -0.6384, -0.5510,  ..., -0.3614, -0.1231, -1.0349],\n",
       "          [ 0.8216, -0.9736,  0.2836,  ...,  0.3743, -1.1566, -0.8302],\n",
       "          [ 0.4892, -0.4693,  0.4004,  ...,  0.0359, -0.7215,  0.0329]],\n",
       "\n",
       "         [[-0.0370, -0.0060,  0.0860,  ..., -0.0481, -0.0321, -0.0415],\n",
       "          [-0.0224, -0.0172,  0.1759,  ..., -0.5661,  0.2182,  0.1458],\n",
       "          [ 0.5752,  0.2640,  0.4476,  ...,  0.2551,  0.0408,  0.0593],\n",
       "          ...,\n",
       "          [-0.0980, -0.0441,  0.4770,  ..., -0.0451,  0.9228,  0.0603],\n",
       "          [ 0.1290, -0.6224, -0.5038,  ...,  0.1079,  1.4721, -0.8357],\n",
       "          [-0.2517, -0.1776, -0.0662,  ...,  0.3674, -0.0999, -0.3257]],\n",
       "\n",
       "         [[ 0.0379, -0.1080, -0.0521,  ..., -0.0238,  0.0912, -0.1588],\n",
       "          [-0.6998, -0.8556, -0.1666,  ..., -0.2326, -0.4881, -0.2475],\n",
       "          [-0.3331, -0.2729, -0.7333,  ...,  0.3359,  0.1084,  0.2180],\n",
       "          ...,\n",
       "          [ 0.4124, -0.0480, -0.2980,  ...,  0.1347, -0.8052, -0.1216],\n",
       "          [ 0.1733,  0.0343, -0.0658,  ..., -0.0232, -0.3383,  0.2128],\n",
       "          [ 0.3982,  0.5500, -0.7470,  ...,  0.0511,  0.0229, -0.2001]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0240,  0.1224, -0.0202,  ..., -0.0300,  0.0600, -0.0402],\n",
       "          [ 1.3650, -0.7501,  0.1402,  ...,  0.1321, -0.5979, -0.2270],\n",
       "          [ 0.2362,  0.0888, -0.0963,  ...,  0.0641, -0.0712,  0.0394],\n",
       "          ...,\n",
       "          [ 0.3255, -0.1735, -0.5254,  ..., -0.3457,  0.1392, -0.4045],\n",
       "          [ 0.2060, -0.4866, -0.1222,  ...,  0.3207,  0.3841,  0.4049],\n",
       "          [ 0.6318, -0.1536,  0.5999,  ...,  0.4425,  0.1532, -0.3278]],\n",
       "\n",
       "         [[-0.1720, -0.1402, -0.0733,  ..., -0.2602, -0.0069, -0.0317],\n",
       "          [ 0.3568, -1.1721,  1.3779,  ...,  0.1305, -0.0797,  1.4384],\n",
       "          [ 0.5992, -0.6912,  0.8325,  ...,  0.1693,  0.0202,  0.9524],\n",
       "          ...,\n",
       "          [ 0.6478,  0.1221,  1.1135,  ...,  0.5086,  0.3427,  0.3872],\n",
       "          [ 0.6315, -0.1036,  0.9566,  ...,  0.1442,  1.0635,  0.2900],\n",
       "          [ 0.5724,  0.6639, -0.0398,  ..., -0.4357,  0.3034,  0.1011]],\n",
       "\n",
       "         [[ 0.1171, -0.0691, -0.0401,  ..., -0.0101, -0.0916, -0.1016],\n",
       "          [-0.0882,  0.4114, -0.6870,  ...,  0.5879, -0.4463, -0.5986],\n",
       "          [ 0.0721,  0.0632,  0.0686,  ..., -0.4381, -0.6824,  0.1972],\n",
       "          ...,\n",
       "          [-1.0467, -0.4053, -0.3132,  ...,  1.0175,  0.3323, -0.4922],\n",
       "          [-0.1076, -0.1300,  0.0499,  ...,  0.4810, -0.1411,  0.0181],\n",
       "          [ 0.5544,  0.0199, -0.0053,  ..., -0.3968,  0.0115,  0.5144]]]])), (tensor([[[[-8.8634e-01, -1.2741e-01,  3.3525e-01,  ..., -9.7193e-01,\n",
       "            1.3296e-02, -2.9506e+00],\n",
       "          [ 1.7445e+00, -2.6433e-01, -2.2375e+00,  ..., -2.4209e+00,\n",
       "           -1.5493e+00,  6.6878e+00],\n",
       "          [ 1.4227e+00,  1.2998e-01, -3.5650e+00,  ..., -1.8761e+00,\n",
       "           -2.5070e+00,  8.2701e+00],\n",
       "          ...,\n",
       "          [ 4.2660e-01,  3.4381e-01, -3.5046e+00,  ..., -2.7200e+00,\n",
       "           -2.6232e+00,  1.0812e+01],\n",
       "          [ 6.8735e-01, -1.2055e+00, -3.1191e+00,  ..., -2.3433e+00,\n",
       "           -2.8093e+00,  8.4622e+00],\n",
       "          [-2.2309e-01, -5.4647e-01, -2.8729e+00,  ..., -2.8185e-01,\n",
       "           -1.5160e+00,  8.6323e+00]],\n",
       "\n",
       "         [[ 3.7392e-01, -4.8946e-02,  4.5849e-01,  ..., -1.3692e-01,\n",
       "           -7.6564e-02, -2.2149e+00],\n",
       "          [-1.6356e+00,  5.1355e-01,  2.8143e+00,  ..., -1.9159e+00,\n",
       "           -1.0938e+00,  6.4587e+00],\n",
       "          [-1.8601e+00,  1.1466e+00,  2.8777e+00,  ..., -1.0415e+00,\n",
       "           -1.3176e+00,  7.1011e+00],\n",
       "          ...,\n",
       "          [-1.9916e+00, -5.4894e-02,  3.0463e+00,  ..., -7.0684e-01,\n",
       "           -7.2943e-01,  5.5959e+00],\n",
       "          [-1.5392e+00,  3.5862e-01,  1.7634e+00,  ..., -5.9739e-01,\n",
       "           -1.1110e+00,  5.5085e+00],\n",
       "          [-3.3997e+00,  7.8509e-01,  3.5415e+00,  ..., -1.9083e+00,\n",
       "           -4.4499e-01,  4.8076e+00]],\n",
       "\n",
       "         [[ 1.2887e-01, -6.4301e-01, -2.1930e-01,  ...,  1.6649e-01,\n",
       "            2.6193e-01, -1.6482e-01],\n",
       "          [ 1.2288e-01,  1.9436e+00,  1.0919e+00,  ...,  1.2882e+00,\n",
       "            8.5136e-01,  8.2024e-01],\n",
       "          [-1.5390e-01,  2.3128e+00,  9.4490e-01,  ...,  9.7578e-01,\n",
       "            7.2407e-01,  2.5180e-01],\n",
       "          ...,\n",
       "          [ 5.3759e-01,  2.3207e+00, -1.0559e+00,  ...,  7.3857e-01,\n",
       "            1.2389e+00, -1.9015e-01],\n",
       "          [-1.1019e+00,  2.4956e+00, -4.2797e-02,  ...,  9.7101e-01,\n",
       "            1.0225e+00,  2.2550e-01],\n",
       "          [-4.1461e-01,  2.0684e+00, -4.4203e-01,  ...,  2.9144e-01,\n",
       "            7.4780e-01, -5.1814e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.1012e-01,  1.9586e-02,  2.1157e-03,  ...,  1.2636e+00,\n",
       "            5.7946e-02,  1.7798e+00],\n",
       "          [ 7.3691e-01, -1.9237e+00, -1.9676e+00,  ..., -1.0468e+00,\n",
       "           -1.8809e+00, -4.8448e-01],\n",
       "          [ 7.0753e-02, -1.2861e+00, -1.3465e+00,  ..., -2.1750e+00,\n",
       "           -6.4976e-01, -7.9380e-01],\n",
       "          ...,\n",
       "          [ 6.8312e-01, -3.8200e-01,  3.1117e-01,  ..., -2.1958e+00,\n",
       "           -6.4750e-01, -9.5564e-01],\n",
       "          [ 1.1031e+00,  1.8151e+00,  1.7402e+00,  ..., -2.8965e+00,\n",
       "           -9.8449e-01, -9.7134e-01],\n",
       "          [ 7.9350e-01,  7.0691e-01,  3.0002e-01,  ..., -1.4375e+00,\n",
       "           -1.2902e+00, -9.0790e-01]],\n",
       "\n",
       "         [[-3.4512e-01, -1.2568e-01,  2.0433e-01,  ...,  2.6652e-01,\n",
       "           -3.4889e-02,  2.5611e-02],\n",
       "          [ 1.2005e+00, -4.1670e-01,  6.8137e-01,  ...,  1.0946e+00,\n",
       "            1.0826e+00, -6.1822e-01],\n",
       "          [ 3.2666e-01,  2.4284e-01, -3.5038e-02,  ...,  9.5797e-01,\n",
       "            7.5872e-01, -2.5475e-01],\n",
       "          ...,\n",
       "          [-1.3782e+00, -5.8392e-01, -6.6957e-01,  ...,  6.1246e-01,\n",
       "            8.3596e-01,  3.3907e-01],\n",
       "          [-1.3096e+00,  7.9166e-01, -9.0834e-01,  ...,  5.9547e-01,\n",
       "           -9.2671e-01,  9.2150e-01],\n",
       "          [-2.6463e-01,  1.9439e-02,  1.0144e-01,  ...,  6.6851e-01,\n",
       "            1.1403e+00, -6.9103e-01]],\n",
       "\n",
       "         [[ 3.4203e+00,  2.1306e+00, -2.1177e+00,  ..., -2.8719e+00,\n",
       "           -3.8684e+00, -1.1901e+00],\n",
       "          [-2.9591e+00, -1.7263e+00,  5.1974e+00,  ..., -2.6870e+00,\n",
       "            7.5958e+00, -9.8684e-01],\n",
       "          [-7.4693e+00, -3.9646e+00,  5.1026e+00,  ..., -1.7824e+00,\n",
       "            1.1478e+01, -2.6124e-01],\n",
       "          ...,\n",
       "          [-4.7504e+00, -2.3042e+00,  3.3681e+00,  ..., -3.7850e+00,\n",
       "            1.0003e+01,  7.1591e-01],\n",
       "          [-4.5830e+00, -4.4672e-01,  5.4224e+00,  ..., -2.3626e+00,\n",
       "            8.8377e+00, -2.2447e+00],\n",
       "          [-2.3501e+00, -1.0374e-01,  9.3060e+00,  ..., -2.3956e+00,\n",
       "            6.2525e+00, -8.5833e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.8634e-01, -1.2741e-01,  3.3525e-01,  ..., -9.7193e-01,\n",
       "            1.3296e-02, -2.9506e+00],\n",
       "          [ 1.7445e+00, -2.6433e-01, -2.2375e+00,  ..., -2.4209e+00,\n",
       "           -1.5493e+00,  6.6878e+00],\n",
       "          [ 1.4227e+00,  1.2998e-01, -3.5650e+00,  ..., -1.8761e+00,\n",
       "           -2.5070e+00,  8.2701e+00],\n",
       "          ...,\n",
       "          [-7.5435e-02, -5.1335e-01, -2.3560e+00,  ..., -1.9645e+00,\n",
       "           -2.1287e+00,  9.1252e+00],\n",
       "          [-2.9694e-02, -2.0304e-01, -2.9183e+00,  ...,  1.2379e-01,\n",
       "           -1.4668e+00,  8.9755e+00],\n",
       "          [ 6.2474e-01, -1.3529e+00, -2.5297e+00,  ..., -2.0787e+00,\n",
       "           -1.8154e+00,  7.8455e+00]],\n",
       "\n",
       "         [[ 3.7392e-01, -4.8946e-02,  4.5849e-01,  ..., -1.3692e-01,\n",
       "           -7.6564e-02, -2.2149e+00],\n",
       "          [-1.6356e+00,  5.1355e-01,  2.8143e+00,  ..., -1.9159e+00,\n",
       "           -1.0938e+00,  6.4587e+00],\n",
       "          [-1.8601e+00,  1.1466e+00,  2.8777e+00,  ..., -1.0415e+00,\n",
       "           -1.3176e+00,  7.1011e+00],\n",
       "          ...,\n",
       "          [-3.5303e+00, -4.0901e-01,  3.3384e+00,  ..., -1.3224e+00,\n",
       "           -1.5939e+00,  5.9811e+00],\n",
       "          [-3.8794e+00,  3.9458e-01,  3.8154e+00,  ..., -1.7878e+00,\n",
       "           -2.6092e-01,  4.8487e+00],\n",
       "          [-1.0321e+00,  8.1377e-01,  4.2163e+00,  ..., -6.1343e-01,\n",
       "           -5.0298e-01,  5.6061e+00]],\n",
       "\n",
       "         [[ 1.2887e-01, -6.4301e-01, -2.1930e-01,  ...,  1.6649e-01,\n",
       "            2.6193e-01, -1.6482e-01],\n",
       "          [ 1.2288e-01,  1.9436e+00,  1.0919e+00,  ...,  1.2882e+00,\n",
       "            8.5136e-01,  8.2024e-01],\n",
       "          [-1.5390e-01,  2.3128e+00,  9.4490e-01,  ...,  9.7578e-01,\n",
       "            7.2407e-01,  2.5180e-01],\n",
       "          ...,\n",
       "          [ 9.4597e-01,  3.6732e+00,  1.3976e-01,  ...,  1.6717e-01,\n",
       "           -2.5408e-01,  5.4767e-01],\n",
       "          [-4.9795e-01,  2.2946e+00, -2.3364e-01,  ...,  3.2897e-02,\n",
       "            5.6125e-01, -1.1503e-01],\n",
       "          [-6.2750e-02,  3.2031e+00, -3.3980e-02,  ..., -1.2077e+00,\n",
       "            2.5567e-01,  8.4144e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.1012e-01,  1.9586e-02,  2.1157e-03,  ...,  1.2636e+00,\n",
       "            5.7946e-02,  1.7798e+00],\n",
       "          [ 7.3691e-01, -1.9237e+00, -1.9676e+00,  ..., -1.0468e+00,\n",
       "           -1.8809e+00, -4.8448e-01],\n",
       "          [ 7.0753e-02, -1.2861e+00, -1.3465e+00,  ..., -2.1750e+00,\n",
       "           -6.4976e-01, -7.9380e-01],\n",
       "          ...,\n",
       "          [ 8.6568e-01, -8.7231e-01,  4.2665e-01,  ..., -2.1922e+00,\n",
       "           -9.7182e-01, -5.0270e-01],\n",
       "          [ 9.3564e-01,  1.5453e-01, -4.7873e-01,  ..., -1.5809e+00,\n",
       "           -1.0002e+00, -8.6923e-01],\n",
       "          [ 5.9715e-01, -1.0197e+00, -1.2079e+00,  ..., -3.2198e+00,\n",
       "           -9.2541e-01,  4.8082e-02]],\n",
       "\n",
       "         [[-3.4512e-01, -1.2568e-01,  2.0433e-01,  ...,  2.6652e-01,\n",
       "           -3.4889e-02,  2.5611e-02],\n",
       "          [ 1.2005e+00, -4.1670e-01,  6.8137e-01,  ...,  1.0946e+00,\n",
       "            1.0826e+00, -6.1822e-01],\n",
       "          [ 3.2666e-01,  2.4284e-01, -3.5038e-02,  ...,  9.5797e-01,\n",
       "            7.5872e-01, -2.5475e-01],\n",
       "          ...,\n",
       "          [-7.9845e-01,  1.1949e-01, -5.5257e-01,  ...,  2.5064e-01,\n",
       "            2.1075e-01, -1.1664e-01],\n",
       "          [-7.9072e-03,  2.2357e-01,  7.7857e-03,  ...,  7.9324e-01,\n",
       "            8.6778e-01, -8.5777e-01],\n",
       "          [-5.1733e-01, -1.0219e-01,  1.4039e+00,  ...,  1.2700e+00,\n",
       "           -8.2228e-02,  1.2454e+00]],\n",
       "\n",
       "         [[ 3.4203e+00,  2.1306e+00, -2.1177e+00,  ..., -2.8719e+00,\n",
       "           -3.8684e+00, -1.1901e+00],\n",
       "          [-2.9591e+00, -1.7263e+00,  5.1974e+00,  ..., -2.6870e+00,\n",
       "            7.5958e+00, -9.8684e-01],\n",
       "          [-7.4693e+00, -3.9646e+00,  5.1026e+00,  ..., -1.7824e+00,\n",
       "            1.1478e+01, -2.6124e-01],\n",
       "          ...,\n",
       "          [-2.5040e+00, -2.9436e+00,  5.7494e+00,  ..., -2.1083e+00,\n",
       "            8.2952e+00,  1.2264e+00],\n",
       "          [-2.4690e+00, -1.5931e+00,  9.7498e+00,  ..., -2.2073e+00,\n",
       "            7.3528e+00, -5.2789e-01],\n",
       "          [-6.2875e+00, -3.4899e+00,  1.0453e+01,  ..., -5.2967e-01,\n",
       "            9.6367e+00, -1.6094e+00]]]]), tensor([[[[-5.9413e-03, -5.1325e-02,  2.1233e-02,  ...,  5.8183e-02,\n",
       "            2.7543e-02,  7.3718e-02],\n",
       "          [-6.8224e-01, -3.5075e-01,  2.1327e-01,  ..., -5.7191e-01,\n",
       "            5.6553e-01, -6.4773e-01],\n",
       "          [ 5.4235e-01,  1.1635e-01, -5.7858e-01,  ..., -8.4099e-01,\n",
       "            7.2675e-03, -5.2381e-01],\n",
       "          ...,\n",
       "          [-3.1606e-01, -3.4750e-01, -4.6751e-01,  ..., -6.8005e-02,\n",
       "            8.2166e-01,  1.9525e-02],\n",
       "          [ 3.6074e-01, -4.0451e-01,  5.8718e-01,  ...,  2.3456e-01,\n",
       "           -5.7201e-02,  3.2041e-01],\n",
       "          [ 9.0813e-02, -4.0222e-02,  7.0150e-02,  ...,  1.8862e-01,\n",
       "            4.9243e-01, -1.9690e-01]],\n",
       "\n",
       "         [[-6.1497e-02, -1.2752e-02, -1.4568e-01,  ..., -4.8172e-02,\n",
       "            4.7348e-02, -2.4290e-02],\n",
       "          [ 1.3147e-01,  6.3121e-01, -3.5953e-01,  ...,  1.1897e-01,\n",
       "            1.0056e+00,  5.0474e-01],\n",
       "          [ 7.5941e-01,  3.6526e-01,  2.9423e-01,  ..., -2.5376e-01,\n",
       "            2.0285e-01, -1.3698e-01],\n",
       "          ...,\n",
       "          [-9.0494e-01,  7.2794e-01, -2.8529e-01,  ...,  6.8825e-01,\n",
       "           -1.8926e-01,  6.6767e-02],\n",
       "          [-4.5001e-01,  6.5209e-01,  2.0510e-01,  ...,  2.0697e-02,\n",
       "           -1.1903e-02, -4.5393e-01],\n",
       "          [-1.7863e-01,  2.4241e-01, -1.4379e-01,  ...,  4.0304e-01,\n",
       "            4.0498e-01,  2.3119e-01]],\n",
       "\n",
       "         [[ 7.0930e-02,  1.0169e-01,  9.1266e-02,  ...,  8.8514e-03,\n",
       "           -8.6669e-02, -4.9571e-03],\n",
       "          [-1.3191e-01, -6.2046e-02,  2.0125e-01,  ...,  6.1535e-01,\n",
       "            1.4052e+00,  4.1019e-01],\n",
       "          [-1.8832e-01,  5.7359e-01, -3.5219e-01,  ...,  5.9092e-01,\n",
       "           -7.1859e-01,  4.5427e-01],\n",
       "          ...,\n",
       "          [-1.2907e-01,  8.3319e-01,  6.2682e-01,  ...,  8.5798e-01,\n",
       "            5.7819e-01, -1.0753e-01],\n",
       "          [ 6.8188e-01,  2.8558e-01,  1.1029e+00,  ...,  6.3688e-01,\n",
       "            8.9107e-01,  7.4493e-02],\n",
       "          [ 2.9096e-01,  1.2166e+00,  4.1716e-01,  ..., -1.5627e-01,\n",
       "            1.5755e-01,  2.9076e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8660e-02,  7.2212e-02, -8.0057e-02,  ...,  4.5701e-02,\n",
       "            3.9052e-02, -1.3910e-01],\n",
       "          [-1.1430e-01, -3.6511e-01,  1.4372e-01,  ..., -2.7121e-01,\n",
       "           -4.0517e-01,  6.1041e-01],\n",
       "          [-6.9271e-02,  4.4875e-01, -2.1989e-01,  ..., -7.6546e-02,\n",
       "           -5.8184e-02,  2.2824e-01],\n",
       "          ...,\n",
       "          [-3.8704e-01,  6.3645e-01, -3.2129e-01,  ..., -4.5689e-01,\n",
       "            6.7386e-02, -2.2518e-02],\n",
       "          [-2.2790e-01,  2.8139e-01, -1.3041e+00,  ..., -7.8454e-01,\n",
       "            7.9042e-01, -5.0694e-01],\n",
       "          [-4.9218e-01,  7.6567e-02, -7.0122e-01,  ..., -1.1251e+00,\n",
       "           -2.9172e-01,  1.5797e-01]],\n",
       "\n",
       "         [[-1.3049e-01, -4.8274e-02,  1.0504e-01,  ..., -6.2010e-02,\n",
       "            5.3580e-02, -9.2564e-03],\n",
       "          [-4.8729e-01,  2.3826e-01, -1.6930e-01,  ...,  1.6099e-01,\n",
       "           -3.8354e-02, -3.2734e-01],\n",
       "          [-2.6120e-01, -8.0902e-01, -1.3709e-02,  ..., -3.5803e-01,\n",
       "           -2.5394e-01,  6.7826e-01],\n",
       "          ...,\n",
       "          [-1.1585e-01,  6.5850e-02, -5.0949e-01,  ..., -7.4932e-01,\n",
       "           -6.6953e-01, -9.9277e-01],\n",
       "          [ 2.0975e+00,  1.7260e+00, -1.4236e-01,  ...,  1.9757e+00,\n",
       "           -6.9813e-01, -6.2983e-01],\n",
       "          [ 3.7916e-01,  1.0791e+00,  1.2831e-01,  ...,  1.4665e+00,\n",
       "           -1.1523e-01, -1.0500e+00]],\n",
       "\n",
       "         [[-1.4202e-02, -8.2563e-03, -2.4499e-02,  ..., -2.0446e-02,\n",
       "            5.8805e-03, -1.2406e-02],\n",
       "          [-4.9147e-01, -2.7316e-01,  6.5958e-01,  ...,  3.8073e-01,\n",
       "            6.0782e-01, -7.4229e-01],\n",
       "          [-3.1975e-01, -5.4548e-01,  1.6030e-03,  ..., -2.1414e-01,\n",
       "            2.5946e-01,  1.5577e-01],\n",
       "          ...,\n",
       "          [-2.4797e-01,  3.4496e-01,  5.2548e-01,  ..., -7.7637e-01,\n",
       "            6.3922e-01, -1.2932e-01],\n",
       "          [ 1.0972e-01, -8.4694e-02, -3.9300e-01,  ..., -3.8349e-01,\n",
       "            2.3465e-01, -1.1936e-01],\n",
       "          [-3.9311e-02, -6.7230e-01,  4.3672e-02,  ..., -4.5500e-01,\n",
       "            1.4901e-01,  4.5368e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.9413e-03, -5.1325e-02,  2.1233e-02,  ...,  5.8183e-02,\n",
       "            2.7543e-02,  7.3718e-02],\n",
       "          [-6.8224e-01, -3.5075e-01,  2.1327e-01,  ..., -5.7191e-01,\n",
       "            5.6553e-01, -6.4773e-01],\n",
       "          [ 5.4235e-01,  1.1635e-01, -5.7858e-01,  ..., -8.4099e-01,\n",
       "            7.2675e-03, -5.2381e-01],\n",
       "          ...,\n",
       "          [-7.6434e-01, -2.1918e-01,  9.1622e-01,  ...,  3.8329e-01,\n",
       "            7.0098e-02,  5.2440e-01],\n",
       "          [ 5.6770e-02,  1.6301e-01,  9.4071e-02,  ...,  2.4920e-01,\n",
       "            5.7850e-01, -1.2538e-01],\n",
       "          [-3.2877e-01,  1.5562e-02,  1.2176e-01,  ..., -7.1036e-02,\n",
       "           -1.5564e-01, -1.0564e-01]],\n",
       "\n",
       "         [[-6.1497e-02, -1.2752e-02, -1.4568e-01,  ..., -4.8172e-02,\n",
       "            4.7348e-02, -2.4290e-02],\n",
       "          [ 1.3147e-01,  6.3121e-01, -3.5953e-01,  ...,  1.1897e-01,\n",
       "            1.0056e+00,  5.0474e-01],\n",
       "          [ 7.5941e-01,  3.6526e-01,  2.9423e-01,  ..., -2.5376e-01,\n",
       "            2.0285e-01, -1.3698e-01],\n",
       "          ...,\n",
       "          [-1.6025e-01,  1.2065e-01, -6.3712e-01,  ...,  7.3582e-01,\n",
       "           -2.4661e-01,  7.7589e-01],\n",
       "          [ 5.5425e-03,  2.1024e-01, -5.0255e-01,  ...,  4.3895e-01,\n",
       "            4.6785e-01,  3.1110e-01],\n",
       "          [-2.8857e-01,  1.4645e-01,  1.4382e-01,  ..., -1.5963e-01,\n",
       "            4.2173e-02,  1.7991e-02]],\n",
       "\n",
       "         [[ 7.0930e-02,  1.0169e-01,  9.1266e-02,  ...,  8.8514e-03,\n",
       "           -8.6669e-02, -4.9571e-03],\n",
       "          [-1.3191e-01, -6.2046e-02,  2.0125e-01,  ...,  6.1535e-01,\n",
       "            1.4052e+00,  4.1019e-01],\n",
       "          [-1.8832e-01,  5.7359e-01, -3.5219e-01,  ...,  5.9092e-01,\n",
       "           -7.1859e-01,  4.5427e-01],\n",
       "          ...,\n",
       "          [-1.3442e-02, -1.4873e-01,  3.2663e-01,  ...,  1.3741e-01,\n",
       "            4.3527e-01,  8.5873e-02],\n",
       "          [ 4.5680e-01,  1.3189e+00,  5.8013e-01,  ..., -1.8441e-01,\n",
       "            1.3306e-01,  3.0174e-02],\n",
       "          [ 3.3733e-01,  3.4459e-01, -7.0331e-01,  ...,  3.5378e-01,\n",
       "            2.7479e-01,  7.5353e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8660e-02,  7.2212e-02, -8.0057e-02,  ...,  4.5701e-02,\n",
       "            3.9052e-02, -1.3910e-01],\n",
       "          [-1.1430e-01, -3.6511e-01,  1.4372e-01,  ..., -2.7121e-01,\n",
       "           -4.0517e-01,  6.1041e-01],\n",
       "          [-6.9271e-02,  4.4875e-01, -2.1989e-01,  ..., -7.6546e-02,\n",
       "           -5.8184e-02,  2.2824e-01],\n",
       "          ...,\n",
       "          [-2.4523e-01,  4.4308e-01,  4.0595e-01,  ..., -3.7889e-01,\n",
       "            8.7256e-01,  3.8068e-02],\n",
       "          [-5.3003e-01,  2.6609e-01, -6.9370e-01,  ..., -8.4596e-01,\n",
       "           -1.1458e-01,  8.4023e-02],\n",
       "          [-8.4252e-03,  6.2449e-01, -3.0089e-01,  ..., -4.0039e-01,\n",
       "           -2.1078e-01, -1.0181e+00]],\n",
       "\n",
       "         [[-1.3049e-01, -4.8274e-02,  1.0504e-01,  ..., -6.2010e-02,\n",
       "            5.3580e-02, -9.2564e-03],\n",
       "          [-4.8729e-01,  2.3826e-01, -1.6930e-01,  ...,  1.6099e-01,\n",
       "           -3.8354e-02, -3.2734e-01],\n",
       "          [-2.6120e-01, -8.0902e-01, -1.3709e-02,  ..., -3.5803e-01,\n",
       "           -2.5394e-01,  6.7826e-01],\n",
       "          ...,\n",
       "          [ 5.6112e-01,  6.5342e-01, -6.1568e-01,  ...,  5.1000e-01,\n",
       "           -5.2503e-01, -2.6359e-01],\n",
       "          [ 5.9869e-01,  6.2687e-01, -4.3407e-01,  ...,  1.4323e+00,\n",
       "           -1.9565e-01, -1.0417e+00],\n",
       "          [ 8.3101e-01, -4.7988e-01,  4.9332e-01,  ...,  3.4920e-01,\n",
       "           -4.0104e-01, -6.5770e-01]],\n",
       "\n",
       "         [[-1.4202e-02, -8.2563e-03, -2.4499e-02,  ..., -2.0446e-02,\n",
       "            5.8805e-03, -1.2406e-02],\n",
       "          [-4.9147e-01, -2.7316e-01,  6.5958e-01,  ...,  3.8073e-01,\n",
       "            6.0782e-01, -7.4229e-01],\n",
       "          [-3.1975e-01, -5.4548e-01,  1.6030e-03,  ..., -2.1414e-01,\n",
       "            2.5946e-01,  1.5577e-01],\n",
       "          ...,\n",
       "          [ 3.5453e-02,  2.0713e-01, -3.3553e-01,  ..., -3.0803e-01,\n",
       "           -1.1032e-02,  2.5546e-02],\n",
       "          [-1.2866e-01, -8.2000e-01, -1.7901e-01,  ..., -3.3406e-01,\n",
       "            1.4968e-01,  3.8440e-01],\n",
       "          [ 5.5203e-01,  9.2679e-02,  1.9785e-01,  ..., -8.6708e-02,\n",
       "            1.9566e-01, -5.3393e-02]]]])), (tensor([[[[ 2.3020e-02, -3.0505e-01,  2.4262e-01,  ...,  1.7027e+00,\n",
       "           -2.1129e-01, -7.5293e-02],\n",
       "          [ 1.2862e+00,  3.6941e-01,  1.2597e+00,  ..., -3.2001e+00,\n",
       "            1.0091e+00, -1.4380e+00],\n",
       "          [-6.3545e-01, -2.8809e-01,  1.3472e+00,  ..., -4.4604e+00,\n",
       "           -4.8687e-01, -1.4778e+00],\n",
       "          ...,\n",
       "          [-1.0607e+00,  5.4603e-01,  2.2172e-01,  ..., -3.1671e+00,\n",
       "            2.3333e-01, -3.7846e-01],\n",
       "          [ 2.5830e-01, -5.2587e-01,  5.1597e-01,  ..., -3.5446e+00,\n",
       "           -6.4635e-01, -1.0550e+00],\n",
       "          [-1.9637e+00, -8.6970e-01,  2.7319e-01,  ..., -4.6700e+00,\n",
       "           -1.8306e-01, -1.9066e+00]],\n",
       "\n",
       "         [[ 1.5326e-01,  9.8001e-01, -1.4271e+00,  ..., -1.3742e-01,\n",
       "            2.5738e-01,  9.2065e-01],\n",
       "          [-2.3634e+00, -5.1856e+00, -9.9490e-01,  ..., -1.4218e+00,\n",
       "           -9.0449e-01, -4.2296e+00],\n",
       "          [-2.5667e+00, -6.3145e+00,  2.1038e+00,  ..., -1.9777e+00,\n",
       "            1.2382e+00, -2.1935e+00],\n",
       "          ...,\n",
       "          [ 6.4163e-01, -2.7717e+00,  2.0617e+00,  ..., -1.1267e+00,\n",
       "            3.3553e-01, -1.9766e+00],\n",
       "          [-3.4538e+00, -4.0150e+00,  3.3235e+00,  ..., -4.3824e-01,\n",
       "            2.5949e+00, -1.4696e+00],\n",
       "          [-2.1741e+00, -3.2800e+00,  2.8817e+00,  ..., -9.8228e-01,\n",
       "           -1.9519e-01, -1.5077e+00]],\n",
       "\n",
       "         [[-6.8439e-01,  2.5586e-01, -4.9604e-02,  ...,  1.9378e-01,\n",
       "            3.5518e-02, -2.8871e-01],\n",
       "          [ 2.3737e+00, -6.9573e-01, -2.1743e-01,  ..., -3.1446e-01,\n",
       "           -7.8987e-01, -1.2595e+00],\n",
       "          [ 2.1284e-01,  1.5497e+00,  1.6277e-01,  ..., -9.5407e-01,\n",
       "           -9.5921e-01,  3.4427e-01],\n",
       "          ...,\n",
       "          [ 2.0482e+00, -1.3359e+00,  1.8224e+00,  ...,  8.7901e-01,\n",
       "           -3.5171e-01,  1.1596e+00],\n",
       "          [ 2.4282e+00, -1.2442e+00,  1.4286e+00,  ..., -6.3740e-01,\n",
       "           -5.4541e-01, -5.8334e-01],\n",
       "          [ 1.4571e+00, -5.8414e-01,  9.2894e-01,  ...,  1.3194e+00,\n",
       "            6.7953e-01,  8.9264e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.2650e-02,  1.2182e-01,  1.3921e-01,  ..., -9.9705e-02,\n",
       "            2.8028e-02,  1.5744e-01],\n",
       "          [ 1.0794e+00,  5.7138e-01, -1.2490e+00,  ...,  1.4306e+00,\n",
       "            7.3433e-01,  1.2053e+00],\n",
       "          [ 9.4338e-01,  3.4334e-01, -4.9489e-01,  ...,  1.3444e+00,\n",
       "            2.1557e-01,  9.6003e-01],\n",
       "          ...,\n",
       "          [-8.9899e-01,  5.8224e-01,  3.2655e-01,  ...,  2.3048e+00,\n",
       "           -2.2143e-01,  8.8870e-01],\n",
       "          [ 4.2845e-01,  3.6532e-01,  3.9170e-01,  ...,  1.2456e+00,\n",
       "            8.4092e-01,  1.4813e+00],\n",
       "          [ 1.1755e+00, -2.0814e-02,  5.0413e-01,  ...,  1.2154e+00,\n",
       "            7.2114e-01,  1.0906e+00]],\n",
       "\n",
       "         [[-3.0115e+00,  3.9998e-01, -1.5440e-02,  ..., -4.7665e-01,\n",
       "           -3.4538e-01,  1.2295e+00],\n",
       "          [ 5.2279e+00,  4.5302e-01, -1.0859e+00,  ...,  2.6250e-01,\n",
       "            1.6243e+00, -1.0199e+00],\n",
       "          [ 4.8111e+00, -1.3690e+00, -1.7302e+00,  ..., -1.3908e+00,\n",
       "            1.0571e+00, -5.2920e-01],\n",
       "          ...,\n",
       "          [ 5.1840e+00,  9.8605e-01, -7.7415e-01,  ..., -5.0673e-02,\n",
       "            7.3396e-01, -4.7702e-01],\n",
       "          [ 5.0582e+00, -5.1806e-01,  2.8630e-01,  ..., -7.5360e-01,\n",
       "            9.8398e-01, -9.0141e-01],\n",
       "          [ 5.3985e+00,  2.9966e-01, -2.6308e+00,  ..., -1.1882e+00,\n",
       "            9.9457e-02, -1.3040e+00]],\n",
       "\n",
       "         [[-7.0842e-03, -2.4107e-01,  6.2859e-03,  ..., -1.6995e-01,\n",
       "            3.2477e-01,  7.3423e-02],\n",
       "          [ 1.0591e+00,  1.3931e-01,  7.3233e-01,  ...,  1.5405e-01,\n",
       "           -1.0153e+00,  3.6435e-01],\n",
       "          [ 3.6486e-01, -1.5759e+00,  1.5822e+00,  ...,  1.3524e+00,\n",
       "            2.7826e-01, -7.2583e-01],\n",
       "          ...,\n",
       "          [ 1.3448e+00,  9.1676e-01, -4.5466e-01,  ...,  7.1612e-01,\n",
       "            1.1728e-01,  1.1014e+00],\n",
       "          [ 1.8514e+00, -2.8364e-01, -7.7078e-01,  ...,  2.0019e-02,\n",
       "            7.6290e-01,  1.2320e-01],\n",
       "          [-3.0141e-01, -1.8272e+00, -1.5394e-01,  ...,  8.3638e-01,\n",
       "           -3.8555e-01, -9.1633e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3020e-02, -3.0505e-01,  2.4262e-01,  ...,  1.7027e+00,\n",
       "           -2.1129e-01, -7.5293e-02],\n",
       "          [ 1.2862e+00,  3.6941e-01,  1.2597e+00,  ..., -3.2001e+00,\n",
       "            1.0091e+00, -1.4380e+00],\n",
       "          [-6.3545e-01, -2.8809e-01,  1.3472e+00,  ..., -4.4604e+00,\n",
       "           -4.8687e-01, -1.4778e+00],\n",
       "          ...,\n",
       "          [-7.3652e-01,  2.4522e-01,  1.0599e+00,  ..., -3.3826e+00,\n",
       "           -5.6081e-01,  2.6417e-01],\n",
       "          [-1.5186e+00, -9.9834e-01,  8.1383e-02,  ..., -4.9205e+00,\n",
       "           -3.3045e-01, -1.8014e+00],\n",
       "          [-1.6142e+00, -1.5423e+00, -2.5770e-02,  ..., -3.7978e+00,\n",
       "           -4.2740e-01, -1.0152e+00]],\n",
       "\n",
       "         [[ 1.5326e-01,  9.8001e-01, -1.4271e+00,  ..., -1.3742e-01,\n",
       "            2.5738e-01,  9.2065e-01],\n",
       "          [-2.3634e+00, -5.1856e+00, -9.9490e-01,  ..., -1.4218e+00,\n",
       "           -9.0449e-01, -4.2296e+00],\n",
       "          [-2.5667e+00, -6.3145e+00,  2.1038e+00,  ..., -1.9777e+00,\n",
       "            1.2382e+00, -2.1935e+00],\n",
       "          ...,\n",
       "          [ 9.5662e-01, -2.8278e+00,  1.8691e+00,  ..., -1.1726e+00,\n",
       "           -3.7897e-04, -1.8699e+00],\n",
       "          [-1.7891e+00, -4.5559e+00,  1.9009e+00,  ..., -7.5030e-01,\n",
       "            6.3202e-01, -3.0242e+00],\n",
       "          [-1.0500e+00, -4.2385e+00,  3.5954e+00,  ..., -1.0667e+00,\n",
       "            1.6876e+00, -2.0938e+00]],\n",
       "\n",
       "         [[-6.8439e-01,  2.5586e-01, -4.9604e-02,  ...,  1.9378e-01,\n",
       "            3.5518e-02, -2.8871e-01],\n",
       "          [ 2.3737e+00, -6.9573e-01, -2.1743e-01,  ..., -3.1446e-01,\n",
       "           -7.8987e-01, -1.2595e+00],\n",
       "          [ 2.1284e-01,  1.5497e+00,  1.6277e-01,  ..., -9.5407e-01,\n",
       "           -9.5921e-01,  3.4427e-01],\n",
       "          ...,\n",
       "          [ 2.5627e+00, -1.3076e+00,  1.4332e+00,  ...,  8.8537e-02,\n",
       "           -3.6764e-01,  5.9456e-01],\n",
       "          [ 1.2846e+00,  3.1677e-02,  1.0016e+00,  ...,  1.4507e+00,\n",
       "            1.0623e+00,  9.2249e-01],\n",
       "          [ 1.2352e+00, -2.6487e-01, -8.5471e-02,  ..., -9.1188e-01,\n",
       "           -2.2179e-01,  7.5055e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-5.2650e-02,  1.2182e-01,  1.3921e-01,  ..., -9.9705e-02,\n",
       "            2.8028e-02,  1.5744e-01],\n",
       "          [ 1.0794e+00,  5.7138e-01, -1.2490e+00,  ...,  1.4306e+00,\n",
       "            7.3433e-01,  1.2053e+00],\n",
       "          [ 9.4338e-01,  3.4334e-01, -4.9489e-01,  ...,  1.3444e+00,\n",
       "            2.1557e-01,  9.6003e-01],\n",
       "          ...,\n",
       "          [-3.2284e-01,  1.9645e-01,  7.7806e-01,  ...,  1.7610e+00,\n",
       "            6.2028e-01,  8.7604e-01],\n",
       "          [ 1.0005e+00,  1.6041e-01,  8.6679e-01,  ...,  1.6236e+00,\n",
       "            1.1455e+00,  1.3410e+00],\n",
       "          [ 7.3519e-01, -3.5268e-01, -2.4565e-01,  ...,  8.2388e-01,\n",
       "           -3.1944e-01,  5.9066e-01]],\n",
       "\n",
       "         [[-3.0115e+00,  3.9998e-01, -1.5440e-02,  ..., -4.7665e-01,\n",
       "           -3.4538e-01,  1.2295e+00],\n",
       "          [ 5.2279e+00,  4.5302e-01, -1.0859e+00,  ...,  2.6250e-01,\n",
       "            1.6243e+00, -1.0199e+00],\n",
       "          [ 4.8111e+00, -1.3690e+00, -1.7302e+00,  ..., -1.3908e+00,\n",
       "            1.0571e+00, -5.2920e-01],\n",
       "          ...,\n",
       "          [ 5.7363e+00,  7.4370e-01, -3.3313e-01,  ..., -3.1107e-01,\n",
       "           -5.1197e-01, -1.0751e+00],\n",
       "          [ 5.7422e+00,  6.6817e-01, -3.0661e+00,  ..., -1.2816e+00,\n",
       "           -2.5641e-01, -1.5945e+00],\n",
       "          [ 4.6158e+00, -2.5076e-01, -1.5779e+00,  ..., -1.2317e+00,\n",
       "            4.9668e-01, -1.1502e+00]],\n",
       "\n",
       "         [[-7.0842e-03, -2.4107e-01,  6.2859e-03,  ..., -1.6995e-01,\n",
       "            3.2477e-01,  7.3423e-02],\n",
       "          [ 1.0591e+00,  1.3931e-01,  7.3233e-01,  ...,  1.5405e-01,\n",
       "           -1.0153e+00,  3.6435e-01],\n",
       "          [ 3.6486e-01, -1.5759e+00,  1.5822e+00,  ...,  1.3524e+00,\n",
       "            2.7826e-01, -7.2583e-01],\n",
       "          ...,\n",
       "          [ 4.3895e-01,  4.9018e-02, -1.6093e+00,  ..., -6.8066e-01,\n",
       "           -8.6207e-02,  1.2008e-01],\n",
       "          [-5.0314e-01, -1.4431e+00, -1.1318e+00,  ...,  8.5453e-01,\n",
       "           -4.5191e-01, -4.9380e-01],\n",
       "          [ 6.0167e-01, -1.3922e+00, -1.8982e-01,  ...,  3.2189e-01,\n",
       "            2.6382e-01, -9.8451e-02]]]]), tensor([[[[-2.7155e-02, -2.7188e-02,  1.0668e-02,  ...,  9.3251e-04,\n",
       "           -3.0306e-02,  3.4526e-01],\n",
       "          [ 1.2152e+00,  5.9157e-01,  1.5323e+00,  ...,  4.0784e-01,\n",
       "            5.5866e-01, -1.3667e+00],\n",
       "          [ 3.2929e-01, -9.6470e-01,  5.7085e-01,  ...,  3.3488e-01,\n",
       "            7.5620e-01, -7.7334e-01],\n",
       "          ...,\n",
       "          [ 3.5701e-01,  1.3717e-01, -1.2413e-01,  ..., -6.3871e-01,\n",
       "            7.5637e-01, -1.5512e-01],\n",
       "          [ 8.9877e-01, -5.0085e-01, -1.3353e-01,  ...,  5.0869e-01,\n",
       "            1.4649e-01,  1.8668e-01],\n",
       "          [ 1.0500e+00, -4.9877e-01, -2.5511e-01,  ...,  2.9883e-01,\n",
       "            4.2095e-01, -7.7147e-01]],\n",
       "\n",
       "         [[ 2.6579e-03, -2.7392e-02,  2.1020e-02,  ..., -1.0329e-02,\n",
       "            1.8776e-02,  1.3874e-02],\n",
       "          [ 7.9200e-01, -1.8003e-01,  1.4703e+00,  ..., -8.6426e-01,\n",
       "            1.4149e+00, -8.0732e-01],\n",
       "          [ 7.1245e-01,  6.1741e-02,  6.5467e-01,  ..., -7.2336e-01,\n",
       "            1.2534e+00, -2.1623e-01],\n",
       "          ...,\n",
       "          [-5.0868e-01, -3.6841e-01,  1.0877e+00,  ...,  2.4139e-01,\n",
       "            1.4532e+00, -7.8096e-01],\n",
       "          [-2.9166e-01,  8.2247e-01,  2.6063e-01,  ...,  2.2036e-01,\n",
       "            1.4924e+00, -1.5151e-01],\n",
       "          [ 6.6674e-01, -4.3474e-01,  6.3684e-02,  ..., -4.5954e-01,\n",
       "            1.5195e+00,  1.9962e-01]],\n",
       "\n",
       "         [[-5.7998e-02,  4.3486e-03, -4.1018e-02,  ..., -4.0250e-02,\n",
       "            1.7191e-02, -8.2310e-02],\n",
       "          [ 4.2022e-01, -2.2604e-01,  3.9398e-01,  ..., -1.6460e+00,\n",
       "            3.6984e-01,  6.5250e-01],\n",
       "          [ 2.2156e-01, -1.3655e-01,  9.5562e-01,  ..., -1.3445e+00,\n",
       "            2.4857e-01,  1.6776e-01],\n",
       "          ...,\n",
       "          [-6.7335e-01, -3.5413e-01, -5.6404e-01,  ..., -5.4039e-01,\n",
       "            8.0971e-01,  7.9681e-02],\n",
       "          [-8.9356e-01, -2.5885e-01, -7.0300e-01,  ..., -1.0708e-01,\n",
       "           -3.3455e-01, -7.9536e-02],\n",
       "          [ 3.5970e-01, -3.4472e-01,  1.8971e-01,  ..., -3.8301e-02,\n",
       "            3.2024e-01,  1.0566e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.2384e-01, -1.9268e-01, -7.3070e-02,  ..., -4.6824e-01,\n",
       "            2.2304e-01,  9.5033e-02],\n",
       "          [ 1.8947e+00, -3.3652e-01,  1.2550e+00,  ...,  1.3533e+00,\n",
       "           -2.6148e+00, -2.7860e-01],\n",
       "          [ 1.9978e+00, -1.5758e+00,  6.2170e-01,  ...,  1.0044e+00,\n",
       "           -6.5646e-01, -1.9490e-01],\n",
       "          ...,\n",
       "          [ 1.2062e+00, -2.1251e-01,  8.4790e-02,  ...,  4.9520e-01,\n",
       "           -2.2918e+00,  8.9258e-01],\n",
       "          [-2.7384e-02, -2.0337e+00,  1.4435e-01,  ..., -9.9669e-02,\n",
       "           -4.8818e-01, -5.5251e-01],\n",
       "          [ 7.4418e-01, -7.7186e-01,  1.1883e+00,  ...,  7.4771e-01,\n",
       "           -6.0523e-01, -1.0858e+00]],\n",
       "\n",
       "         [[-7.8598e-02, -1.3463e-01, -4.5632e-02,  ..., -1.9290e-01,\n",
       "           -1.4357e-01,  1.2455e-01],\n",
       "          [-3.5941e-01, -6.8116e-01, -9.9880e-01,  ...,  1.0653e+00,\n",
       "            1.0728e-01,  7.9258e-02],\n",
       "          [-5.1206e-02, -5.4652e-01, -5.3534e-01,  ..., -3.4223e-01,\n",
       "           -3.1468e-01, -4.2074e-01],\n",
       "          ...,\n",
       "          [-6.9826e-01, -4.1885e-02, -4.6684e-01,  ..., -3.8774e-01,\n",
       "            3.0837e-01, -6.4372e-01],\n",
       "          [ 8.8183e-02,  9.7595e-01,  7.5708e-01,  ...,  7.4703e-01,\n",
       "            1.9275e-01,  6.3002e-01],\n",
       "          [-2.2465e-01,  7.4126e-01, -3.3641e-02,  ...,  4.5438e-01,\n",
       "           -8.2702e-01, -3.0152e-01]],\n",
       "\n",
       "         [[-3.6613e-02, -3.6841e-02,  9.3909e-02,  ...,  7.2996e-02,\n",
       "           -3.5712e-02,  1.9252e-02],\n",
       "          [-2.9972e-01,  2.5515e+00, -8.3471e-01,  ..., -1.6416e+00,\n",
       "           -1.0924e+00, -3.8776e-01],\n",
       "          [ 1.0264e-01,  4.1386e-01, -8.7022e-01,  ..., -1.8202e-01,\n",
       "           -5.0509e-01,  5.3114e-01],\n",
       "          ...,\n",
       "          [-8.5012e-01,  6.4857e-01,  1.3410e+00,  ...,  5.3838e-01,\n",
       "           -5.2255e-04, -1.6547e-01],\n",
       "          [-1.4473e+00, -3.1375e-01, -8.3077e-01,  ..., -1.4824e-01,\n",
       "           -2.2201e-02, -1.6198e+00],\n",
       "          [-8.1993e-01, -1.6832e+00,  4.8476e-01,  ..., -3.4714e-01,\n",
       "            2.5853e-01, -1.3019e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.7155e-02, -2.7188e-02,  1.0668e-02,  ...,  9.3251e-04,\n",
       "           -3.0306e-02,  3.4526e-01],\n",
       "          [ 1.2152e+00,  5.9157e-01,  1.5323e+00,  ...,  4.0784e-01,\n",
       "            5.5866e-01, -1.3667e+00],\n",
       "          [ 3.2929e-01, -9.6470e-01,  5.7085e-01,  ...,  3.3488e-01,\n",
       "            7.5620e-01, -7.7334e-01],\n",
       "          ...,\n",
       "          [ 5.1038e-01,  3.9776e-01,  2.4309e-01,  ..., -6.1550e-01,\n",
       "            8.5371e-01, -8.8984e-01],\n",
       "          [ 8.1935e-01, -8.5997e-01, -6.5425e-02,  ...,  4.5582e-01,\n",
       "            2.9216e-01, -9.1355e-01],\n",
       "          [ 5.3908e-01,  1.8345e-01, -2.6269e-01,  ...,  3.0765e-01,\n",
       "            7.9928e-01, -8.5683e-01]],\n",
       "\n",
       "         [[ 2.6579e-03, -2.7392e-02,  2.1020e-02,  ..., -1.0329e-02,\n",
       "            1.8776e-02,  1.3874e-02],\n",
       "          [ 7.9200e-01, -1.8003e-01,  1.4703e+00,  ..., -8.6426e-01,\n",
       "            1.4149e+00, -8.0732e-01],\n",
       "          [ 7.1245e-01,  6.1741e-02,  6.5467e-01,  ..., -7.2336e-01,\n",
       "            1.2534e+00, -2.1623e-01],\n",
       "          ...,\n",
       "          [ 7.0048e-01, -1.7926e-01,  3.2790e-01,  ...,  1.0342e-01,\n",
       "            2.1874e+00, -3.4559e-01],\n",
       "          [ 8.6340e-01, -4.4227e-01, -9.5193e-02,  ..., -4.2772e-01,\n",
       "            1.6447e+00,  2.1664e-03],\n",
       "          [-6.0206e-01,  3.2989e-01,  6.6664e-01,  ..., -1.4917e+00,\n",
       "            1.2397e+00, -9.0744e-01]],\n",
       "\n",
       "         [[-5.7998e-02,  4.3486e-03, -4.1018e-02,  ..., -4.0250e-02,\n",
       "            1.7191e-02, -8.2310e-02],\n",
       "          [ 4.2022e-01, -2.2604e-01,  3.9398e-01,  ..., -1.6460e+00,\n",
       "            3.6984e-01,  6.5250e-01],\n",
       "          [ 2.2156e-01, -1.3655e-01,  9.5562e-01,  ..., -1.3445e+00,\n",
       "            2.4857e-01,  1.6776e-01],\n",
       "          ...,\n",
       "          [-7.6982e-01, -5.4663e-01, -5.9447e-01,  ..., -6.5907e-01,\n",
       "            1.0043e+00, -3.5425e-01],\n",
       "          [ 6.7295e-01, -2.0242e-01,  1.0241e-01,  ...,  1.3036e-01,\n",
       "            1.5452e-01,  1.7895e-01],\n",
       "          [-2.4612e-01,  1.1226e+00,  4.0571e-01,  ...,  4.6721e-01,\n",
       "            7.5796e-01,  3.3922e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.2384e-01, -1.9268e-01, -7.3070e-02,  ..., -4.6824e-01,\n",
       "            2.2304e-01,  9.5033e-02],\n",
       "          [ 1.8947e+00, -3.3652e-01,  1.2550e+00,  ...,  1.3533e+00,\n",
       "           -2.6148e+00, -2.7860e-01],\n",
       "          [ 1.9978e+00, -1.5758e+00,  6.2170e-01,  ...,  1.0044e+00,\n",
       "           -6.5646e-01, -1.9490e-01],\n",
       "          ...,\n",
       "          [ 5.5753e-01, -2.3429e-01,  7.8198e-01,  ...,  1.5114e-01,\n",
       "           -1.7080e+00,  1.8032e-01],\n",
       "          [ 1.1299e+00, -1.4558e+00,  1.1953e+00,  ...,  3.4959e-01,\n",
       "           -5.8080e-01, -1.9005e-01],\n",
       "          [ 6.4049e-01, -9.0027e-01, -5.7748e-01,  ...,  1.1095e+00,\n",
       "           -6.5925e-01, -5.3639e-01]],\n",
       "\n",
       "         [[-7.8598e-02, -1.3463e-01, -4.5632e-02,  ..., -1.9290e-01,\n",
       "           -1.4357e-01,  1.2455e-01],\n",
       "          [-3.5941e-01, -6.8116e-01, -9.9880e-01,  ...,  1.0653e+00,\n",
       "            1.0728e-01,  7.9258e-02],\n",
       "          [-5.1206e-02, -5.4652e-01, -5.3534e-01,  ..., -3.4223e-01,\n",
       "           -3.1468e-01, -4.2074e-01],\n",
       "          ...,\n",
       "          [ 1.5103e-01, -8.8309e-01, -2.3934e-01,  ...,  4.6570e-01,\n",
       "            1.0358e+00,  4.9085e-01],\n",
       "          [ 4.0667e-02,  4.7748e-02,  5.5696e-02,  ...,  5.5555e-01,\n",
       "           -8.1095e-01,  5.3679e-02],\n",
       "          [-4.4084e-01, -4.7297e-01, -3.6107e-01,  ...,  4.7460e-01,\n",
       "           -6.6458e-01, -9.3093e-01]],\n",
       "\n",
       "         [[-3.6613e-02, -3.6841e-02,  9.3909e-02,  ...,  7.2996e-02,\n",
       "           -3.5712e-02,  1.9252e-02],\n",
       "          [-2.9972e-01,  2.5515e+00, -8.3471e-01,  ..., -1.6416e+00,\n",
       "           -1.0924e+00, -3.8776e-01],\n",
       "          [ 1.0264e-01,  4.1386e-01, -8.7022e-01,  ..., -1.8202e-01,\n",
       "           -5.0509e-01,  5.3114e-01],\n",
       "          ...,\n",
       "          [-8.7267e-01, -6.6426e-02, -7.3594e-01,  ...,  3.4294e-01,\n",
       "            1.9605e-01,  3.3836e-01],\n",
       "          [-7.7703e-01, -1.8704e+00, -4.4478e-01,  ..., -5.8328e-01,\n",
       "            1.0979e+00, -1.1794e+00],\n",
       "          [-4.5660e-01, -2.9170e-01, -1.4435e+00,  ...,  7.7781e-01,\n",
       "           -2.1941e-01, -8.6451e-02]]]])), (tensor([[[[-0.3339,  0.8832, -0.1579,  ...,  1.1393, -0.1814,  0.1396],\n",
       "          [-0.0112, -4.2478,  0.8130,  ..., -3.2530, -0.3098,  2.1649],\n",
       "          [-0.5420, -3.9133, -0.2124,  ..., -3.9456, -0.7525,  0.8528],\n",
       "          ...,\n",
       "          [ 1.1466, -2.8270,  0.9862,  ..., -3.1559, -1.6827,  0.9194],\n",
       "          [-0.0807, -2.6629,  1.8906,  ..., -2.8957, -1.1356,  0.9873],\n",
       "          [-0.1664, -4.4941,  0.8641,  ..., -3.8159, -0.2670,  0.9661]],\n",
       "\n",
       "         [[ 0.0602,  0.8579, -0.6282,  ..., -0.0366,  0.3021,  0.0332],\n",
       "          [-0.1124, -0.0933,  2.2647,  ...,  1.6213, -0.3506,  0.1679],\n",
       "          [-0.4610, -0.7795,  1.5831,  ...,  2.4977,  1.2497, -0.1462],\n",
       "          ...,\n",
       "          [-1.0454,  1.3204, -0.3578,  ...,  0.9700,  1.7237,  0.3503],\n",
       "          [-0.9753,  1.1879,  0.7022,  ...,  0.3264,  1.3306, -0.0243],\n",
       "          [-1.6077,  0.5157,  0.3509,  ...,  0.7864,  0.8829, -0.5624]],\n",
       "\n",
       "         [[-0.3200,  0.1287, -0.9790,  ..., -0.3494, -0.0584, -0.1392],\n",
       "          [-0.6200,  0.6003,  3.8523,  ...,  1.8905, -0.0778, -0.2056],\n",
       "          [-0.9694, -0.0295,  3.2736,  ...,  0.1666, -0.1531, -1.0054],\n",
       "          ...,\n",
       "          [-0.8916,  0.6999,  4.0023,  ...,  1.4355,  0.1376,  0.2917],\n",
       "          [-0.6537,  1.0219,  2.8160,  ...,  1.0409,  0.0421,  0.3152],\n",
       "          [-1.2756, -0.1223,  2.5665,  ...,  1.4309, -0.5520,  0.7729]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3674,  0.0785, -0.0839,  ..., -0.0541,  0.2363,  0.0133],\n",
       "          [-1.1357, -2.1568, -0.9987,  ..., -0.0851,  1.2143,  0.6327],\n",
       "          [-0.1401, -2.1064, -0.3645,  ..., -0.3517, -0.2392, -0.6098],\n",
       "          ...,\n",
       "          [-0.2497, -0.7123, -0.1600,  ..., -0.6532,  1.8316, -0.0558],\n",
       "          [ 2.0702, -1.0509,  1.6006,  ...,  0.6717, -0.7634, -1.7976],\n",
       "          [ 1.4719, -0.0129, -0.7336,  ...,  0.5294, -0.0200, -1.2744]],\n",
       "\n",
       "         [[ 0.1896,  0.0656,  0.3179,  ...,  0.4060,  0.0196,  0.2254],\n",
       "          [ 0.5124,  0.1373,  1.6424,  ..., -0.5922, -0.1873, -0.1938],\n",
       "          [ 0.7358,  0.7410,  1.7245,  ..., -0.8931, -0.2398,  0.4509],\n",
       "          ...,\n",
       "          [ 1.2127, -0.3211,  1.3601,  ..., -1.3824, -0.3791,  0.5343],\n",
       "          [ 0.8253,  0.6497,  1.4663,  ..., -1.8481, -0.8794,  0.9917],\n",
       "          [ 1.0226,  0.4631,  1.4147,  ..., -0.9879, -0.8126,  1.6053]],\n",
       "\n",
       "         [[-3.0126,  0.5365,  0.5567,  ..., -0.9270,  0.3162,  0.1957],\n",
       "          [ 7.3600, -0.8517, -2.6750,  ...,  3.2755, -0.1587, -1.2805],\n",
       "          [ 8.6604,  0.1449, -1.7049,  ...,  2.4676, -0.6957, -0.8990],\n",
       "          ...,\n",
       "          [ 9.3230, -0.4082, -1.1917,  ...,  2.7718, -0.4882, -1.8892],\n",
       "          [ 7.4100, -0.8336, -0.4678,  ...,  3.1024,  0.6681, -1.4756],\n",
       "          [ 8.1631, -0.6299, -2.9186,  ...,  2.2346,  0.0947, -1.2432]]],\n",
       "\n",
       "\n",
       "        [[[-0.3339,  0.8832, -0.1579,  ...,  1.1393, -0.1814,  0.1396],\n",
       "          [-0.0112, -4.2478,  0.8130,  ..., -3.2530, -0.3098,  2.1649],\n",
       "          [-0.5420, -3.9133, -0.2124,  ..., -3.9456, -0.7525,  0.8528],\n",
       "          ...,\n",
       "          [ 0.7432, -3.4698,  0.9098,  ..., -3.4810, -1.6693,  1.0219],\n",
       "          [-0.2532, -4.8485,  0.3368,  ..., -3.8727,  0.3018,  1.2711],\n",
       "          [-0.7463, -4.0062,  0.9212,  ..., -4.2596,  0.0821,  0.1783]],\n",
       "\n",
       "         [[ 0.0602,  0.8579, -0.6282,  ..., -0.0366,  0.3021,  0.0332],\n",
       "          [-0.1124, -0.0933,  2.2647,  ...,  1.6213, -0.3506,  0.1679],\n",
       "          [-0.4610, -0.7795,  1.5831,  ...,  2.4977,  1.2497, -0.1462],\n",
       "          ...,\n",
       "          [-0.9733,  1.2046, -0.6191,  ...,  1.6417,  0.9447,  0.1396],\n",
       "          [-0.8459,  0.9496,  0.5037,  ...,  0.8440,  0.0776,  1.1320],\n",
       "          [-0.6190,  0.3036,  1.4378,  ...,  2.3942,  1.3131,  0.1032]],\n",
       "\n",
       "         [[-0.3200,  0.1287, -0.9790,  ..., -0.3494, -0.0584, -0.1392],\n",
       "          [-0.6200,  0.6003,  3.8523,  ...,  1.8905, -0.0778, -0.2056],\n",
       "          [-0.9694, -0.0295,  3.2736,  ...,  0.1666, -0.1531, -1.0054],\n",
       "          ...,\n",
       "          [-0.7747, -0.0825,  3.6884,  ...,  1.5271,  0.4167,  0.1836],\n",
       "          [-0.8167, -0.2760,  2.5881,  ...,  1.2932, -0.1978,  1.0409],\n",
       "          [-1.2919,  1.1409,  2.0373,  ...,  0.4952,  0.1140, -0.3994]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.3674,  0.0785, -0.0839,  ..., -0.0541,  0.2363,  0.0133],\n",
       "          [-1.1357, -2.1568, -0.9987,  ..., -0.0851,  1.2143,  0.6327],\n",
       "          [-0.1401, -2.1064, -0.3645,  ..., -0.3517, -0.2392, -0.6098],\n",
       "          ...,\n",
       "          [-0.8114, -1.4375,  0.3997,  ..., -0.5519,  1.8199, -0.3636],\n",
       "          [ 0.7222, -1.9326,  0.6863,  ..., -0.5721, -0.0650, -1.6360],\n",
       "          [-0.1925,  0.1800, -0.7874,  ...,  0.0531,  0.4097,  0.5987]],\n",
       "\n",
       "         [[ 0.1896,  0.0656,  0.3179,  ...,  0.4060,  0.0196,  0.2254],\n",
       "          [ 0.5124,  0.1373,  1.6424,  ..., -0.5922, -0.1873, -0.1938],\n",
       "          [ 0.7358,  0.7410,  1.7245,  ..., -0.8931, -0.2398,  0.4509],\n",
       "          ...,\n",
       "          [ 1.8959,  0.7453,  0.9381,  ..., -1.3519, -0.9779, -0.2761],\n",
       "          [ 1.0234,  0.4848,  1.1856,  ..., -1.0741, -0.6702,  1.0617],\n",
       "          [ 2.0174, -1.2932,  0.7084,  ..., -0.8824, -0.4109,  1.3051]],\n",
       "\n",
       "         [[-3.0126,  0.5365,  0.5567,  ..., -0.9270,  0.3162,  0.1957],\n",
       "          [ 7.3600, -0.8517, -2.6750,  ...,  3.2755, -0.1587, -1.2805],\n",
       "          [ 8.6604,  0.1449, -1.7049,  ...,  2.4676, -0.6957, -0.8990],\n",
       "          ...,\n",
       "          [ 8.9261, -0.5608, -1.3625,  ...,  1.8077, -0.6073, -1.0903],\n",
       "          [ 7.8155,  0.0471, -2.9213,  ...,  1.6713, -0.7822, -1.1789],\n",
       "          [ 8.7222, -1.1886, -0.8783,  ...,  3.2702,  0.0594, -1.4108]]]]), tensor([[[[ 4.1256e-02, -5.1608e-02,  1.9007e-02,  ..., -7.1646e-02,\n",
       "           -6.8897e-03, -1.0085e-01],\n",
       "          [-8.9544e-02, -1.9730e-01,  5.2907e-01,  ...,  3.0307e-01,\n",
       "           -1.6986e-01, -3.2264e-01],\n",
       "          [-7.8745e-02,  1.1392e-01,  5.5491e-01,  ...,  2.3021e-01,\n",
       "            3.6979e-01, -4.7231e-01],\n",
       "          ...,\n",
       "          [ 6.3732e-01, -5.4198e-01,  3.3375e-02,  ..., -3.3028e-01,\n",
       "           -4.5383e-01, -2.8808e-01],\n",
       "          [ 2.6841e-01, -2.8291e-01,  5.0992e-01,  ...,  8.0359e-02,\n",
       "            2.3631e-01,  4.1461e-01],\n",
       "          [ 5.0659e-01, -3.7244e-01,  7.6646e-01,  ...,  6.4715e-01,\n",
       "           -3.3868e-01,  6.6680e-01]],\n",
       "\n",
       "         [[ 6.7747e-02,  2.1723e-02, -3.8099e-02,  ..., -3.9011e-02,\n",
       "            2.3689e-02,  1.1724e-03],\n",
       "          [-7.8540e-01, -8.3013e-01,  2.3576e-01,  ...,  1.2471e+00,\n",
       "           -2.7760e-01, -2.8777e-02],\n",
       "          [ 2.3721e-01,  6.6427e-02, -1.3587e+00,  ...,  1.5889e-01,\n",
       "           -8.3478e-01,  7.9751e-01],\n",
       "          ...,\n",
       "          [ 1.3360e+00, -5.5506e-01, -5.7238e-01,  ...,  2.9254e-01,\n",
       "            1.3101e-01, -7.3583e-01],\n",
       "          [ 1.4301e-01, -2.1371e-01,  5.7697e-01,  ..., -3.4001e-01,\n",
       "           -1.7463e-01,  4.1656e-01],\n",
       "          [-9.7475e-01, -3.9634e-01, -9.9969e-01,  ...,  7.7408e-01,\n",
       "           -4.2060e-01, -1.9729e-01]],\n",
       "\n",
       "         [[ 7.6431e-02,  9.5054e-03,  2.5425e-03,  ...,  2.7293e-02,\n",
       "           -7.7690e-02, -6.5371e-02],\n",
       "          [-3.2776e-01,  7.9279e-01, -3.9783e-02,  ..., -1.0361e+00,\n",
       "            7.1417e-01, -2.2441e-01],\n",
       "          [-6.0069e-01,  2.3084e-02,  2.5820e-01,  ..., -7.3405e-01,\n",
       "            5.2939e-01, -8.9682e-01],\n",
       "          ...,\n",
       "          [ 4.3803e-01,  6.9847e-01, -3.1219e-01,  ..., -5.9952e-01,\n",
       "           -9.0526e-02,  7.0327e-01],\n",
       "          [ 1.3231e+00,  9.9944e-01,  2.0180e+00,  ..., -3.4217e-01,\n",
       "            4.3051e-02,  1.2441e+00],\n",
       "          [ 1.2821e-01,  5.4979e-01,  1.1891e+00,  ..., -1.5865e-01,\n",
       "            9.3243e-01,  2.2628e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.3259e-03,  1.9788e-02,  1.7440e-02,  ..., -6.6362e-02,\n",
       "           -2.5949e-02,  2.2967e-02],\n",
       "          [-1.7083e+00, -4.1000e-01, -3.5604e-01,  ...,  1.1526e+00,\n",
       "            1.8841e-01,  3.8700e-01],\n",
       "          [-6.1640e-01, -8.2295e-01, -1.6814e-01,  ...,  9.7704e-01,\n",
       "           -3.3866e-01,  2.5587e-01],\n",
       "          ...,\n",
       "          [-1.1882e+00, -8.7336e-01, -1.0756e+00,  ...,  8.6553e-01,\n",
       "           -6.5032e-01,  5.1667e-01],\n",
       "          [-2.3426e-01,  2.0931e-01, -3.4395e-01,  ...,  1.5369e+00,\n",
       "           -3.8579e-01, -8.9765e-01],\n",
       "          [-1.4451e+00, -8.5626e-01,  8.4805e-01,  ..., -3.6455e-01,\n",
       "           -4.9214e-01,  4.7833e-01]],\n",
       "\n",
       "         [[ 3.7895e-02, -7.8421e-03,  2.2710e-02,  ...,  2.6063e-02,\n",
       "           -1.1852e-02,  1.0835e-02],\n",
       "          [-7.5730e-01, -9.8235e-02, -1.2391e+00,  ...,  1.1498e+00,\n",
       "            4.6780e-01,  2.7294e-01],\n",
       "          [-6.7263e-01, -3.5302e-01, -1.0088e+00,  ...,  5.6922e-01,\n",
       "            6.4065e-01, -8.5078e-01],\n",
       "          ...,\n",
       "          [ 3.1127e-01, -1.5713e-01,  9.1565e-02,  ...,  8.8040e-01,\n",
       "           -4.9524e-01, -1.6419e-01],\n",
       "          [-2.2940e-01,  5.2252e-01, -3.1537e-01,  ..., -8.9229e-02,\n",
       "            7.4367e-01, -8.0693e-01],\n",
       "          [-1.1023e+00, -4.7340e-01, -1.1242e+00,  ..., -3.1980e-01,\n",
       "            1.5241e+00, -5.7050e-01]],\n",
       "\n",
       "         [[ 6.8745e-02, -1.8406e-01, -7.1606e-02,  ..., -3.1181e-02,\n",
       "            1.9129e-01, -4.9275e-02],\n",
       "          [-7.7535e-01,  2.3566e-01, -4.0298e-01,  ...,  5.5079e-01,\n",
       "           -8.3872e-01, -1.8929e-01],\n",
       "          [-7.5092e-01,  3.5143e-01, -8.7308e-01,  ...,  3.0752e-01,\n",
       "           -1.0127e+00, -1.3017e-01],\n",
       "          ...,\n",
       "          [ 3.7611e-01,  3.9529e-01, -3.7087e-01,  ...,  5.9302e-01,\n",
       "           -4.4713e-01, -3.8466e-01],\n",
       "          [-1.0314e-01, -1.0810e-01,  6.4546e-01,  ...,  3.9330e-01,\n",
       "           -2.3982e-01, -6.1272e-01],\n",
       "          [-7.8101e-02, -6.2025e-01,  1.7340e-02,  ...,  4.3926e-01,\n",
       "           -2.8064e-01,  6.4358e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1256e-02, -5.1608e-02,  1.9007e-02,  ..., -7.1646e-02,\n",
       "           -6.8897e-03, -1.0085e-01],\n",
       "          [-8.9544e-02, -1.9730e-01,  5.2907e-01,  ...,  3.0307e-01,\n",
       "           -1.6986e-01, -3.2264e-01],\n",
       "          [-7.8745e-02,  1.1392e-01,  5.5491e-01,  ...,  2.3021e-01,\n",
       "            3.6979e-01, -4.7231e-01],\n",
       "          ...,\n",
       "          [ 6.9128e-01, -3.4649e-01,  2.8227e-01,  ...,  1.1161e-01,\n",
       "           -6.4086e-01,  3.4071e-01],\n",
       "          [ 1.0710e+00, -5.4893e-01,  7.3892e-01,  ...,  1.1171e+00,\n",
       "           -7.9407e-01,  6.2078e-01],\n",
       "          [-6.5202e-02,  4.4958e-02,  9.3074e-01,  ...,  7.3625e-01,\n",
       "            1.5747e-01,  9.5449e-02]],\n",
       "\n",
       "         [[ 6.7747e-02,  2.1723e-02, -3.8099e-02,  ..., -3.9011e-02,\n",
       "            2.3689e-02,  1.1724e-03],\n",
       "          [-7.8540e-01, -8.3013e-01,  2.3576e-01,  ...,  1.2471e+00,\n",
       "           -2.7760e-01, -2.8777e-02],\n",
       "          [ 2.3721e-01,  6.6427e-02, -1.3587e+00,  ...,  1.5889e-01,\n",
       "           -8.3478e-01,  7.9751e-01],\n",
       "          ...,\n",
       "          [-5.0370e-01,  1.0263e-01, -1.8999e-01,  ...,  1.0142e+00,\n",
       "            1.2082e-01, -1.5523e+00],\n",
       "          [-9.2940e-01, -5.2178e-01, -1.0857e+00,  ...,  5.7674e-01,\n",
       "           -1.1994e-01, -7.9351e-01],\n",
       "          [ 1.2232e+00,  9.5918e-01, -2.1017e-02,  ...,  1.4782e-01,\n",
       "           -1.3908e-01,  6.8375e-01]],\n",
       "\n",
       "         [[ 7.6431e-02,  9.5054e-03,  2.5425e-03,  ...,  2.7293e-02,\n",
       "           -7.7690e-02, -6.5371e-02],\n",
       "          [-3.2776e-01,  7.9279e-01, -3.9783e-02,  ..., -1.0361e+00,\n",
       "            7.1417e-01, -2.2441e-01],\n",
       "          [-6.0069e-01,  2.3084e-02,  2.5820e-01,  ..., -7.3405e-01,\n",
       "            5.2939e-01, -8.9682e-01],\n",
       "          ...,\n",
       "          [ 1.0917e+00,  8.7962e-01,  5.9859e-01,  ..., -8.4161e-01,\n",
       "            2.3694e-01,  1.5697e+00],\n",
       "          [ 1.4183e-01,  6.6507e-01,  1.3084e+00,  ..., -9.2870e-01,\n",
       "            4.8764e-01,  5.9188e-02],\n",
       "          [ 2.5219e-01, -6.8311e-02,  7.0401e-01,  ..., -7.2406e-01,\n",
       "            1.4709e-01,  1.8204e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.3259e-03,  1.9788e-02,  1.7440e-02,  ..., -6.6362e-02,\n",
       "           -2.5949e-02,  2.2967e-02],\n",
       "          [-1.7083e+00, -4.1000e-01, -3.5604e-01,  ...,  1.1526e+00,\n",
       "            1.8841e-01,  3.8700e-01],\n",
       "          [-6.1640e-01, -8.2295e-01, -1.6814e-01,  ...,  9.7704e-01,\n",
       "           -3.3866e-01,  2.5587e-01],\n",
       "          ...,\n",
       "          [-1.3543e+00, -3.6762e-01, -3.5448e-01,  ...,  6.5924e-01,\n",
       "           -7.5164e-01, -3.7876e-01],\n",
       "          [-1.8478e+00, -9.2900e-01,  5.5968e-01,  ..., -3.3059e-01,\n",
       "           -5.0625e-01, -6.1427e-02],\n",
       "          [-3.2993e-01, -1.1854e+00,  8.8694e-01,  ...,  5.3400e-01,\n",
       "           -4.3274e-01, -1.1744e-01]],\n",
       "\n",
       "         [[ 3.7895e-02, -7.8421e-03,  2.2710e-02,  ...,  2.6063e-02,\n",
       "           -1.1852e-02,  1.0835e-02],\n",
       "          [-7.5730e-01, -9.8235e-02, -1.2391e+00,  ...,  1.1498e+00,\n",
       "            4.6780e-01,  2.7294e-01],\n",
       "          [-6.7263e-01, -3.5302e-01, -1.0088e+00,  ...,  5.6922e-01,\n",
       "            6.4065e-01, -8.5078e-01],\n",
       "          ...,\n",
       "          [ 5.5397e-01,  1.5838e+00, -8.2688e-01,  ..., -1.1639e+00,\n",
       "            1.2784e-01,  1.2841e+00],\n",
       "          [-9.4849e-01, -1.0093e-01, -1.4392e+00,  ..., -3.8947e-01,\n",
       "            1.1694e+00,  2.3951e-01],\n",
       "          [ 2.7611e-01,  2.6880e-02, -2.1798e+00,  ..., -4.1513e-01,\n",
       "           -3.8929e-01,  1.9915e-01]],\n",
       "\n",
       "         [[ 6.8745e-02, -1.8406e-01, -7.1606e-02,  ..., -3.1181e-02,\n",
       "            1.9129e-01, -4.9275e-02],\n",
       "          [-7.7535e-01,  2.3566e-01, -4.0298e-01,  ...,  5.5079e-01,\n",
       "           -8.3872e-01, -1.8929e-01],\n",
       "          [-7.5092e-01,  3.5143e-01, -8.7308e-01,  ...,  3.0752e-01,\n",
       "           -1.0127e+00, -1.3017e-01],\n",
       "          ...,\n",
       "          [-6.6686e-02, -3.0970e-02, -3.7053e-01,  ...,  1.1382e+00,\n",
       "           -7.7904e-01, -4.2438e-02],\n",
       "          [-4.5967e-02, -7.1837e-01, -1.6134e-01,  ...,  1.2315e+00,\n",
       "           -3.6065e-01,  4.0388e-01],\n",
       "          [-5.8076e-01, -8.9747e-01,  9.0270e-02,  ..., -3.1410e-01,\n",
       "           -2.5024e-01,  1.1230e+00]]]])), (tensor([[[[ 1.0225e+00, -2.6098e-01, -1.4215e-01,  ...,  6.1752e-01,\n",
       "            7.3629e-01, -3.2262e-01],\n",
       "          [-4.0861e+00, -9.1938e-01,  1.7235e+00,  ..., -6.5204e-01,\n",
       "           -6.0544e+00, -4.1151e-02],\n",
       "          [-5.3521e+00, -3.8035e+00,  1.6204e+00,  ..., -9.2202e-01,\n",
       "           -5.6677e+00,  1.0367e+00],\n",
       "          ...,\n",
       "          [-6.7946e+00, -3.2523e+00,  5.3635e-01,  ..., -1.2456e-01,\n",
       "           -5.0291e+00, -1.5328e+00],\n",
       "          [-6.0609e+00, -1.4389e+00,  9.6208e-01,  ..., -9.3454e-01,\n",
       "           -3.3601e+00, -6.2918e-01],\n",
       "          [-4.9125e+00, -1.3991e+00,  1.0346e-01,  ..., -1.5211e+00,\n",
       "           -4.7974e+00, -2.3881e-01]],\n",
       "\n",
       "         [[-1.4261e-01, -7.1774e-02,  1.6139e-01,  ..., -4.4730e-02,\n",
       "           -8.8859e-01, -1.9640e-01],\n",
       "          [-2.3841e+00,  1.8255e+00, -1.8674e-01,  ..., -2.3799e-01,\n",
       "           -1.6121e+00,  6.9660e-01],\n",
       "          [-1.9670e+00,  9.8529e-01, -1.3042e+00,  ..., -1.2552e+00,\n",
       "           -1.8318e+00, -3.2559e-01],\n",
       "          ...,\n",
       "          [-3.2261e-01,  6.2273e-01,  5.7835e-01,  ..., -7.6789e-01,\n",
       "           -5.7703e-01,  3.6745e-01],\n",
       "          [-3.0322e-02, -5.8934e-01,  9.4793e-02,  ...,  1.1560e-01,\n",
       "           -8.6856e-01, -7.0792e-01],\n",
       "          [-2.4798e-01,  5.5603e-01,  9.2151e-01,  ..., -2.2568e-01,\n",
       "           -1.0261e+00, -7.7484e-01]],\n",
       "\n",
       "         [[ 1.9294e-01,  3.0250e-01,  1.1319e+00,  ..., -4.7184e-01,\n",
       "            4.3261e-01, -4.9817e-01],\n",
       "          [-7.0154e-01, -1.1903e+00, -8.7413e-01,  ..., -1.1760e+00,\n",
       "           -2.4876e+00,  1.8829e+00],\n",
       "          [-1.4539e+00, -9.3912e-01, -3.8294e+00,  ...,  1.7074e-01,\n",
       "           -1.6523e+00,  1.3380e+00],\n",
       "          ...,\n",
       "          [ 4.2182e-01, -1.1582e+00, -1.7217e+00,  ...,  1.0685e-01,\n",
       "           -1.7121e+00,  3.4239e+00],\n",
       "          [ 2.0129e+00, -1.4116e+00, -4.0200e+00,  ..., -5.0394e-01,\n",
       "           -2.8577e+00,  1.1422e+00],\n",
       "          [ 1.3528e+00, -7.7337e-01, -4.8203e+00,  ...,  4.4716e-01,\n",
       "           -2.1534e+00,  2.0696e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5608e-01,  6.6680e-02, -2.3160e-01,  ...,  1.1243e-02,\n",
       "            1.5497e-01,  7.7967e-03],\n",
       "          [-2.1770e+00, -3.2147e-02, -1.0677e+00,  ...,  1.2241e+00,\n",
       "           -5.1377e-01, -1.2139e+00],\n",
       "          [-1.1277e+00,  7.5494e-01, -9.1192e-01,  ...,  8.8744e-01,\n",
       "            6.5578e-02, -1.3872e+00],\n",
       "          ...,\n",
       "          [-7.3847e-01,  8.0133e-01, -7.5453e-01,  ...,  2.1442e+00,\n",
       "           -1.2628e+00, -3.3656e-01],\n",
       "          [-2.0560e+00, -6.3739e-01,  2.0634e-01,  ...,  2.0361e+00,\n",
       "            1.3538e+00, -1.0952e+00],\n",
       "          [-1.3778e+00, -7.9139e-01,  6.8800e-01,  ...,  1.9523e+00,\n",
       "           -6.0282e-01, -7.3041e-01]],\n",
       "\n",
       "         [[-3.4629e-01, -2.1823e+00,  1.2192e-01,  ..., -8.7768e-02,\n",
       "           -4.3906e-02,  9.2719e-01],\n",
       "          [ 1.1169e+00,  1.8548e+00, -5.8406e-01,  ..., -2.3569e+00,\n",
       "           -8.7319e-01, -2.9664e-01],\n",
       "          [ 3.9748e-01,  4.0174e+00, -1.6584e+00,  ...,  3.0095e-01,\n",
       "           -1.4238e+00, -1.1653e+00],\n",
       "          ...,\n",
       "          [ 1.0551e+00,  2.5510e+00, -4.4743e-01,  ...,  1.2029e+00,\n",
       "           -2.3307e+00,  2.1949e-01],\n",
       "          [ 1.8716e+00,  2.7128e+00, -2.6236e-01,  ...,  2.6232e+00,\n",
       "           -6.5333e-01, -4.3041e-01],\n",
       "          [ 2.4262e+00,  2.7384e+00,  1.0509e+00,  ...,  2.4941e+00,\n",
       "           -1.3865e+00, -8.1618e-02]],\n",
       "\n",
       "         [[ 3.7377e-01,  6.7300e-02, -1.3961e-01,  ...,  6.6039e-01,\n",
       "            1.2582e-01,  2.6275e-01],\n",
       "          [-1.6159e+00, -9.1976e-01,  2.2513e-01,  ..., -1.0895e+00,\n",
       "            5.5670e-01, -8.1271e-01],\n",
       "          [-1.7085e+00, -8.3162e-01, -6.3185e-01,  ..., -1.0845e+00,\n",
       "            1.0163e+00,  3.9324e-01],\n",
       "          ...,\n",
       "          [-7.5422e-01, -8.2579e-01, -4.6966e-02,  ..., -9.8567e-01,\n",
       "            6.9801e-01, -1.1072e+00],\n",
       "          [-1.9383e+00, -1.1568e+00, -5.0171e-01,  ..., -7.0271e-01,\n",
       "            1.8364e+00,  1.6553e-01],\n",
       "          [-1.9006e+00,  5.3561e-01,  1.5930e-01,  ...,  1.9625e-01,\n",
       "            7.2311e-01,  2.7645e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0225e+00, -2.6098e-01, -1.4215e-01,  ...,  6.1752e-01,\n",
       "            7.3629e-01, -3.2262e-01],\n",
       "          [-4.0861e+00, -9.1938e-01,  1.7235e+00,  ..., -6.5204e-01,\n",
       "           -6.0544e+00, -4.1151e-02],\n",
       "          [-5.3521e+00, -3.8035e+00,  1.6204e+00,  ..., -9.2202e-01,\n",
       "           -5.6677e+00,  1.0367e+00],\n",
       "          ...,\n",
       "          [-6.7111e+00, -1.9947e+00,  1.0038e+00,  ..., -1.7077e+00,\n",
       "           -3.5024e+00, -8.9564e-01],\n",
       "          [-5.2006e+00, -1.4450e+00, -2.4406e-01,  ..., -1.8804e+00,\n",
       "           -3.7774e+00,  3.6080e-04],\n",
       "          [-4.8577e+00, -1.7717e+00, -5.5026e-01,  ..., -1.1812e+00,\n",
       "           -3.9851e+00,  7.7513e-01]],\n",
       "\n",
       "         [[-1.4261e-01, -7.1774e-02,  1.6139e-01,  ..., -4.4730e-02,\n",
       "           -8.8859e-01, -1.9640e-01],\n",
       "          [-2.3841e+00,  1.8255e+00, -1.8674e-01,  ..., -2.3799e-01,\n",
       "           -1.6121e+00,  6.9660e-01],\n",
       "          [-1.9670e+00,  9.8529e-01, -1.3042e+00,  ..., -1.2552e+00,\n",
       "           -1.8318e+00, -3.2559e-01],\n",
       "          ...,\n",
       "          [-1.3798e+00,  1.1220e+00, -1.0032e+00,  ..., -6.7121e-01,\n",
       "           -5.8745e-01,  5.2878e-01],\n",
       "          [-6.5550e-01,  7.3570e-01,  3.2738e-01,  ..., -1.0595e+00,\n",
       "           -6.6586e-01, -1.7961e+00],\n",
       "          [-5.8650e-03,  1.2300e+00, -7.6776e-01,  ..., -8.8806e-01,\n",
       "           -6.1619e-01, -1.5040e+00]],\n",
       "\n",
       "         [[ 1.9294e-01,  3.0250e-01,  1.1319e+00,  ..., -4.7184e-01,\n",
       "            4.3261e-01, -4.9817e-01],\n",
       "          [-7.0154e-01, -1.1903e+00, -8.7413e-01,  ..., -1.1760e+00,\n",
       "           -2.4876e+00,  1.8829e+00],\n",
       "          [-1.4539e+00, -9.3912e-01, -3.8294e+00,  ...,  1.7074e-01,\n",
       "           -1.6523e+00,  1.3380e+00],\n",
       "          ...,\n",
       "          [ 9.1143e-01, -1.6072e+00, -1.8853e+00,  ...,  1.3975e-01,\n",
       "           -2.2159e+00,  2.9316e+00],\n",
       "          [ 1.9037e+00, -1.3022e+00, -4.7516e+00,  ...,  1.0390e-01,\n",
       "           -2.5210e+00,  1.4641e+00],\n",
       "          [ 1.2663e+00, -9.1350e-02, -3.9132e+00,  ..., -3.1634e-01,\n",
       "           -1.7452e+00,  1.0003e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5608e-01,  6.6680e-02, -2.3160e-01,  ...,  1.1243e-02,\n",
       "            1.5497e-01,  7.7967e-03],\n",
       "          [-2.1770e+00, -3.2147e-02, -1.0677e+00,  ...,  1.2241e+00,\n",
       "           -5.1377e-01, -1.2139e+00],\n",
       "          [-1.1277e+00,  7.5494e-01, -9.1192e-01,  ...,  8.8744e-01,\n",
       "            6.5578e-02, -1.3872e+00],\n",
       "          ...,\n",
       "          [-1.4567e+00, -1.7064e-01, -3.7620e-01,  ...,  2.1290e+00,\n",
       "            4.6980e-01, -1.1445e+00],\n",
       "          [-1.1497e+00, -9.3703e-01,  4.9647e-01,  ...,  2.3771e+00,\n",
       "           -5.6354e-01, -1.0615e+00],\n",
       "          [-1.2994e+00,  1.3541e+00, -1.7473e-01,  ...,  1.7483e+00,\n",
       "           -8.9261e-01, -1.5360e+00]],\n",
       "\n",
       "         [[-3.4629e-01, -2.1823e+00,  1.2192e-01,  ..., -8.7768e-02,\n",
       "           -4.3906e-02,  9.2719e-01],\n",
       "          [ 1.1169e+00,  1.8548e+00, -5.8406e-01,  ..., -2.3569e+00,\n",
       "           -8.7319e-01, -2.9664e-01],\n",
       "          [ 3.9748e-01,  4.0174e+00, -1.6584e+00,  ...,  3.0095e-01,\n",
       "           -1.4238e+00, -1.1653e+00],\n",
       "          ...,\n",
       "          [ 1.3548e+00,  2.4976e+00, -5.2370e-01,  ...,  9.6187e-01,\n",
       "           -2.3310e+00,  1.0699e-01],\n",
       "          [ 2.2972e+00,  2.6055e+00,  1.3310e+00,  ...,  1.8188e+00,\n",
       "           -2.0343e+00,  3.1105e-01],\n",
       "          [ 9.7121e-01,  1.4562e+00, -7.5268e-01,  ...,  3.1464e+00,\n",
       "           -1.0377e+00, -7.8325e-01]],\n",
       "\n",
       "         [[ 3.7377e-01,  6.7300e-02, -1.3961e-01,  ...,  6.6039e-01,\n",
       "            1.2582e-01,  2.6275e-01],\n",
       "          [-1.6159e+00, -9.1976e-01,  2.2513e-01,  ..., -1.0895e+00,\n",
       "            5.5670e-01, -8.1271e-01],\n",
       "          [-1.7085e+00, -8.3162e-01, -6.3185e-01,  ..., -1.0845e+00,\n",
       "            1.0163e+00,  3.9324e-01],\n",
       "          ...,\n",
       "          [-8.8343e-01, -8.7929e-01,  7.2782e-01,  ..., -1.1761e+00,\n",
       "           -1.5803e-01, -3.8684e-01],\n",
       "          [-1.9637e+00,  8.5178e-02, -6.1825e-02,  ..., -2.3890e-02,\n",
       "            9.5385e-01,  8.2656e-01],\n",
       "          [-1.9750e+00,  7.7108e-01,  7.0830e-01,  ..., -4.5365e-01,\n",
       "            2.6948e-01,  3.0600e-01]]]]), tensor([[[[-3.0141e-02,  4.0703e-02, -5.5849e-02,  ..., -1.3770e-02,\n",
       "           -1.6352e-03,  2.3996e-02],\n",
       "          [-1.2551e-02, -4.0474e-01, -2.0677e-01,  ...,  6.8313e-01,\n",
       "           -3.4719e-01, -9.1885e-02],\n",
       "          [ 2.2239e-01, -5.8836e-01,  1.4129e-01,  ...,  5.1488e-02,\n",
       "            1.9850e-01, -3.5654e-01],\n",
       "          ...,\n",
       "          [ 7.6011e-01, -6.8763e-02,  3.5144e-02,  ...,  3.1157e-01,\n",
       "           -1.5069e-01, -3.5883e-01],\n",
       "          [-2.0445e-01, -3.2821e-01, -3.6779e-02,  ...,  2.8237e-01,\n",
       "            8.4353e-01,  2.7049e-01],\n",
       "          [-2.9056e-01, -6.3379e-01, -3.6756e-01,  ...,  4.0881e-02,\n",
       "            1.7034e-01,  5.1142e-01]],\n",
       "\n",
       "         [[ 7.7639e-03, -2.9706e-02,  2.4699e-02,  ...,  1.9086e-02,\n",
       "           -4.3664e-02,  2.0150e-02],\n",
       "          [ 5.1171e-01, -4.0971e-01, -1.4064e+00,  ..., -4.0179e-01,\n",
       "            1.5097e-01,  2.2897e-02],\n",
       "          [ 4.9334e-01, -4.4025e-01, -1.7371e+00,  ...,  1.0483e+00,\n",
       "            9.3214e-01, -8.8534e-01],\n",
       "          ...,\n",
       "          [ 1.5243e-01,  6.4150e-01, -2.7283e+00,  ..., -1.0657e+00,\n",
       "           -5.1740e-01, -9.4287e-01],\n",
       "          [ 1.3009e+00, -1.6304e-01, -1.6552e+00,  ..., -6.3302e-01,\n",
       "            2.2800e-01,  3.9024e-02],\n",
       "          [ 1.3066e+00,  3.2656e-01, -3.5321e+00,  ...,  1.1229e+00,\n",
       "            1.0793e+00,  5.3708e-02]],\n",
       "\n",
       "         [[ 4.0643e-02, -2.7453e-02,  5.3587e-02,  ...,  2.3040e-02,\n",
       "           -6.4741e-03,  7.5894e-03],\n",
       "          [-5.8425e-01, -7.7572e-01,  2.1730e-01,  ..., -2.0238e-01,\n",
       "           -8.7631e-02, -1.1003e+00],\n",
       "          [-6.3011e-01,  1.0527e+00, -4.2522e-01,  ..., -9.3451e-01,\n",
       "            8.0163e-01, -4.4889e-01],\n",
       "          ...,\n",
       "          [ 2.3169e-01, -2.3811e+00,  1.1863e+00,  ...,  6.2093e-01,\n",
       "            1.8621e-01,  8.8274e-01],\n",
       "          [-3.8667e-01, -1.6694e+00,  3.5176e-01,  ..., -8.6020e-01,\n",
       "           -2.6087e-01,  3.8839e-01],\n",
       "          [ 7.4657e-01,  2.4278e-01,  1.9139e-01,  ...,  3.3426e-01,\n",
       "            1.6190e+00, -1.3767e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9379e-01,  8.5108e-02,  5.7978e-02,  ...,  4.2688e-02,\n",
       "            3.5037e-02, -1.3477e-01],\n",
       "          [ 2.8828e-01, -5.1470e-01, -1.3935e+00,  ..., -2.9423e-01,\n",
       "            6.9849e-01, -2.5133e-01],\n",
       "          [ 5.7110e-01, -1.0825e-01, -3.1062e-01,  ..., -2.6026e-01,\n",
       "            1.2018e+00, -9.1843e-01],\n",
       "          ...,\n",
       "          [ 1.0590e+00,  3.7766e-01,  2.4655e-01,  ...,  2.5930e+00,\n",
       "           -2.4943e-03, -5.0017e-01],\n",
       "          [ 8.8989e-01, -2.8692e-01, -5.7039e-01,  ...,  7.3016e-01,\n",
       "            1.6443e-01, -4.0737e-01],\n",
       "          [-2.7203e-01,  7.4183e-01, -9.1425e-01,  ...,  4.1118e-01,\n",
       "            5.5327e-01,  1.3136e-01]],\n",
       "\n",
       "         [[-5.8751e-01, -1.4811e-03,  4.8367e-02,  ..., -9.5946e-03,\n",
       "            1.7851e-02, -7.5577e-03],\n",
       "          [-1.8416e+00, -1.2987e+00, -1.9768e-01,  ...,  1.1255e-02,\n",
       "           -4.4920e-01,  1.4130e+00],\n",
       "          [-2.1241e+00,  6.7685e-01, -2.5526e-01,  ..., -2.9741e-01,\n",
       "           -3.8258e-03,  5.0660e-01],\n",
       "          ...,\n",
       "          [-2.7026e+00,  1.1128e-01, -1.2104e+00,  ...,  3.8994e-01,\n",
       "            9.1334e-01, -5.4311e-01],\n",
       "          [-7.8086e-01,  4.2890e-01, -8.3204e-02,  ...,  3.5577e-01,\n",
       "            8.2441e-01,  7.1306e-01],\n",
       "          [-2.2139e+00,  1.6482e-01, -7.4932e-01,  ...,  9.4811e-02,\n",
       "            2.3441e-02, -1.2573e+00]],\n",
       "\n",
       "         [[ 1.4457e-02,  8.0386e-02, -3.8783e-02,  ...,  5.8079e-02,\n",
       "            3.2031e-02, -5.1529e-02],\n",
       "          [ 1.9352e-01, -2.8583e-01, -2.2119e-01,  ...,  3.9840e-01,\n",
       "           -3.2101e+00, -1.1686e-01],\n",
       "          [-3.3025e-01, -1.0156e-01, -6.0255e-01,  ..., -6.1179e-01,\n",
       "           -1.3078e+00,  3.7803e-01],\n",
       "          ...,\n",
       "          [-3.7404e-01,  6.4202e-03, -2.2777e-01,  ..., -3.9829e-01,\n",
       "           -1.7016e+00, -6.7429e-01],\n",
       "          [-1.4625e-01,  6.1227e-02,  1.1609e+00,  ...,  2.8255e-01,\n",
       "           -6.6435e-01, -9.1281e-01],\n",
       "          [ 8.7975e-01, -1.0663e+00,  7.3564e-03,  ...,  1.3886e-01,\n",
       "           -1.9357e+00,  4.6310e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.0141e-02,  4.0703e-02, -5.5849e-02,  ..., -1.3770e-02,\n",
       "           -1.6352e-03,  2.3996e-02],\n",
       "          [-1.2551e-02, -4.0474e-01, -2.0677e-01,  ...,  6.8313e-01,\n",
       "           -3.4719e-01, -9.1885e-02],\n",
       "          [ 2.2239e-01, -5.8836e-01,  1.4129e-01,  ...,  5.1488e-02,\n",
       "            1.9850e-01, -3.5654e-01],\n",
       "          ...,\n",
       "          [ 2.8493e-01,  2.0491e-01,  4.0472e-01,  ...,  4.2392e-02,\n",
       "            9.9272e-01, -3.8559e-01],\n",
       "          [-1.9608e-02, -3.9032e-01, -5.9926e-01,  ..., -2.1188e-02,\n",
       "            4.9456e-01,  5.3034e-01],\n",
       "          [ 9.7163e-02,  2.7446e-01, -6.3073e-01,  ..., -1.8509e-01,\n",
       "            3.3057e-01, -8.1016e-02]],\n",
       "\n",
       "         [[ 7.7639e-03, -2.9706e-02,  2.4699e-02,  ...,  1.9086e-02,\n",
       "           -4.3664e-02,  2.0150e-02],\n",
       "          [ 5.1171e-01, -4.0971e-01, -1.4064e+00,  ..., -4.0179e-01,\n",
       "            1.5097e-01,  2.2897e-02],\n",
       "          [ 4.9334e-01, -4.4025e-01, -1.7371e+00,  ...,  1.0483e+00,\n",
       "            9.3214e-01, -8.8534e-01],\n",
       "          ...,\n",
       "          [-1.5734e-01,  5.8394e-01, -7.3002e-01,  ...,  1.2763e-03,\n",
       "           -1.2470e+00,  6.1570e-01],\n",
       "          [ 1.4853e+00,  5.1466e-01, -3.2256e+00,  ...,  1.0299e+00,\n",
       "            4.6468e-01, -5.1291e-02],\n",
       "          [ 1.6336e+00, -1.6401e-02, -2.0829e+00,  ...,  4.3822e-01,\n",
       "           -3.7791e-01, -6.2113e-01]],\n",
       "\n",
       "         [[ 4.0643e-02, -2.7453e-02,  5.3587e-02,  ...,  2.3040e-02,\n",
       "           -6.4741e-03,  7.5894e-03],\n",
       "          [-5.8425e-01, -7.7572e-01,  2.1730e-01,  ..., -2.0238e-01,\n",
       "           -8.7631e-02, -1.1003e+00],\n",
       "          [-6.3011e-01,  1.0527e+00, -4.2522e-01,  ..., -9.3451e-01,\n",
       "            8.0163e-01, -4.4889e-01],\n",
       "          ...,\n",
       "          [-1.2312e-01, -1.8621e+00, -7.8470e-01,  ...,  1.4891e+00,\n",
       "           -8.8267e-01,  5.2374e-01],\n",
       "          [ 1.2289e+00, -1.4904e-01,  4.3938e-01,  ...,  3.5691e-01,\n",
       "            1.1809e+00, -1.2881e+00],\n",
       "          [ 6.5388e-01, -1.9256e-01,  5.4137e-01,  ...,  7.0554e-01,\n",
       "            4.0136e-01,  3.9996e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.9379e-01,  8.5108e-02,  5.7978e-02,  ...,  4.2688e-02,\n",
       "            3.5037e-02, -1.3477e-01],\n",
       "          [ 2.8828e-01, -5.1470e-01, -1.3935e+00,  ..., -2.9423e-01,\n",
       "            6.9849e-01, -2.5133e-01],\n",
       "          [ 5.7110e-01, -1.0825e-01, -3.1062e-01,  ..., -2.6026e-01,\n",
       "            1.2018e+00, -9.1843e-01],\n",
       "          ...,\n",
       "          [-1.4406e-01, -4.0282e-02,  3.9017e-01,  ...,  1.8150e+00,\n",
       "           -1.0981e+00, -6.6386e-01],\n",
       "          [ 9.3798e-02,  6.0254e-01, -9.8749e-01,  ...,  7.1478e-01,\n",
       "           -1.7794e-01, -9.2844e-02],\n",
       "          [ 4.0170e-01,  5.5083e-01, -6.9468e-01,  ...,  1.6780e-01,\n",
       "            4.1694e-01,  3.4649e-01]],\n",
       "\n",
       "         [[-5.8751e-01, -1.4811e-03,  4.8367e-02,  ..., -9.5946e-03,\n",
       "            1.7851e-02, -7.5577e-03],\n",
       "          [-1.8416e+00, -1.2987e+00, -1.9768e-01,  ...,  1.1255e-02,\n",
       "           -4.4920e-01,  1.4130e+00],\n",
       "          [-2.1241e+00,  6.7685e-01, -2.5526e-01,  ..., -2.9741e-01,\n",
       "           -3.8258e-03,  5.0660e-01],\n",
       "          ...,\n",
       "          [-1.1538e+00, -8.3309e-01, -1.1366e+00,  ...,  2.3603e-01,\n",
       "           -6.1338e-02, -1.2788e+00],\n",
       "          [-2.0110e+00,  2.9200e-01, -6.7064e-01,  ...,  1.5147e-01,\n",
       "           -2.2564e-01, -1.2647e+00],\n",
       "          [-1.7168e+00, -6.2667e-02, -5.3769e-01,  ..., -9.7716e-01,\n",
       "            4.3852e-01,  7.1761e-01]],\n",
       "\n",
       "         [[ 1.4457e-02,  8.0386e-02, -3.8783e-02,  ...,  5.8079e-02,\n",
       "            3.2031e-02, -5.1529e-02],\n",
       "          [ 1.9352e-01, -2.8583e-01, -2.2119e-01,  ...,  3.9840e-01,\n",
       "           -3.2101e+00, -1.1686e-01],\n",
       "          [-3.3025e-01, -1.0156e-01, -6.0255e-01,  ..., -6.1179e-01,\n",
       "           -1.3078e+00,  3.7803e-01],\n",
       "          ...,\n",
       "          [-5.3995e-01,  3.1688e-02,  3.0687e-01,  ..., -2.7516e-01,\n",
       "           -2.3773e+00,  3.8270e-01],\n",
       "          [ 6.8967e-01, -5.8646e-01, -3.5520e-01,  ..., -1.7711e-01,\n",
       "           -2.0081e+00, -2.6702e-02],\n",
       "          [-2.5609e-02, -3.0073e-01,  2.0181e-01,  ..., -1.0191e+00,\n",
       "            5.6934e-01,  7.1767e-01]]]])), (tensor([[[[-2.6219e-02, -2.3464e+00,  1.7290e-01,  ..., -2.3472e-01,\n",
       "           -1.8784e-01,  6.4591e-02],\n",
       "          [-9.9440e-01,  4.9520e+00,  1.6863e+00,  ..., -8.1644e-02,\n",
       "            1.1980e+00,  7.8224e-01],\n",
       "          [-9.7379e-01,  5.3265e+00,  1.2723e+00,  ...,  9.8039e-02,\n",
       "           -2.2452e-02, -3.8018e-01],\n",
       "          ...,\n",
       "          [-9.0848e-01,  5.6022e+00,  1.5002e-01,  ..., -2.4535e+00,\n",
       "           -6.3772e-01, -3.9684e-01],\n",
       "          [ 3.8504e-01,  4.6597e+00,  5.6131e-01,  ..., -1.4686e+00,\n",
       "           -6.2075e-01, -3.5228e-01],\n",
       "          [-4.8997e-01,  5.3775e+00, -1.1574e+00,  ..., -5.1703e-01,\n",
       "           -1.3451e+00, -6.3926e-02]],\n",
       "\n",
       "         [[-7.9621e-01,  2.1614e-01,  4.7777e-01,  ..., -5.2559e-01,\n",
       "            1.0660e+00,  1.1141e+00],\n",
       "          [-1.0150e+00,  1.5689e-01,  9.8726e-01,  ...,  4.5106e-01,\n",
       "            1.5577e+00, -1.0727e+00],\n",
       "          [-2.5283e-01,  1.4459e-01,  2.7414e-01,  ...,  1.4876e+00,\n",
       "            1.6565e+00, -2.3525e+00],\n",
       "          ...,\n",
       "          [ 2.2516e-01,  1.1750e-01, -1.2927e+00,  ...,  2.4908e-01,\n",
       "            4.9330e-01, -9.8875e-01],\n",
       "          [ 7.4387e-01,  2.3059e-01, -1.8871e-01,  ..., -8.5308e-01,\n",
       "            6.0314e-02, -2.0652e+00],\n",
       "          [ 1.1384e+00,  2.4038e-01, -4.1852e-01,  ...,  8.7366e-01,\n",
       "            2.0398e+00, -1.4002e+00]],\n",
       "\n",
       "         [[-8.5166e-01,  4.7201e-01,  2.3665e-02,  ...,  5.0047e-01,\n",
       "           -2.2706e-01,  1.1627e+00],\n",
       "          [ 1.7545e+00, -1.0227e+00,  7.3845e-03,  ...,  5.9884e-02,\n",
       "            7.0759e-01,  2.1299e-02],\n",
       "          [ 1.0181e+00, -5.9871e-01,  4.2961e-02,  ..., -6.8642e-01,\n",
       "            1.3873e+00,  6.1362e-01],\n",
       "          ...,\n",
       "          [ 1.7102e+00,  7.9361e-01,  1.5794e+00,  ...,  1.4250e-02,\n",
       "            1.7019e+00, -8.6892e-01],\n",
       "          [ 8.6923e-01, -4.0751e-02,  1.1914e+00,  ...,  8.4519e-01,\n",
       "            1.0267e+00,  9.0550e-02],\n",
       "          [ 1.3983e+00, -5.3094e-01,  1.8935e+00,  ...,  8.8416e-01,\n",
       "            6.1477e-01, -8.0667e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9489e-01, -1.1737e-01,  1.4996e-01,  ...,  1.8549e-01,\n",
       "            1.7379e+00, -2.8689e+00],\n",
       "          [ 2.9848e-01,  3.6325e-01, -1.0361e+00,  ..., -9.1926e-01,\n",
       "           -4.6307e+00,  4.5633e+00],\n",
       "          [ 1.2727e+00,  4.1608e-01, -7.1292e-01,  ...,  2.1065e-01,\n",
       "           -4.9022e+00,  5.2918e+00],\n",
       "          ...,\n",
       "          [-4.5735e-01, -5.7328e-01, -8.2383e-01,  ..., -4.3504e-01,\n",
       "           -4.7238e+00,  5.0117e+00],\n",
       "          [ 4.3719e-01,  1.9292e-01, -4.1618e-01,  ..., -1.0658e-01,\n",
       "           -3.8056e+00,  4.7565e+00],\n",
       "          [-1.9097e-01,  1.5559e-01, -1.1793e+00,  ...,  4.6425e-01,\n",
       "           -4.0537e+00,  4.0920e+00]],\n",
       "\n",
       "         [[ 1.9490e-01,  3.6346e-01,  2.2225e-01,  ..., -2.2882e-01,\n",
       "            2.0427e-02, -1.4902e-01],\n",
       "          [-1.0555e+00, -1.3612e+00, -3.5712e-01,  ...,  3.5669e-01,\n",
       "            8.2591e-01,  6.9813e-01],\n",
       "          [-6.9261e-01, -7.1890e-01, -4.4546e-01,  ...,  4.4647e-01,\n",
       "            8.5138e-02,  6.3694e-01],\n",
       "          ...,\n",
       "          [-1.7186e+00, -1.1115e+00, -8.5079e-01,  ..., -5.5613e-01,\n",
       "            6.0236e-01,  1.4498e+00],\n",
       "          [-1.3878e+00, -1.1798e+00, -1.2779e+00,  ...,  5.4728e-01,\n",
       "            8.2437e-02,  1.9195e+00],\n",
       "          [-1.0245e+00, -9.2662e-01, -3.6492e-01,  ...,  4.9089e-02,\n",
       "           -4.5711e-01,  1.3195e+00]],\n",
       "\n",
       "         [[ 3.7068e-01,  1.1181e-01,  6.2315e-01,  ...,  5.2661e-01,\n",
       "            5.7319e-01, -3.3444e-01],\n",
       "          [ 9.7697e-01, -9.6007e-01, -3.1213e-01,  ..., -1.3569e+00,\n",
       "           -4.1630e+00, -1.1700e-02],\n",
       "          [ 1.1135e+00, -1.2668e+00,  4.2678e-01,  ..., -1.5629e+00,\n",
       "           -4.1928e+00,  6.4668e-01],\n",
       "          ...,\n",
       "          [-1.7161e-02,  1.4803e-01, -1.6789e+00,  ..., -1.7150e+00,\n",
       "           -2.9922e+00,  4.8074e-01],\n",
       "          [-1.9055e-01,  9.4711e-01, -1.4945e+00,  ..., -8.6859e-01,\n",
       "           -2.8590e+00,  7.7180e-01],\n",
       "          [ 7.5219e-01, -4.6234e-02, -8.1465e-01,  ..., -8.4776e-02,\n",
       "           -3.8686e+00, -8.2346e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.6219e-02, -2.3464e+00,  1.7290e-01,  ..., -2.3472e-01,\n",
       "           -1.8784e-01,  6.4591e-02],\n",
       "          [-9.9440e-01,  4.9520e+00,  1.6863e+00,  ..., -8.1644e-02,\n",
       "            1.1980e+00,  7.8224e-01],\n",
       "          [-9.7379e-01,  5.3265e+00,  1.2723e+00,  ...,  9.8039e-02,\n",
       "           -2.2452e-02, -3.8018e-01],\n",
       "          ...,\n",
       "          [-2.5940e-01,  5.4535e+00,  2.3873e-01,  ..., -1.6086e+00,\n",
       "           -5.1050e-01,  5.7829e-01],\n",
       "          [-6.1820e-01,  5.5990e+00, -1.3478e+00,  ..., -7.3517e-01,\n",
       "           -1.2840e+00, -1.2599e-01],\n",
       "          [-4.0166e-01,  3.9601e+00, -5.8928e-01,  ..., -9.8751e-01,\n",
       "            3.1724e-01, -2.4062e-01]],\n",
       "\n",
       "         [[-7.9621e-01,  2.1614e-01,  4.7777e-01,  ..., -5.2559e-01,\n",
       "            1.0660e+00,  1.1141e+00],\n",
       "          [-1.0150e+00,  1.5689e-01,  9.8726e-01,  ...,  4.5106e-01,\n",
       "            1.5577e+00, -1.0727e+00],\n",
       "          [-2.5283e-01,  1.4459e-01,  2.7414e-01,  ...,  1.4876e+00,\n",
       "            1.6565e+00, -2.3525e+00],\n",
       "          ...,\n",
       "          [ 3.8616e-01,  4.8363e-01, -7.6170e-01,  ..., -5.1951e-01,\n",
       "            6.4094e-01, -9.6902e-01],\n",
       "          [ 9.5487e-01,  8.9032e-01,  4.3643e-03,  ...,  9.1656e-01,\n",
       "            1.6553e+00, -8.7583e-01],\n",
       "          [ 7.4229e-01,  7.8324e-02, -1.1869e-01,  ..., -4.6644e-01,\n",
       "            2.7441e+00, -1.5139e+00]],\n",
       "\n",
       "         [[-8.5166e-01,  4.7201e-01,  2.3665e-02,  ...,  5.0047e-01,\n",
       "           -2.2706e-01,  1.1627e+00],\n",
       "          [ 1.7545e+00, -1.0227e+00,  7.3845e-03,  ...,  5.9884e-02,\n",
       "            7.0759e-01,  2.1299e-02],\n",
       "          [ 1.0181e+00, -5.9871e-01,  4.2961e-02,  ..., -6.8642e-01,\n",
       "            1.3873e+00,  6.1362e-01],\n",
       "          ...,\n",
       "          [ 6.5799e-01,  3.1905e-01,  1.5691e+00,  ...,  2.9370e-01,\n",
       "            5.3822e-01, -1.0085e-02],\n",
       "          [ 6.6265e-01, -2.3661e-02,  1.8195e+00,  ...,  7.3394e-01,\n",
       "            7.3746e-02, -4.3631e-01],\n",
       "          [ 1.1610e-01, -4.5384e-01,  1.3689e+00,  ...,  6.0419e-01,\n",
       "            9.4287e-01, -2.6463e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9489e-01, -1.1737e-01,  1.4996e-01,  ...,  1.8549e-01,\n",
       "            1.7379e+00, -2.8689e+00],\n",
       "          [ 2.9848e-01,  3.6325e-01, -1.0361e+00,  ..., -9.1926e-01,\n",
       "           -4.6307e+00,  4.5633e+00],\n",
       "          [ 1.2727e+00,  4.1608e-01, -7.1292e-01,  ...,  2.1065e-01,\n",
       "           -4.9022e+00,  5.2918e+00],\n",
       "          ...,\n",
       "          [ 9.2147e-01,  1.6389e-02, -4.4748e-01,  ..., -1.7361e-01,\n",
       "           -4.1739e+00,  5.4493e+00],\n",
       "          [ 1.8827e-01,  1.5591e-01, -1.1012e+00,  ...,  1.3410e-01,\n",
       "           -3.8323e+00,  4.2072e+00],\n",
       "          [ 3.9089e-01,  3.4086e-01, -3.9981e-01,  ..., -2.2204e-02,\n",
       "           -4.3952e+00,  4.6243e+00]],\n",
       "\n",
       "         [[ 1.9490e-01,  3.6346e-01,  2.2225e-01,  ..., -2.2882e-01,\n",
       "            2.0427e-02, -1.4902e-01],\n",
       "          [-1.0555e+00, -1.3612e+00, -3.5712e-01,  ...,  3.5669e-01,\n",
       "            8.2591e-01,  6.9813e-01],\n",
       "          [-6.9261e-01, -7.1890e-01, -4.4546e-01,  ...,  4.4647e-01,\n",
       "            8.5138e-02,  6.3694e-01],\n",
       "          ...,\n",
       "          [-1.2072e+00, -8.9963e-01, -6.6234e-01,  ...,  7.6009e-01,\n",
       "            7.3728e-01,  1.5392e+00],\n",
       "          [-6.6113e-01, -1.1225e+00, -4.5297e-01,  ...,  6.9010e-01,\n",
       "           -8.0011e-02,  1.0472e+00],\n",
       "          [-1.0274e+00, -4.7799e-01, -1.0947e-01,  ...,  8.4825e-01,\n",
       "           -7.3461e-01,  1.0766e+00]],\n",
       "\n",
       "         [[ 3.7068e-01,  1.1181e-01,  6.2315e-01,  ...,  5.2661e-01,\n",
       "            5.7319e-01, -3.3444e-01],\n",
       "          [ 9.7697e-01, -9.6007e-01, -3.1213e-01,  ..., -1.3569e+00,\n",
       "           -4.1630e+00, -1.1700e-02],\n",
       "          [ 1.1135e+00, -1.2668e+00,  4.2678e-01,  ..., -1.5629e+00,\n",
       "           -4.1928e+00,  6.4668e-01],\n",
       "          ...,\n",
       "          [ 4.5401e-02,  1.1736e+00, -7.7501e-01,  ..., -7.0781e-01,\n",
       "           -2.8090e+00,  6.9356e-01],\n",
       "          [ 6.0757e-01,  5.7285e-02, -7.0556e-01,  ..., -7.3918e-02,\n",
       "           -3.7446e+00,  5.7823e-01],\n",
       "          [ 6.0644e-02, -1.8567e-01, -2.0090e-01,  ..., -8.0274e-01,\n",
       "           -4.2442e+00,  3.8493e-01]]]]), tensor([[[[ 7.0340e-02, -1.3298e-02, -2.6878e-02,  ...,  1.2709e-01,\n",
       "           -8.1964e-02, -4.6323e-02],\n",
       "          [-8.7537e-01,  5.8726e-01,  5.0024e-01,  ...,  5.4489e-01,\n",
       "            3.7365e-01,  7.3212e-01],\n",
       "          [-4.3954e-01, -5.2312e-01,  9.7326e-02,  ...,  8.1977e-01,\n",
       "            2.3033e-01,  1.7002e-01],\n",
       "          ...,\n",
       "          [-3.5250e-01,  1.6706e+00, -1.4326e-01,  ...,  1.0905e+00,\n",
       "            3.6225e-01, -2.8411e-01],\n",
       "          [-1.3441e+00,  3.5256e-01,  2.0035e-02,  ..., -7.7327e-01,\n",
       "           -7.1671e-01,  8.0219e-01],\n",
       "          [-2.0987e+00,  5.9603e-01,  7.8617e-01,  ..., -1.1183e+00,\n",
       "           -1.1565e+00,  5.9873e-01]],\n",
       "\n",
       "         [[ 9.8191e-03,  3.1127e-02,  5.5325e-02,  ...,  2.3694e-03,\n",
       "           -3.6194e-03,  1.1651e-02],\n",
       "          [ 2.3379e-01,  1.0323e+00,  5.1448e-01,  ...,  2.7895e-01,\n",
       "           -5.3069e-01,  1.1096e-01],\n",
       "          [-8.3193e-01,  7.8568e-01,  2.0929e-01,  ..., -1.2180e+00,\n",
       "           -6.8237e-01,  8.1226e-02],\n",
       "          ...,\n",
       "          [-1.0441e+00,  9.7078e-01, -1.1152e-01,  ...,  1.1298e-01,\n",
       "           -1.3662e+00,  2.6663e-01],\n",
       "          [-1.2697e+00, -4.3146e-01, -3.0558e-01,  ..., -7.9704e-01,\n",
       "            8.5797e-01, -5.2695e-01],\n",
       "          [-6.9060e-03,  7.0861e-01,  6.3989e-01,  ...,  6.6860e-01,\n",
       "           -1.5501e-01, -2.1687e-01]],\n",
       "\n",
       "         [[ 5.1820e-02, -3.8483e-02,  5.5745e-02,  ...,  5.8374e-02,\n",
       "           -6.5499e-02, -6.8603e-02],\n",
       "          [ 3.5901e-01,  6.9082e-01,  1.3846e+00,  ...,  7.2112e-01,\n",
       "           -6.4698e-01,  1.3922e+00],\n",
       "          [-2.6271e-01,  8.7886e-03,  4.5272e-01,  ...,  2.2419e-01,\n",
       "           -4.7247e-01,  5.5424e-01],\n",
       "          ...,\n",
       "          [-1.7000e+00, -2.6120e-02, -2.0297e+00,  ...,  8.3406e-02,\n",
       "            9.3033e-02,  1.1499e-01],\n",
       "          [-5.5037e-01, -4.0938e-01, -1.1132e+00,  ..., -3.5406e-01,\n",
       "           -3.6377e-01,  5.6613e-01],\n",
       "          [ 3.5279e-02, -1.0149e+00, -9.4005e-01,  ..., -5.4082e-01,\n",
       "           -6.3852e-01,  1.2094e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.9928e-02, -4.0799e-02,  3.3523e-02,  ..., -8.5036e-02,\n",
       "            3.3863e-02,  7.3905e-03],\n",
       "          [ 7.7646e-01,  1.1034e+00,  1.1277e+00,  ...,  7.1504e-02,\n",
       "            6.5833e-01,  1.0784e+00],\n",
       "          [ 1.6653e+00,  6.6364e-01,  1.1367e+00,  ..., -6.9510e-03,\n",
       "           -2.7777e-01,  5.0956e-01],\n",
       "          ...,\n",
       "          [ 1.1349e+00,  1.1558e+00, -1.4620e+00,  ..., -2.2028e-01,\n",
       "            1.4338e+00,  3.6682e-01],\n",
       "          [ 1.1150e+00,  2.7730e-01, -2.9615e-01,  ..., -5.0933e-01,\n",
       "            2.2917e-01, -1.1476e+00],\n",
       "          [ 1.0871e+00, -5.1447e-01,  8.5756e-01,  ...,  9.7539e-03,\n",
       "            5.8921e-01, -1.0240e+00]],\n",
       "\n",
       "         [[ 1.4170e-01, -7.8824e-02,  1.3760e-01,  ...,  6.6726e-02,\n",
       "            2.7263e-02, -1.2786e-01],\n",
       "          [-1.5349e+00, -8.7958e-01, -2.1105e+00,  ...,  1.1987e+00,\n",
       "           -1.8142e+00,  1.6708e+00],\n",
       "          [-1.1810e+00,  1.7591e-01, -1.2447e+00,  ...,  1.7825e+00,\n",
       "           -1.6663e+00, -3.8519e-03],\n",
       "          ...,\n",
       "          [-6.8215e-01, -9.6625e-01,  4.6002e-02,  ...,  2.3771e-01,\n",
       "           -2.0625e+00,  1.5875e+00],\n",
       "          [-1.2214e+00, -3.4498e-01,  1.6366e+00,  ..., -4.0131e-01,\n",
       "           -2.0913e+00,  1.3273e+00],\n",
       "          [-4.5757e-01, -1.4241e+00, -9.2533e-01,  ..., -1.5896e+00,\n",
       "           -2.7188e+00, -1.1169e+00]],\n",
       "\n",
       "         [[ 2.1236e-01, -5.3957e-02, -5.8075e-02,  ...,  3.4223e-02,\n",
       "            4.0703e-02,  2.4087e-02],\n",
       "          [-4.3145e-01, -3.6304e-02,  3.2334e-01,  ...,  2.7693e-01,\n",
       "           -3.0853e-01, -1.4929e+00],\n",
       "          [-4.6276e-01, -3.7124e-01,  6.1947e-01,  ..., -2.4564e-01,\n",
       "           -3.8707e-02,  1.4433e-01],\n",
       "          ...,\n",
       "          [ 1.5067e+00,  6.2152e-02,  2.6064e+00,  ...,  4.0703e-01,\n",
       "           -2.4134e+00, -1.3274e+00],\n",
       "          [ 1.5874e+00,  4.5328e-01,  3.6642e-01,  ...,  3.1148e-01,\n",
       "            4.0489e-02,  5.4880e-02],\n",
       "          [ 9.3565e-01,  3.4607e-01,  1.1957e+00,  ...,  6.2538e-01,\n",
       "            1.2303e+00, -9.2403e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.0340e-02, -1.3298e-02, -2.6878e-02,  ...,  1.2709e-01,\n",
       "           -8.1964e-02, -4.6323e-02],\n",
       "          [-8.7537e-01,  5.8726e-01,  5.0024e-01,  ...,  5.4489e-01,\n",
       "            3.7365e-01,  7.3212e-01],\n",
       "          [-4.3954e-01, -5.2312e-01,  9.7326e-02,  ...,  8.1977e-01,\n",
       "            2.3033e-01,  1.7002e-01],\n",
       "          ...,\n",
       "          [-4.8094e-01,  5.1977e-01,  2.4170e-01,  ..., -1.2581e+00,\n",
       "            1.7060e-01,  6.7202e-01],\n",
       "          [-1.9993e+00,  3.7750e-01,  1.0577e+00,  ..., -1.3692e+00,\n",
       "           -5.7994e-01,  1.2607e+00],\n",
       "          [-2.4285e-01,  8.9216e-01, -1.1216e+00,  ..., -4.6753e-01,\n",
       "           -1.6479e-01, -3.1164e-01]],\n",
       "\n",
       "         [[ 9.8191e-03,  3.1127e-02,  5.5325e-02,  ...,  2.3694e-03,\n",
       "           -3.6194e-03,  1.1651e-02],\n",
       "          [ 2.3379e-01,  1.0323e+00,  5.1448e-01,  ...,  2.7895e-01,\n",
       "           -5.3069e-01,  1.1096e-01],\n",
       "          [-8.3193e-01,  7.8568e-01,  2.0929e-01,  ..., -1.2180e+00,\n",
       "           -6.8237e-01,  8.1226e-02],\n",
       "          ...,\n",
       "          [-1.3645e+00, -1.0636e-01,  1.1301e+00,  ..., -1.8059e+00,\n",
       "            2.3925e-01, -4.4729e-01],\n",
       "          [ 6.2954e-02,  5.7760e-01,  1.2296e+00,  ...,  6.5726e-02,\n",
       "            9.0508e-03, -7.6784e-02],\n",
       "          [-8.2236e-01, -5.4276e-01, -6.9580e-02,  ..., -7.1345e-01,\n",
       "           -1.3919e+00, -1.8101e-01]],\n",
       "\n",
       "         [[ 5.1820e-02, -3.8483e-02,  5.5745e-02,  ...,  5.8374e-02,\n",
       "           -6.5499e-02, -6.8603e-02],\n",
       "          [ 3.5901e-01,  6.9082e-01,  1.3846e+00,  ...,  7.2112e-01,\n",
       "           -6.4698e-01,  1.3922e+00],\n",
       "          [-2.6271e-01,  8.7886e-03,  4.5272e-01,  ...,  2.2419e-01,\n",
       "           -4.7247e-01,  5.5424e-01],\n",
       "          ...,\n",
       "          [-1.3244e-01, -1.5808e+00, -1.1396e+00,  ..., -8.2896e-01,\n",
       "            2.4640e-01, -1.0271e-01],\n",
       "          [ 4.4914e-01, -1.8511e+00, -7.0230e-01,  ..., -9.7060e-01,\n",
       "            2.1146e-01, -1.0917e-01],\n",
       "          [-9.1352e-01,  3.8970e-01, -9.3213e-01,  ..., -7.0359e-01,\n",
       "           -2.8697e-02,  5.3237e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.9928e-02, -4.0799e-02,  3.3523e-02,  ..., -8.5036e-02,\n",
       "            3.3863e-02,  7.3905e-03],\n",
       "          [ 7.7646e-01,  1.1034e+00,  1.1277e+00,  ...,  7.1504e-02,\n",
       "            6.5833e-01,  1.0784e+00],\n",
       "          [ 1.6653e+00,  6.6364e-01,  1.1367e+00,  ..., -6.9510e-03,\n",
       "           -2.7777e-01,  5.0956e-01],\n",
       "          ...,\n",
       "          [ 1.7087e+00, -3.3303e-01, -1.8498e-02,  ..., -1.5784e+00,\n",
       "            1.1566e+00, -8.5062e-01],\n",
       "          [ 1.4899e+00, -5.1055e-01,  6.9307e-01,  ..., -9.1986e-01,\n",
       "            4.8003e-01, -8.4838e-01],\n",
       "          [ 6.4917e-01, -4.6692e-01,  1.4127e+00,  ..., -3.7264e-01,\n",
       "            5.7790e-01, -8.6400e-01]],\n",
       "\n",
       "         [[ 1.4170e-01, -7.8824e-02,  1.3760e-01,  ...,  6.6726e-02,\n",
       "            2.7263e-02, -1.2786e-01],\n",
       "          [-1.5349e+00, -8.7958e-01, -2.1105e+00,  ...,  1.1987e+00,\n",
       "           -1.8142e+00,  1.6708e+00],\n",
       "          [-1.1810e+00,  1.7591e-01, -1.2447e+00,  ...,  1.7825e+00,\n",
       "           -1.6663e+00, -3.8519e-03],\n",
       "          ...,\n",
       "          [-1.8467e+00, -1.0881e+00, -2.7505e-01,  ...,  3.4211e-01,\n",
       "           -1.4994e+00,  1.6360e+00],\n",
       "          [-1.1285e+00, -1.7818e+00, -1.5691e+00,  ..., -1.2261e+00,\n",
       "           -3.3293e+00, -1.1818e+00],\n",
       "          [-4.9157e-01, -4.6910e-02, -4.0377e-01,  ..., -1.1239e+00,\n",
       "           -7.0364e-01,  5.9970e-01]],\n",
       "\n",
       "         [[ 2.1236e-01, -5.3957e-02, -5.8075e-02,  ...,  3.4223e-02,\n",
       "            4.0703e-02,  2.4087e-02],\n",
       "          [-4.3145e-01, -3.6304e-02,  3.2334e-01,  ...,  2.7693e-01,\n",
       "           -3.0853e-01, -1.4929e+00],\n",
       "          [-4.6276e-01, -3.7124e-01,  6.1947e-01,  ..., -2.4564e-01,\n",
       "           -3.8707e-02,  1.4433e-01],\n",
       "          ...,\n",
       "          [ 1.3451e-01, -6.2542e-01,  3.2428e-01,  ...,  1.3089e+00,\n",
       "            3.9599e-01, -9.6374e-01],\n",
       "          [ 8.6964e-01,  5.9863e-01,  1.4425e+00,  ...,  1.0228e+00,\n",
       "            1.2491e+00, -9.4657e-01],\n",
       "          [-5.3766e-01,  1.9961e+00,  1.1142e+00,  ...,  9.6799e-01,\n",
       "           -4.2165e-02, -3.1126e-02]]]])), (tensor([[[[ 3.5835e-02, -2.5267e-01, -4.5317e-01,  ...,  3.1819e-01,\n",
       "            3.2817e-01,  3.6666e-01],\n",
       "          [ 6.0318e-01,  2.5473e-01, -4.2042e-02,  ..., -3.3896e-01,\n",
       "            3.1347e-02,  1.6602e+00],\n",
       "          [-7.2873e-01, -5.0369e-01, -3.7533e-01,  ...,  8.4255e-01,\n",
       "           -2.1558e-01,  3.7320e-01],\n",
       "          ...,\n",
       "          [ 3.8508e-01, -8.0423e-01, -9.2214e-01,  ..., -1.8565e+00,\n",
       "            1.0518e+00,  5.6196e-01],\n",
       "          [ 1.9618e-01, -3.8188e-01, -4.1528e-02,  ..., -5.3380e-01,\n",
       "            8.6205e-01,  6.8851e-01],\n",
       "          [-5.0276e-01,  3.8106e-01, -4.8999e-01,  ..., -9.4264e-01,\n",
       "            1.4625e+00,  9.6584e-01]],\n",
       "\n",
       "         [[-2.7522e-01,  1.5262e-01,  1.2255e-01,  ...,  4.0994e-02,\n",
       "           -1.1579e+00, -1.4357e-01],\n",
       "          [ 3.2639e-01,  5.4595e-01,  2.7377e-01,  ...,  7.0111e-02,\n",
       "           -5.8070e-01, -6.3028e-01],\n",
       "          [ 1.0019e-01,  7.1997e-01, -9.4876e-02,  ...,  5.3608e-01,\n",
       "            5.2781e-01, -1.1258e-01],\n",
       "          ...,\n",
       "          [ 3.1096e-01, -1.5522e+00, -6.2735e-01,  ...,  1.4369e+00,\n",
       "           -1.9758e-01,  9.2395e-01],\n",
       "          [ 8.7418e-01,  4.7754e-01,  1.3559e+00,  ...,  1.3180e+00,\n",
       "           -1.1972e+00,  6.0584e-01],\n",
       "          [-1.5872e-02, -3.8104e-01, -4.2658e-01,  ...,  3.2360e-01,\n",
       "           -5.5391e-01,  4.3961e-01]],\n",
       "\n",
       "         [[-1.2310e+00, -9.9518e-02,  5.5481e-01,  ..., -6.6696e-01,\n",
       "            4.5639e-01, -2.9156e-01],\n",
       "          [ 2.2303e+00,  1.0436e+00,  5.4446e-02,  ..., -9.4876e-02,\n",
       "            2.2391e-01, -2.2189e-01],\n",
       "          [ 2.2829e+00,  7.5580e-01, -7.8399e-01,  ...,  1.4500e-01,\n",
       "           -9.8183e-02, -1.3611e+00],\n",
       "          ...,\n",
       "          [ 2.8723e+00,  1.7615e+00,  1.3321e+00,  ..., -6.8629e-01,\n",
       "            3.9445e-01,  2.0601e-01],\n",
       "          [ 2.5291e+00,  8.2703e-01,  5.1269e-01,  ..., -3.9448e-01,\n",
       "            2.9573e-01,  1.8150e+00],\n",
       "          [ 1.8768e+00,  1.2367e+00,  1.3933e-01,  ..., -1.5791e-01,\n",
       "            5.0701e-01,  2.2363e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.9312e-01, -8.9896e-01, -3.9603e-01,  ..., -1.0522e+00,\n",
       "           -4.2584e-01,  4.9221e-01],\n",
       "          [-1.0445e-01, -1.1973e+00, -9.7141e-01,  ...,  1.7565e-01,\n",
       "           -3.2206e-01, -1.5880e+00],\n",
       "          [ 9.5657e-01, -4.9343e-01, -6.6816e-01,  ..., -5.7704e-01,\n",
       "            1.7239e+00, -1.7396e+00],\n",
       "          ...,\n",
       "          [-1.8883e-01, -1.6223e+00, -1.9367e+00,  ..., -3.3559e-01,\n",
       "           -5.1908e-01, -2.5042e+00],\n",
       "          [-1.0414e-01, -1.8137e+00, -3.8355e+00,  ..., -7.1242e-01,\n",
       "           -4.4441e-01, -2.4708e+00],\n",
       "          [ 8.3401e-02, -6.4668e-01, -8.8830e-01,  ..., -1.1246e+00,\n",
       "           -7.9815e-01, -1.8989e+00]],\n",
       "\n",
       "         [[-9.0050e-01,  2.5622e+00,  3.0536e-01,  ...,  3.5956e-01,\n",
       "            1.9543e+00, -5.3737e-01],\n",
       "          [-3.6747e-01, -2.2365e+00,  5.6648e-01,  ...,  1.6513e+00,\n",
       "           -2.8733e+00,  1.2070e+00],\n",
       "          [-8.0931e-01, -4.7269e+00, -7.4180e-02,  ...,  1.0067e+00,\n",
       "           -3.4493e+00,  1.3495e+00],\n",
       "          ...,\n",
       "          [-6.0875e-01, -4.0939e+00,  1.7788e+00,  ..., -2.7883e-01,\n",
       "           -3.9367e+00, -6.4289e-01],\n",
       "          [ 1.4280e+00, -2.1293e+00,  8.8660e-01,  ..., -1.7067e-03,\n",
       "           -2.6843e+00,  6.9804e-02],\n",
       "          [ 1.4347e-02, -4.3330e+00,  5.6642e-02,  ...,  9.2019e-01,\n",
       "           -2.5232e+00, -5.7698e-01]],\n",
       "\n",
       "         [[-2.0278e+00, -3.6497e-01, -1.1259e+00,  ..., -3.9892e-01,\n",
       "            4.4918e-02,  2.5028e-01],\n",
       "          [ 1.8488e+00,  1.1874e+00,  1.2333e+00,  ...,  3.0355e-02,\n",
       "            2.1839e-01,  1.9899e-01],\n",
       "          [ 2.8843e+00,  5.8415e-01,  1.8840e+00,  ..., -3.2556e-01,\n",
       "            2.4125e-01,  7.9505e-01],\n",
       "          ...,\n",
       "          [ 9.7716e-01,  2.2331e+00,  1.6164e+00,  ...,  1.1055e+00,\n",
       "           -9.7223e-01,  5.2543e-01],\n",
       "          [ 1.4555e+00,  3.2433e-01,  7.2715e-01,  ..., -6.2165e-01,\n",
       "            2.7764e-01,  5.0700e-01],\n",
       "          [ 1.5344e+00,  3.7444e-01,  1.4026e+00,  ..., -2.0962e-02,\n",
       "            9.0852e-02, -5.2391e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.5835e-02, -2.5267e-01, -4.5317e-01,  ...,  3.1819e-01,\n",
       "            3.2817e-01,  3.6666e-01],\n",
       "          [ 6.0318e-01,  2.5473e-01, -4.2042e-02,  ..., -3.3896e-01,\n",
       "            3.1347e-02,  1.6602e+00],\n",
       "          [-7.2873e-01, -5.0369e-01, -3.7533e-01,  ...,  8.4255e-01,\n",
       "           -2.1558e-01,  3.7320e-01],\n",
       "          ...,\n",
       "          [ 8.8105e-01, -1.4007e+00, -1.0966e-01,  ..., -2.7432e-01,\n",
       "            3.9774e-01, -4.4805e-01],\n",
       "          [-6.5987e-01,  2.0501e-01, -2.8165e-01,  ...,  4.3837e-02,\n",
       "            9.5524e-01,  7.9114e-01],\n",
       "          [-2.3093e-01, -1.6844e-01, -8.3947e-01,  ..., -1.5279e-03,\n",
       "            7.8731e-02,  1.5189e+00]],\n",
       "\n",
       "         [[-2.7522e-01,  1.5262e-01,  1.2255e-01,  ...,  4.0994e-02,\n",
       "           -1.1579e+00, -1.4357e-01],\n",
       "          [ 3.2639e-01,  5.4595e-01,  2.7377e-01,  ...,  7.0111e-02,\n",
       "           -5.8070e-01, -6.3028e-01],\n",
       "          [ 1.0019e-01,  7.1997e-01, -9.4876e-02,  ...,  5.3608e-01,\n",
       "            5.2781e-01, -1.1258e-01],\n",
       "          ...,\n",
       "          [ 1.3937e+00, -8.8737e-01, -4.5440e-02,  ...,  8.0919e-01,\n",
       "           -1.3384e+00, -2.6461e-01],\n",
       "          [-2.8234e-02, -8.1780e-01,  1.3216e-01,  ...,  5.9648e-01,\n",
       "           -6.5399e-01, -4.1747e-02],\n",
       "          [-4.6505e-01,  3.3272e-01, -5.3749e-01,  ..., -2.5936e-01,\n",
       "            1.3935e+00,  6.7197e-01]],\n",
       "\n",
       "         [[-1.2310e+00, -9.9518e-02,  5.5481e-01,  ..., -6.6696e-01,\n",
       "            4.5639e-01, -2.9156e-01],\n",
       "          [ 2.2303e+00,  1.0436e+00,  5.4446e-02,  ..., -9.4876e-02,\n",
       "            2.2391e-01, -2.2189e-01],\n",
       "          [ 2.2829e+00,  7.5580e-01, -7.8399e-01,  ...,  1.4500e-01,\n",
       "           -9.8183e-02, -1.3611e+00],\n",
       "          ...,\n",
       "          [ 2.4334e+00,  8.8891e-01,  4.8830e-01,  ..., -1.6096e-01,\n",
       "            1.3220e-01,  4.8059e-01],\n",
       "          [ 1.6811e+00,  9.7707e-01,  1.3899e-02,  ..., -2.2141e-01,\n",
       "            5.3650e-02,  1.8998e+00],\n",
       "          [ 1.1053e+00,  2.0565e+00,  7.4851e-02,  ..., -4.6097e-01,\n",
       "           -1.3353e+00,  1.7804e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 7.9312e-01, -8.9896e-01, -3.9603e-01,  ..., -1.0522e+00,\n",
       "           -4.2584e-01,  4.9221e-01],\n",
       "          [-1.0445e-01, -1.1973e+00, -9.7141e-01,  ...,  1.7565e-01,\n",
       "           -3.2206e-01, -1.5880e+00],\n",
       "          [ 9.5657e-01, -4.9343e-01, -6.6816e-01,  ..., -5.7704e-01,\n",
       "            1.7239e+00, -1.7396e+00],\n",
       "          ...,\n",
       "          [ 2.9906e-01, -5.8949e-01, -3.5297e+00,  ..., -1.2381e-01,\n",
       "           -1.3005e+00, -2.6039e+00],\n",
       "          [-2.6475e-02, -4.4417e-01, -1.3387e+00,  ..., -7.2643e-01,\n",
       "           -1.1751e+00, -2.0103e+00],\n",
       "          [ 1.0783e+00, -6.2369e-01, -8.5287e-01,  ...,  5.3258e-01,\n",
       "            3.5638e-01, -8.9993e-01]],\n",
       "\n",
       "         [[-9.0050e-01,  2.5622e+00,  3.0536e-01,  ...,  3.5956e-01,\n",
       "            1.9543e+00, -5.3737e-01],\n",
       "          [-3.6747e-01, -2.2365e+00,  5.6648e-01,  ...,  1.6513e+00,\n",
       "           -2.8733e+00,  1.2070e+00],\n",
       "          [-8.0931e-01, -4.7269e+00, -7.4180e-02,  ...,  1.0067e+00,\n",
       "           -3.4493e+00,  1.3495e+00],\n",
       "          ...,\n",
       "          [ 1.5161e+00, -2.3472e+00,  8.0442e-01,  ...,  8.5330e-01,\n",
       "           -3.5781e+00,  6.7402e-01],\n",
       "          [ 3.7777e-01, -4.1467e+00,  2.9355e-01,  ...,  2.7224e-01,\n",
       "           -2.5001e+00,  1.1049e-01],\n",
       "          [ 9.3951e-01, -3.5702e+00, -1.2870e-02,  ...,  3.7115e-01,\n",
       "           -3.0850e+00,  1.0437e+00]],\n",
       "\n",
       "         [[-2.0278e+00, -3.6497e-01, -1.1259e+00,  ..., -3.9892e-01,\n",
       "            4.4918e-02,  2.5028e-01],\n",
       "          [ 1.8488e+00,  1.1874e+00,  1.2333e+00,  ...,  3.0355e-02,\n",
       "            2.1839e-01,  1.9899e-01],\n",
       "          [ 2.8843e+00,  5.8415e-01,  1.8840e+00,  ..., -3.2556e-01,\n",
       "            2.4125e-01,  7.9505e-01],\n",
       "          ...,\n",
       "          [ 1.6540e+00,  3.9870e-01,  9.3295e-01,  ...,  8.1971e-01,\n",
       "            4.6612e-01,  8.0458e-01],\n",
       "          [ 2.0776e+00,  1.3177e-01,  1.2619e+00,  ...,  3.9230e-01,\n",
       "            4.7121e-01, -8.4133e-02],\n",
       "          [ 8.6631e-01,  3.7827e-01,  2.0952e+00,  ...,  8.0257e-02,\n",
       "           -2.1718e-01,  4.4517e-01]]]]), tensor([[[[-5.5911e-02, -8.8373e-02,  1.4088e-02,  ...,  1.1586e-01,\n",
       "           -1.4406e-02,  2.3666e-02],\n",
       "          [ 6.6181e-01, -1.7355e+00, -8.3501e-01,  ...,  1.7955e-01,\n",
       "           -1.7632e-01, -1.9783e+00],\n",
       "          [ 4.1105e-01, -6.9462e-01, -4.3536e-01,  ...,  2.7407e-01,\n",
       "           -1.6279e-01, -1.2511e+00],\n",
       "          ...,\n",
       "          [-1.4021e-01, -6.5280e-01, -7.2387e-01,  ..., -3.5023e-01,\n",
       "           -6.2614e-01, -3.1200e-01],\n",
       "          [ 1.3153e+00, -3.3189e-01,  2.5464e-01,  ..., -1.0983e+00,\n",
       "            1.9529e-01,  8.7180e-02],\n",
       "          [-2.7073e-01,  1.4893e-01,  7.1293e-01,  ..., -1.8733e+00,\n",
       "           -2.4653e-01,  1.1548e-01]],\n",
       "\n",
       "         [[ 2.3027e-02,  2.0390e-02, -5.1958e-02,  ...,  3.2800e-02,\n",
       "            8.2079e-03,  3.5532e-02],\n",
       "          [-7.2800e-01, -1.1801e-01, -2.8563e-01,  ...,  1.6440e+00,\n",
       "           -8.1233e-01, -9.8537e-01],\n",
       "          [-6.5063e-01, -1.0919e+00, -3.8086e-01,  ...,  5.9019e-01,\n",
       "           -5.2108e-01, -4.8363e-01],\n",
       "          ...,\n",
       "          [-6.3799e-01,  1.8645e+00, -1.5518e+00,  ..., -1.2015e+00,\n",
       "            6.4306e-01,  7.5710e-01],\n",
       "          [-3.8424e-01, -4.4173e-01, -8.5943e-01,  ..., -7.6761e-01,\n",
       "           -4.2090e-01,  1.3187e-01],\n",
       "          [-1.5610e+00,  2.3026e-01, -1.3497e+00,  ..., -1.1673e+00,\n",
       "            2.9567e-01, -1.5317e-01]],\n",
       "\n",
       "         [[ 3.4585e-02,  3.2592e-02, -6.6911e-02,  ...,  1.2884e-03,\n",
       "           -1.3041e-02, -1.4410e-03],\n",
       "          [ 1.4425e+00, -1.5593e+00, -8.2674e-01,  ..., -7.3062e-01,\n",
       "           -8.0953e-01, -1.4356e-01],\n",
       "          [ 9.5753e-01, -2.3459e+00, -3.7743e-01,  ..., -3.1975e-01,\n",
       "           -3.3877e-01, -4.6965e-01],\n",
       "          ...,\n",
       "          [ 4.6221e-01,  2.1706e-01, -1.3660e+00,  ..., -6.4379e-01,\n",
       "            7.4039e-01, -3.9987e-01],\n",
       "          [ 6.5348e-01, -1.8350e+00, -7.1464e-01,  ...,  5.2557e-01,\n",
       "           -4.8429e-01, -1.3179e+00],\n",
       "          [ 1.0237e-01, -1.0672e+00, -1.8737e+00,  ..., -3.5996e-01,\n",
       "            5.8115e-02, -1.1766e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.8270e-02,  2.5618e-02, -8.0607e-03,  ..., -1.4416e-02,\n",
       "           -2.2208e-02,  2.8554e-02],\n",
       "          [ 4.2415e-01, -2.9136e-01, -3.7557e-01,  ..., -2.1098e+00,\n",
       "            2.6877e-03,  5.4467e-01],\n",
       "          [ 8.8803e-01,  9.8491e-02,  5.2396e-01,  ..., -2.7730e-01,\n",
       "           -3.1162e-01,  1.0799e-01],\n",
       "          ...,\n",
       "          [-2.0983e+00,  4.2804e+00, -2.9043e+00,  ...,  6.1863e-02,\n",
       "            1.2987e+00,  1.3131e+00],\n",
       "          [-9.4076e-01, -5.7595e-01, -1.1853e+00,  ..., -3.9848e-01,\n",
       "            1.3697e+00,  7.7249e-01],\n",
       "          [-7.6905e-01,  1.6143e-02, -4.1339e-01,  ...,  1.1616e+00,\n",
       "           -7.8535e-01,  2.7317e-01]],\n",
       "\n",
       "         [[-6.8744e-02, -3.1846e-02,  2.3077e-02,  ...,  8.3093e-03,\n",
       "           -5.0493e-02, -9.4521e-02],\n",
       "          [ 1.2431e+00,  5.8843e-01, -8.3619e-01,  ..., -1.3177e+00,\n",
       "           -2.1627e-02, -5.3051e-01],\n",
       "          [ 8.5525e-01, -6.1447e-03, -9.3899e-01,  ...,  4.9952e-01,\n",
       "           -1.6859e-01, -4.4937e-01],\n",
       "          ...,\n",
       "          [ 1.0636e+00,  5.2999e-01,  8.7705e-01,  ...,  4.2109e-01,\n",
       "            2.9409e-01, -1.0716e+00],\n",
       "          [-9.5766e-02,  6.4643e-01, -8.9609e-01,  ..., -7.3144e-01,\n",
       "           -2.4361e-02, -9.1285e-01],\n",
       "          [-4.1151e-04,  7.3029e-01, -7.3788e-01,  ..., -4.8585e-01,\n",
       "            4.6065e-01, -4.0208e-01]],\n",
       "\n",
       "         [[-1.8515e-04,  2.9778e-02, -5.3097e-02,  ..., -5.7227e-03,\n",
       "            1.3003e-02, -7.6026e-03],\n",
       "          [-7.4681e-01, -1.3601e+00, -4.6202e-01,  ...,  1.5997e+00,\n",
       "            1.6080e+00, -1.1606e+00],\n",
       "          [ 2.1542e-01, -6.9375e-01, -2.6529e-01,  ...,  5.6348e-01,\n",
       "            3.5251e-01, -1.3576e-02],\n",
       "          ...,\n",
       "          [ 9.5731e-01, -3.0689e+00, -5.0126e-01,  ...,  2.0877e+00,\n",
       "           -1.9536e+00,  5.8550e-01],\n",
       "          [ 1.2587e+00, -7.1026e-01, -1.8765e-01,  ...,  1.1229e+00,\n",
       "           -6.3109e-01,  4.5189e-01],\n",
       "          [-7.6352e-01, -9.8747e-01, -5.1012e-02,  ...,  5.8097e-01,\n",
       "           -5.9228e-01, -5.4572e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5911e-02, -8.8373e-02,  1.4088e-02,  ...,  1.1586e-01,\n",
       "           -1.4406e-02,  2.3666e-02],\n",
       "          [ 6.6181e-01, -1.7355e+00, -8.3501e-01,  ...,  1.7955e-01,\n",
       "           -1.7632e-01, -1.9783e+00],\n",
       "          [ 4.1105e-01, -6.9462e-01, -4.3536e-01,  ...,  2.7407e-01,\n",
       "           -1.6279e-01, -1.2511e+00],\n",
       "          ...,\n",
       "          [ 7.2846e-01, -2.8382e-01,  9.9303e-01,  ..., -7.9231e-01,\n",
       "           -2.2945e-01,  1.2647e+00],\n",
       "          [-4.6021e-01,  1.4091e-01,  1.4132e-01,  ..., -2.2184e+00,\n",
       "           -1.8174e-01,  8.4047e-01],\n",
       "          [ 4.2042e-01, -5.0899e-01,  6.8531e-01,  ...,  7.4851e-02,\n",
       "           -8.5801e-01,  3.6742e-01]],\n",
       "\n",
       "         [[ 2.3027e-02,  2.0390e-02, -5.1958e-02,  ...,  3.2800e-02,\n",
       "            8.2079e-03,  3.5532e-02],\n",
       "          [-7.2800e-01, -1.1801e-01, -2.8563e-01,  ...,  1.6440e+00,\n",
       "           -8.1233e-01, -9.8537e-01],\n",
       "          [-6.5063e-01, -1.0919e+00, -3.8086e-01,  ...,  5.9019e-01,\n",
       "           -5.2108e-01, -4.8363e-01],\n",
       "          ...,\n",
       "          [-7.8423e-01, -2.6634e-01, -8.3849e-01,  ...,  6.0416e-01,\n",
       "            5.1375e-01,  1.8425e+00],\n",
       "          [-1.3760e+00, -4.2391e-02, -1.1892e+00,  ..., -1.3614e+00,\n",
       "            2.9137e-01, -5.1448e-01],\n",
       "          [ 4.7798e-01,  1.0376e+00,  4.8832e-01,  ..., -1.9422e-01,\n",
       "            8.6171e-01, -7.3391e-01]],\n",
       "\n",
       "         [[ 3.4585e-02,  3.2592e-02, -6.6911e-02,  ...,  1.2884e-03,\n",
       "           -1.3041e-02, -1.4410e-03],\n",
       "          [ 1.4425e+00, -1.5593e+00, -8.2674e-01,  ..., -7.3062e-01,\n",
       "           -8.0953e-01, -1.4356e-01],\n",
       "          [ 9.5753e-01, -2.3459e+00, -3.7743e-01,  ..., -3.1975e-01,\n",
       "           -3.3877e-01, -4.6965e-01],\n",
       "          ...,\n",
       "          [-7.5626e-01, -2.2933e-01, -1.8355e+00,  ...,  2.3485e-01,\n",
       "           -1.0848e+00,  7.1977e-01],\n",
       "          [-6.3281e-01, -5.6851e-01, -2.2509e+00,  ..., -3.2101e-01,\n",
       "           -4.9655e-01, -8.2868e-01],\n",
       "          [ 6.3020e-01, -7.4772e-01, -1.0476e+00,  ..., -4.1421e-01,\n",
       "           -3.0023e-01, -6.3009e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.8270e-02,  2.5618e-02, -8.0607e-03,  ..., -1.4416e-02,\n",
       "           -2.2208e-02,  2.8554e-02],\n",
       "          [ 4.2415e-01, -2.9136e-01, -3.7557e-01,  ..., -2.1098e+00,\n",
       "            2.6877e-03,  5.4467e-01],\n",
       "          [ 8.8803e-01,  9.8491e-02,  5.2396e-01,  ..., -2.7730e-01,\n",
       "           -3.1162e-01,  1.0799e-01],\n",
       "          ...,\n",
       "          [-3.0978e-01, -9.3137e-01,  2.9167e-01,  ...,  4.6259e-01,\n",
       "           -5.5917e-01,  6.0600e-01],\n",
       "          [-5.3936e-01,  3.1907e-01, -4.6563e-01,  ...,  1.5104e+00,\n",
       "           -7.8344e-01,  2.9469e-01],\n",
       "          [ 2.2126e-01, -9.1859e-01,  1.2330e+00,  ..., -7.4395e-01,\n",
       "            1.1641e+00,  1.1473e+00]],\n",
       "\n",
       "         [[-6.8744e-02, -3.1846e-02,  2.3077e-02,  ...,  8.3093e-03,\n",
       "           -5.0493e-02, -9.4521e-02],\n",
       "          [ 1.2431e+00,  5.8843e-01, -8.3619e-01,  ..., -1.3177e+00,\n",
       "           -2.1627e-02, -5.3051e-01],\n",
       "          [ 8.5525e-01, -6.1447e-03, -9.3899e-01,  ...,  4.9952e-01,\n",
       "           -1.6859e-01, -4.4937e-01],\n",
       "          ...,\n",
       "          [ 4.3675e-01,  7.7376e-01, -3.7995e-01,  ..., -9.7950e-01,\n",
       "           -1.1065e+00, -7.3206e-01],\n",
       "          [ 4.7657e-01,  9.3020e-01, -9.3119e-01,  ..., -4.3328e-01,\n",
       "            6.5218e-01, -2.8145e-01],\n",
       "          [-1.1863e-01,  2.4402e-02,  4.8142e-01,  ...,  3.8463e-01,\n",
       "            7.2478e-01,  4.4469e-01]],\n",
       "\n",
       "         [[-1.8515e-04,  2.9778e-02, -5.3097e-02,  ..., -5.7227e-03,\n",
       "            1.3003e-02, -7.6026e-03],\n",
       "          [-7.4681e-01, -1.3601e+00, -4.6202e-01,  ...,  1.5997e+00,\n",
       "            1.6080e+00, -1.1606e+00],\n",
       "          [ 2.1542e-01, -6.9375e-01, -2.6529e-01,  ...,  5.6348e-01,\n",
       "            3.5251e-01, -1.3576e-02],\n",
       "          ...,\n",
       "          [ 1.1639e+00, -1.3295e-01,  7.1293e-02,  ...,  1.3159e+00,\n",
       "            1.9362e-01,  7.2119e-01],\n",
       "          [-4.8884e-01, -1.1277e+00, -3.9276e-01,  ...,  7.3449e-01,\n",
       "           -3.4987e-01, -7.6455e-01],\n",
       "          [-8.4492e-01, -1.5492e-01,  8.0871e-01,  ...,  1.3324e-01,\n",
       "            4.2921e-01, -1.3192e+00]]]])), (tensor([[[[-5.1306e-01,  4.8400e-01, -8.6740e-01,  ..., -1.0240e+00,\n",
       "           -1.3286e+00,  2.1402e-01],\n",
       "          [ 8.2142e-01,  2.8796e-01, -2.4519e+00,  ...,  6.2998e-01,\n",
       "           -6.8284e-01, -1.4383e+00],\n",
       "          [ 9.0844e-01,  4.6793e-01, -1.0011e+00,  ...,  1.2880e+00,\n",
       "            8.2395e-01, -1.8216e+00],\n",
       "          ...,\n",
       "          [ 1.8717e-01, -2.9736e-02,  9.7039e-01,  ...,  2.2774e+00,\n",
       "           -5.8612e-01, -3.2911e+00],\n",
       "          [-1.2409e+00, -1.8755e-01,  6.7532e-01,  ...,  1.9408e+00,\n",
       "            4.0133e-01, -1.3687e+00],\n",
       "          [-1.3494e+00,  4.2698e-01, -9.5411e-01,  ...,  6.7837e-01,\n",
       "           -9.0289e-01, -4.0667e-03]],\n",
       "\n",
       "         [[ 8.6183e-01, -2.0685e+00,  1.5721e-01,  ...,  2.2979e-01,\n",
       "           -2.4632e+00, -4.5158e-01],\n",
       "          [ 1.7022e+00,  4.1256e-01,  2.0663e-02,  ...,  9.2759e-01,\n",
       "           -1.0168e+00, -1.5514e+00],\n",
       "          [ 2.1045e+00,  2.1709e+00,  4.1771e-01,  ..., -2.1357e-01,\n",
       "            2.5485e+00, -7.7641e-01],\n",
       "          ...,\n",
       "          [ 2.5133e+00,  1.8565e+00,  9.0796e-02,  ...,  1.2578e+00,\n",
       "            3.5726e-02,  2.0115e-01],\n",
       "          [ 2.3125e-01,  9.8942e-01,  1.0953e+00,  ...,  1.4346e+00,\n",
       "            5.3180e-02,  8.2499e-01],\n",
       "          [ 1.9616e+00,  2.1448e+00,  9.6400e-01,  ...,  1.9017e+00,\n",
       "            1.1694e+00, -2.1623e-01]],\n",
       "\n",
       "         [[ 1.0117e+00,  3.8225e-01, -1.6986e-01,  ..., -8.0393e-01,\n",
       "           -1.4053e+00, -3.7208e-01],\n",
       "          [-7.5385e-01, -1.0520e+00,  2.0751e-01,  ...,  1.3993e+00,\n",
       "           -1.1189e+00,  7.2032e-01],\n",
       "          [-5.5596e-01, -1.2610e+00,  9.3428e-01,  ...,  8.1683e-01,\n",
       "           -3.1019e-01, -8.0848e-01],\n",
       "          ...,\n",
       "          [-1.3901e+00,  1.6029e-01, -6.1764e-03,  ...,  1.8869e+00,\n",
       "           -6.2248e-01, -3.0463e-01],\n",
       "          [ 1.2651e-01,  8.2582e-01, -5.1682e-01,  ...,  1.6311e+00,\n",
       "            4.3895e-01,  3.7139e-01],\n",
       "          [-4.8538e-01, -1.2060e+00,  6.6328e-01,  ...,  1.8609e+00,\n",
       "            9.2885e-02, -7.1857e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2269e-01, -5.8191e-01,  4.7897e-01,  ..., -6.6687e-01,\n",
       "            1.0642e+00,  2.8843e-01],\n",
       "          [ 3.1309e-01,  1.9245e+00, -2.4937e+00,  ..., -2.8480e+00,\n",
       "           -3.1607e+00,  8.4664e-01],\n",
       "          [-6.7335e-01,  1.8413e+00, -2.3286e+00,  ..., -2.4200e+00,\n",
       "           -2.1986e+00,  6.0143e-01],\n",
       "          ...,\n",
       "          [-6.9396e-01,  1.2926e+00, -4.4845e+00,  ..., -9.1379e-01,\n",
       "           -7.4157e-02,  6.8290e-01],\n",
       "          [-1.2357e+00,  2.0778e+00, -2.2573e+00,  ..., -1.6509e+00,\n",
       "           -1.8174e+00,  1.3846e+00],\n",
       "          [-2.0949e+00,  1.9279e+00, -2.9618e+00,  ..., -1.7951e+00,\n",
       "           -3.3772e+00,  1.5252e+00]],\n",
       "\n",
       "         [[ 2.5260e-01,  5.4636e-01,  5.5585e-01,  ...,  7.2793e-01,\n",
       "            8.0137e-02,  8.4519e-01],\n",
       "          [-9.8370e-01,  2.1467e+00,  2.1108e-01,  ...,  6.3154e-01,\n",
       "           -2.6190e-01,  1.5724e+00],\n",
       "          [ 6.1865e-01,  2.8455e+00,  3.2318e-01,  ..., -1.6027e-01,\n",
       "           -4.1046e-01,  4.1153e-01],\n",
       "          ...,\n",
       "          [ 1.2980e-01,  9.0153e-01,  7.5966e-01,  ...,  8.9098e-02,\n",
       "            2.8834e-01, -9.4337e-02],\n",
       "          [-3.6942e-01,  7.0419e-01, -2.4832e-01,  ..., -9.8519e-01,\n",
       "            8.0984e-01,  7.3002e-02],\n",
       "          [-2.8569e-01,  5.0822e-01,  7.1840e-01,  ...,  5.8660e-01,\n",
       "           -7.6405e-01,  7.4954e-01]],\n",
       "\n",
       "         [[-7.4028e-01,  2.9979e-01, -1.5952e+00,  ..., -4.0198e-01,\n",
       "            2.2518e-01, -1.3001e+00],\n",
       "          [-8.8757e-01, -1.7891e+00,  1.6372e+00,  ..., -7.2935e-01,\n",
       "            2.4469e-02,  1.6078e+00],\n",
       "          [-1.3905e+00, -9.6968e-01,  5.0151e-01,  ..., -6.3453e-01,\n",
       "            1.7226e-01,  7.0181e-01],\n",
       "          ...,\n",
       "          [ 1.5879e+00, -1.6068e+00,  5.4232e-01,  ..., -6.0144e-01,\n",
       "            7.6185e-02, -3.2160e-02],\n",
       "          [ 1.0586e+00, -7.9074e-01,  4.0500e-02,  ...,  1.3087e-01,\n",
       "           -3.7907e-01,  9.5869e-02],\n",
       "          [-4.3970e-01, -3.3435e+00,  8.0661e-01,  ...,  8.4494e-01,\n",
       "           -1.0924e+00,  2.8869e+00]]],\n",
       "\n",
       "\n",
       "        [[[-5.1306e-01,  4.8400e-01, -8.6740e-01,  ..., -1.0240e+00,\n",
       "           -1.3286e+00,  2.1402e-01],\n",
       "          [ 8.2142e-01,  2.8796e-01, -2.4519e+00,  ...,  6.2998e-01,\n",
       "           -6.8284e-01, -1.4383e+00],\n",
       "          [ 9.0844e-01,  4.6793e-01, -1.0011e+00,  ...,  1.2880e+00,\n",
       "            8.2395e-01, -1.8216e+00],\n",
       "          ...,\n",
       "          [-1.3016e+00, -2.9625e-01,  3.9521e-01,  ...,  1.9749e+00,\n",
       "           -9.2286e-01, -1.0797e+00],\n",
       "          [-1.4958e+00,  2.2286e-01, -3.2486e-01,  ...,  1.0296e+00,\n",
       "           -7.6404e-01, -1.5267e-01],\n",
       "          [ 1.0266e-01, -1.3718e-01, -3.7770e-01,  ...,  2.1166e+00,\n",
       "           -1.8034e-01, -1.6256e-01]],\n",
       "\n",
       "         [[ 8.6183e-01, -2.0685e+00,  1.5721e-01,  ...,  2.2979e-01,\n",
       "           -2.4632e+00, -4.5158e-01],\n",
       "          [ 1.7022e+00,  4.1256e-01,  2.0663e-02,  ...,  9.2759e-01,\n",
       "           -1.0168e+00, -1.5514e+00],\n",
       "          [ 2.1045e+00,  2.1709e+00,  4.1771e-01,  ..., -2.1357e-01,\n",
       "            2.5485e+00, -7.7641e-01],\n",
       "          ...,\n",
       "          [ 6.9033e-01,  7.1163e-01,  6.1725e-01,  ...,  1.2692e+00,\n",
       "           -1.3903e+00,  7.4160e-01],\n",
       "          [ 1.8809e+00,  1.6543e+00,  1.2242e+00,  ...,  1.7796e+00,\n",
       "            1.1583e+00,  6.4397e-02],\n",
       "          [ 1.9209e+00,  1.6086e+00,  1.2582e-02,  ...,  6.8370e-01,\n",
       "            1.3645e+00, -1.3780e+00]],\n",
       "\n",
       "         [[ 1.0117e+00,  3.8225e-01, -1.6986e-01,  ..., -8.0393e-01,\n",
       "           -1.4053e+00, -3.7208e-01],\n",
       "          [-7.5385e-01, -1.0520e+00,  2.0751e-01,  ...,  1.3993e+00,\n",
       "           -1.1189e+00,  7.2032e-01],\n",
       "          [-5.5596e-01, -1.2610e+00,  9.3428e-01,  ...,  8.1683e-01,\n",
       "           -3.1019e-01, -8.0848e-01],\n",
       "          ...,\n",
       "          [ 4.3002e-01,  1.2854e+00, -8.2840e-01,  ...,  1.9866e+00,\n",
       "            2.5012e-01, -5.3248e-01],\n",
       "          [-2.2051e-01, -1.4683e+00,  4.4602e-01,  ...,  2.1771e+00,\n",
       "            3.5125e-01, -7.7286e-02],\n",
       "          [ 5.1482e-01, -4.8391e-01,  5.3366e-01,  ...,  5.6483e-01,\n",
       "           -1.8804e-01, -6.4198e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2269e-01, -5.8191e-01,  4.7897e-01,  ..., -6.6687e-01,\n",
       "            1.0642e+00,  2.8843e-01],\n",
       "          [ 3.1309e-01,  1.9245e+00, -2.4937e+00,  ..., -2.8480e+00,\n",
       "           -3.1607e+00,  8.4664e-01],\n",
       "          [-6.7335e-01,  1.8413e+00, -2.3286e+00,  ..., -2.4200e+00,\n",
       "           -2.1986e+00,  6.0143e-01],\n",
       "          ...,\n",
       "          [-1.2981e+00,  2.2627e+00, -2.6319e+00,  ..., -6.2910e-01,\n",
       "           -2.2732e+00,  1.6755e+00],\n",
       "          [-1.8660e+00,  2.3348e+00, -2.4877e+00,  ..., -1.4529e+00,\n",
       "           -3.6815e+00,  1.4560e+00],\n",
       "          [-6.2213e-01,  1.4014e+00, -2.1995e+00,  ..., -3.4333e+00,\n",
       "           -9.8961e-01,  1.4972e+00]],\n",
       "\n",
       "         [[ 2.5260e-01,  5.4636e-01,  5.5585e-01,  ...,  7.2793e-01,\n",
       "            8.0137e-02,  8.4519e-01],\n",
       "          [-9.8370e-01,  2.1467e+00,  2.1108e-01,  ...,  6.3154e-01,\n",
       "           -2.6190e-01,  1.5724e+00],\n",
       "          [ 6.1865e-01,  2.8455e+00,  3.2318e-01,  ..., -1.6027e-01,\n",
       "           -4.1046e-01,  4.1153e-01],\n",
       "          ...,\n",
       "          [-5.7464e-01,  7.9232e-01, -1.6341e-01,  ...,  7.3747e-01,\n",
       "            1.1952e+00, -1.1984e-01],\n",
       "          [ 1.4006e-01,  2.7435e-01,  2.3587e-01,  ...,  6.1069e-01,\n",
       "           -4.8213e-01,  9.2618e-01],\n",
       "          [ 1.3038e+00,  1.1048e+00,  1.5695e+00,  ..., -5.1326e-01,\n",
       "            5.0411e-01, -5.8354e-03]],\n",
       "\n",
       "         [[-7.4028e-01,  2.9979e-01, -1.5952e+00,  ..., -4.0198e-01,\n",
       "            2.2518e-01, -1.3001e+00],\n",
       "          [-8.8757e-01, -1.7891e+00,  1.6372e+00,  ..., -7.2935e-01,\n",
       "            2.4469e-02,  1.6078e+00],\n",
       "          [-1.3905e+00, -9.6968e-01,  5.0151e-01,  ..., -6.3453e-01,\n",
       "            1.7226e-01,  7.0181e-01],\n",
       "          ...,\n",
       "          [ 1.1451e+00, -7.0580e-01, -8.1480e-02,  ...,  1.0760e-01,\n",
       "           -7.6241e-01,  4.6481e-01],\n",
       "          [ 1.3179e-01, -2.6554e+00,  2.6951e-01,  ...,  1.0764e+00,\n",
       "           -6.5243e-01,  2.5726e+00],\n",
       "          [-2.8438e-01, -1.0475e+00,  1.3470e+00,  ...,  1.9810e+00,\n",
       "            1.2170e+00, -7.0130e-01]]]]), tensor([[[[ 3.4877e-03,  5.4099e-02, -7.6774e-02,  ...,  6.7908e-02,\n",
       "           -3.6519e-02, -8.0231e-02],\n",
       "          [-2.3341e-01, -1.5787e+00, -7.1027e-01,  ...,  1.1150e+00,\n",
       "            1.5362e+00, -2.3433e-01],\n",
       "          [-3.8497e-01, -1.2795e+00,  4.3961e-01,  ...,  2.6797e-01,\n",
       "            8.4862e-01, -1.8365e-01],\n",
       "          ...,\n",
       "          [-1.0570e+00, -3.1638e-01, -3.5508e-01,  ..., -1.9299e-01,\n",
       "            1.2683e+00,  1.5534e+00],\n",
       "          [-1.8346e-01, -3.7099e-01,  9.5242e-01,  ..., -1.9050e+00,\n",
       "           -2.8939e-01,  1.4149e+00],\n",
       "          [-1.2609e+00, -1.7542e+00,  1.4142e+00,  ..., -8.0278e-01,\n",
       "            2.0950e+00,  2.3007e-01]],\n",
       "\n",
       "         [[ 4.2233e-02, -3.2100e-03,  4.8646e-02,  ..., -3.7903e-02,\n",
       "           -3.2522e-02, -1.6778e-02],\n",
       "          [-1.9821e-01, -3.4345e-01,  3.0177e-01,  ..., -3.6403e-01,\n",
       "           -1.2885e+00,  6.7610e-01],\n",
       "          [ 3.1459e-01,  1.5976e-01, -1.8640e-01,  ..., -6.4390e-02,\n",
       "           -4.5284e-01,  1.0019e+00],\n",
       "          ...,\n",
       "          [-1.6484e+00, -2.1424e-01,  1.0966e+00,  ..., -2.3043e+00,\n",
       "            2.2008e+00,  3.9691e-01],\n",
       "          [ 4.7539e-01,  2.9828e-03,  5.6319e-01,  ...,  1.1498e+00,\n",
       "           -4.9282e-01, -6.1848e-01],\n",
       "          [ 8.2908e-01,  9.4932e-01, -8.1424e-01,  ...,  5.2514e-01,\n",
       "           -5.3877e-01, -7.9519e-01]],\n",
       "\n",
       "         [[ 1.2726e-03,  1.9560e-02, -2.3396e-02,  ..., -3.1588e-03,\n",
       "            1.6212e-02,  5.3618e-02],\n",
       "          [-3.7750e-01, -6.7064e-01,  3.9452e-01,  ..., -2.4047e+00,\n",
       "           -4.0318e-01, -9.2462e-02],\n",
       "          [ 7.8834e-01,  2.3704e-01, -9.3214e-02,  ...,  7.7101e-02,\n",
       "            7.7282e-02, -1.3740e+00],\n",
       "          ...,\n",
       "          [ 1.5139e+00, -1.3822e-01, -3.4416e-01,  ...,  2.5891e-02,\n",
       "            3.4507e+00,  6.0244e-01],\n",
       "          [ 7.7084e-01,  6.9272e-01, -8.4128e-01,  ..., -6.5196e-01,\n",
       "            6.0831e-01,  3.9912e-01],\n",
       "          [ 3.5311e-01,  7.5288e-01,  1.2350e+00,  ..., -8.3482e-01,\n",
       "           -4.7428e-01, -2.7979e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0430e-02,  2.1742e-02, -5.1555e-03,  ..., -4.0646e-02,\n",
       "           -2.9074e-03, -8.0116e-03],\n",
       "          [ 4.8937e-01, -3.8370e-01,  6.0935e-01,  ...,  5.6020e-01,\n",
       "           -2.3270e-01, -2.5386e-01],\n",
       "          [-3.8444e-01, -3.2789e-01,  1.3799e+00,  ...,  7.8286e-01,\n",
       "            1.7210e-01, -2.7965e-01],\n",
       "          ...,\n",
       "          [ 3.8025e-01,  4.7634e-01,  2.4544e-01,  ...,  8.4104e-01,\n",
       "           -5.7421e-01, -4.3692e-02],\n",
       "          [-3.0980e-02,  3.2682e-01, -8.5138e-01,  ...,  6.2111e-01,\n",
       "           -1.4044e+00,  5.7263e-01],\n",
       "          [-5.1747e-01, -1.1967e+00,  1.0964e+00,  ...,  5.2495e-01,\n",
       "           -1.2610e-01,  6.3991e-02]],\n",
       "\n",
       "         [[ 6.3396e-02,  6.3291e-03,  6.7828e-02,  ...,  3.1932e-02,\n",
       "            6.4551e-02,  2.8262e-02],\n",
       "          [ 6.2670e-02, -4.6917e-02,  1.1962e+00,  ..., -1.6094e-01,\n",
       "            8.7439e-01, -6.4468e-01],\n",
       "          [ 8.2092e-01,  5.2710e-01,  2.2139e+00,  ..., -1.3851e+00,\n",
       "            1.7838e+00, -5.3416e-01],\n",
       "          ...,\n",
       "          [-3.0740e+00,  2.3025e+00,  1.3344e+00,  ..., -7.6247e-01,\n",
       "           -1.4115e+00, -1.7854e+00],\n",
       "          [-9.8457e-01,  5.5409e-01, -1.6180e+00,  ..., -2.7694e-01,\n",
       "           -1.0615e-02, -1.7981e+00],\n",
       "          [ 5.2543e-01, -5.1761e-01, -8.3516e-01,  ...,  1.0463e+00,\n",
       "           -1.2069e+00, -3.3951e-01]],\n",
       "\n",
       "         [[-1.1662e-01,  2.3412e-02, -5.0102e-02,  ..., -8.0458e-02,\n",
       "            7.4676e-02, -1.1760e-02],\n",
       "          [-5.7576e-01,  7.8122e-01, -1.0209e-01,  ..., -3.4294e-01,\n",
       "            8.3690e-01,  2.2389e+00],\n",
       "          [-5.0224e-01,  1.6829e-01, -3.9795e-01,  ..., -2.6498e-01,\n",
       "            5.5370e-01,  9.4029e-01],\n",
       "          ...,\n",
       "          [-6.8302e-01,  4.7791e-01,  1.4326e+00,  ..., -1.7572e+00,\n",
       "           -1.7029e-02,  9.9840e-01],\n",
       "          [ 2.2306e+00, -1.6727e-01,  1.9452e-01,  ...,  1.8586e-01,\n",
       "            8.0896e-01,  9.2590e-01],\n",
       "          [ 2.3514e+00, -5.1381e-02, -7.3963e-02,  ..., -3.4165e-01,\n",
       "            2.6597e+00, -1.1923e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4877e-03,  5.4099e-02, -7.6774e-02,  ...,  6.7908e-02,\n",
       "           -3.6519e-02, -8.0231e-02],\n",
       "          [-2.3341e-01, -1.5787e+00, -7.1027e-01,  ...,  1.1150e+00,\n",
       "            1.5362e+00, -2.3433e-01],\n",
       "          [-3.8497e-01, -1.2795e+00,  4.3961e-01,  ...,  2.6797e-01,\n",
       "            8.4862e-01, -1.8365e-01],\n",
       "          ...,\n",
       "          [ 1.0065e+00,  3.6928e-01,  5.7230e-01,  ...,  7.8138e-01,\n",
       "            6.7600e-01,  6.7787e-01],\n",
       "          [-9.7846e-01, -1.6201e+00,  1.3772e+00,  ..., -6.5545e-01,\n",
       "            2.0255e+00,  7.4488e-01],\n",
       "          [ 7.5713e-04, -5.2176e-01,  1.2190e+00,  ..., -3.5120e-01,\n",
       "            9.3000e-01, -1.3925e+00]],\n",
       "\n",
       "         [[ 4.2233e-02, -3.2100e-03,  4.8646e-02,  ..., -3.7903e-02,\n",
       "           -3.2522e-02, -1.6778e-02],\n",
       "          [-1.9821e-01, -3.4345e-01,  3.0177e-01,  ..., -3.6403e-01,\n",
       "           -1.2885e+00,  6.7610e-01],\n",
       "          [ 3.1459e-01,  1.5976e-01, -1.8640e-01,  ..., -6.4390e-02,\n",
       "           -4.5284e-01,  1.0019e+00],\n",
       "          ...,\n",
       "          [ 1.4418e+00, -2.6121e-01, -2.5725e-01,  ...,  3.1675e-01,\n",
       "            3.5144e-02, -8.7184e-01],\n",
       "          [ 1.0171e+00,  1.2022e+00, -4.6932e-01,  ...,  6.8620e-01,\n",
       "           -2.5083e-01, -1.1216e+00],\n",
       "          [-7.9751e-01, -1.8612e-01, -8.7979e-01,  ..., -1.7291e-01,\n",
       "           -1.5152e-01,  1.6455e+00]],\n",
       "\n",
       "         [[ 1.2726e-03,  1.9560e-02, -2.3396e-02,  ..., -3.1588e-03,\n",
       "            1.6212e-02,  5.3618e-02],\n",
       "          [-3.7750e-01, -6.7064e-01,  3.9452e-01,  ..., -2.4047e+00,\n",
       "           -4.0318e-01, -9.2462e-02],\n",
       "          [ 7.8834e-01,  2.3704e-01, -9.3214e-02,  ...,  7.7101e-02,\n",
       "            7.7282e-02, -1.3740e+00],\n",
       "          ...,\n",
       "          [-4.1467e-01,  9.7023e-01, -4.4185e-01,  ..., -1.5527e+00,\n",
       "           -1.0299e+00,  3.7915e-01],\n",
       "          [-4.2250e-02,  7.1219e-01,  6.4902e-01,  ..., -1.0254e+00,\n",
       "           -8.5628e-01, -2.9192e-01],\n",
       "          [ 3.6282e-01, -5.0238e-01,  8.4005e-01,  ..., -1.4246e-01,\n",
       "            2.1491e-01, -3.5875e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0430e-02,  2.1742e-02, -5.1555e-03,  ..., -4.0646e-02,\n",
       "           -2.9074e-03, -8.0116e-03],\n",
       "          [ 4.8937e-01, -3.8370e-01,  6.0935e-01,  ...,  5.6020e-01,\n",
       "           -2.3270e-01, -2.5386e-01],\n",
       "          [-3.8444e-01, -3.2789e-01,  1.3799e+00,  ...,  7.8286e-01,\n",
       "            1.7210e-01, -2.7965e-01],\n",
       "          ...,\n",
       "          [ 4.3237e-01,  3.5220e-01, -8.6530e-01,  ...,  2.4163e-02,\n",
       "           -9.8692e-01,  1.3595e+00],\n",
       "          [-2.8553e-02, -3.3816e-01,  7.4054e-01,  ...,  3.2959e-01,\n",
       "           -3.3161e-01,  1.1163e+00],\n",
       "          [-3.8603e-01, -4.9734e-01,  2.8421e-01,  ..., -1.2859e-01,\n",
       "           -1.6987e+00,  4.2164e-01]],\n",
       "\n",
       "         [[ 6.3396e-02,  6.3291e-03,  6.7828e-02,  ...,  3.1932e-02,\n",
       "            6.4551e-02,  2.8262e-02],\n",
       "          [ 6.2670e-02, -4.6917e-02,  1.1962e+00,  ..., -1.6094e-01,\n",
       "            8.7439e-01, -6.4468e-01],\n",
       "          [ 8.2092e-01,  5.2710e-01,  2.2139e+00,  ..., -1.3851e+00,\n",
       "            1.7838e+00, -5.3416e-01],\n",
       "          ...,\n",
       "          [ 1.4110e+00,  1.2473e+00, -4.1968e-01,  ...,  7.9852e-01,\n",
       "           -1.1925e+00,  6.8649e-01],\n",
       "          [ 5.4633e-01, -4.3328e-01, -5.9339e-01,  ...,  1.3367e+00,\n",
       "           -1.2991e+00, -2.2312e-02],\n",
       "          [-1.9983e-01,  1.6336e-01,  1.5453e+00,  ...,  1.2978e+00,\n",
       "            1.0604e+00, -1.1195e+00]],\n",
       "\n",
       "         [[-1.1662e-01,  2.3412e-02, -5.0102e-02,  ..., -8.0458e-02,\n",
       "            7.4676e-02, -1.1760e-02],\n",
       "          [-5.7576e-01,  7.8122e-01, -1.0209e-01,  ..., -3.4294e-01,\n",
       "            8.3690e-01,  2.2389e+00],\n",
       "          [-5.0224e-01,  1.6829e-01, -3.9795e-01,  ..., -2.6498e-01,\n",
       "            5.5370e-01,  9.4029e-01],\n",
       "          ...,\n",
       "          [ 2.8457e+00, -2.2208e-02,  7.0472e-02,  ..., -1.4563e+00,\n",
       "           -6.7377e-02,  4.3450e-01],\n",
       "          [ 2.5636e+00,  2.7789e-02,  8.5432e-02,  ..., -1.2292e+00,\n",
       "            2.1772e+00,  1.7840e-01],\n",
       "          [ 2.8892e-01,  7.8489e-01,  1.6313e-01,  ..., -1.0292e+00,\n",
       "           -7.3882e-01,  1.0457e-01]]]])), (tensor([[[[-1.7197, -0.3172, -0.2982,  ...,  0.1676,  0.3189, -0.5208],\n",
       "          [ 0.8313,  0.5837, -0.8842,  ...,  1.8109, -1.2280,  0.0325],\n",
       "          [-0.0383,  0.2969, -0.4364,  ...,  0.4677, -0.7024, -0.0465],\n",
       "          ...,\n",
       "          [ 1.5117, -0.1203,  1.4475,  ...,  1.3150, -0.7306,  1.3104],\n",
       "          [ 1.0505,  0.0958,  0.1309,  ...,  1.7367,  0.2209,  0.5450],\n",
       "          [ 0.5500, -0.3470, -0.4454,  ...,  1.4403, -0.5146,  1.1241]],\n",
       "\n",
       "         [[ 0.1013, -0.0728,  2.3048,  ...,  0.2503,  0.0784, -0.2027],\n",
       "          [ 1.4467, -0.6393, -0.4054,  ...,  0.2150,  0.6911, -0.4351],\n",
       "          [ 1.0181, -1.1459, -0.3038,  ..., -0.3688,  0.6666, -0.9077],\n",
       "          ...,\n",
       "          [ 1.3601, -0.2100, -2.2007,  ..., -0.7022, -0.1674, -0.7642],\n",
       "          [ 1.3721, -0.2648, -1.5300,  ...,  0.8636, -1.3255, -0.8760],\n",
       "          [ 1.1819, -2.2939, -1.8768,  ..., -0.4210,  0.8289, -0.9031]],\n",
       "\n",
       "         [[-0.1962,  1.0595,  0.4657,  ..., -0.5344,  0.3287, -0.1048],\n",
       "          [-1.1209,  0.5617,  0.8123,  ...,  0.8569,  0.2792, -0.2355],\n",
       "          [-1.2016,  0.5797,  0.1971,  ...,  1.3612, -0.0305,  0.3392],\n",
       "          ...,\n",
       "          [-1.1783,  0.6144, -0.1989,  ...,  1.8185,  0.5937,  0.9641],\n",
       "          [-2.1482,  0.2402,  0.2422,  ...,  0.7896,  0.7297, -0.7362],\n",
       "          [-0.1856,  1.0242,  0.2502,  ...,  1.1593,  0.7816, -0.5432]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5605,  0.9806, -0.8678,  ..., -0.7295,  0.7008,  0.8668],\n",
       "          [ 1.2346,  0.6443, -0.7010,  ..., -1.8950,  0.7118,  0.3293],\n",
       "          [ 0.3319,  0.0537, -1.2446,  ..., -1.1221, -0.3653, -0.1282],\n",
       "          ...,\n",
       "          [ 1.0596, -0.2896, -0.1792,  ..., -2.0447, -1.1115,  0.6674],\n",
       "          [-0.7869,  1.2681,  0.5696,  ..., -2.4229, -0.2451,  1.0315],\n",
       "          [ 1.1616, -0.2084, -0.9841,  ..., -2.3323,  0.1893, -0.2942]],\n",
       "\n",
       "         [[-0.3996,  0.3726,  0.3744,  ...,  0.7137,  0.0272, -0.0685],\n",
       "          [-0.8139, -0.7286, -0.5300,  ..., -0.0492, -0.6075,  0.1416],\n",
       "          [-0.9272,  0.1227, -0.8550,  ...,  0.9735, -0.1080,  1.0084],\n",
       "          ...,\n",
       "          [-0.3934, -0.6069, -1.2671,  ..., -0.5706, -0.0069,  1.1593],\n",
       "          [-1.2679,  1.6906, -0.5159,  ..., -0.5153, -1.1886,  0.3016],\n",
       "          [-0.5033,  0.8864, -0.6102,  ..., -0.0966, -0.2563, -0.7410]],\n",
       "\n",
       "         [[-0.7251, -0.0239,  0.4534,  ..., -0.0827,  0.0438, -0.0685],\n",
       "          [ 0.3537, -0.7039,  2.1484,  ...,  0.0240, -1.5127, -0.1816],\n",
       "          [ 0.7433, -0.6278,  0.4807,  ..., -0.5895, -0.5164, -0.7777],\n",
       "          ...,\n",
       "          [ 0.6103, -0.8781,  1.3461,  ...,  0.2048, -2.0428, -0.3102],\n",
       "          [-0.3595, -0.8678,  1.4317,  ..., -1.7012, -0.1965, -1.3894],\n",
       "          [-0.7196, -0.0061,  1.2842,  ..., -0.4804,  0.0540, -0.3639]]],\n",
       "\n",
       "\n",
       "        [[[-1.7197, -0.3172, -0.2982,  ...,  0.1676,  0.3189, -0.5208],\n",
       "          [ 0.8313,  0.5837, -0.8842,  ...,  1.8109, -1.2280,  0.0325],\n",
       "          [-0.0383,  0.2969, -0.4364,  ...,  0.4677, -0.7024, -0.0465],\n",
       "          ...,\n",
       "          [ 1.7713,  0.0448,  0.3024,  ...,  2.0948,  0.1359,  0.9988],\n",
       "          [ 0.2739, -0.2460, -0.1909,  ...,  1.8301, -0.4877,  0.6923],\n",
       "          [ 0.9827, -0.0362, -0.1246,  ...,  0.6512, -1.7606, -0.6017]],\n",
       "\n",
       "         [[ 0.1013, -0.0728,  2.3048,  ...,  0.2503,  0.0784, -0.2027],\n",
       "          [ 1.4467, -0.6393, -0.4054,  ...,  0.2150,  0.6911, -0.4351],\n",
       "          [ 1.0181, -1.1459, -0.3038,  ..., -0.3688,  0.6666, -0.9077],\n",
       "          ...,\n",
       "          [ 0.8034, -0.9765, -1.4349,  ...,  0.4036, -1.0253, -0.4239],\n",
       "          [ 1.1715, -2.2261, -1.6460,  ...,  0.3606,  0.2365, -0.5361],\n",
       "          [ 0.6917, -0.9826, -1.0048,  ...,  0.2933,  0.5807, -0.0887]],\n",
       "\n",
       "         [[-0.1962,  1.0595,  0.4657,  ..., -0.5344,  0.3287, -0.1048],\n",
       "          [-1.1209,  0.5617,  0.8123,  ...,  0.8569,  0.2792, -0.2355],\n",
       "          [-1.2016,  0.5797,  0.1971,  ...,  1.3612, -0.0305,  0.3392],\n",
       "          ...,\n",
       "          [-2.5254,  0.2889, -0.8647,  ...,  2.1103, -0.1837,  0.4205],\n",
       "          [-0.5922,  0.4252, -0.4759,  ...,  1.0327,  0.6309, -0.0902],\n",
       "          [-1.1970,  1.1718, -0.5245,  ...,  1.1210,  0.9980, -0.4170]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5605,  0.9806, -0.8678,  ..., -0.7295,  0.7008,  0.8668],\n",
       "          [ 1.2346,  0.6443, -0.7010,  ..., -1.8950,  0.7118,  0.3293],\n",
       "          [ 0.3319,  0.0537, -1.2446,  ..., -1.1221, -0.3653, -0.1282],\n",
       "          ...,\n",
       "          [ 0.3728,  0.1550,  0.8993,  ..., -2.3930, -0.4281,  2.3415],\n",
       "          [ 1.1050,  0.3585, -0.8204,  ..., -2.1995, -0.3512,  0.1166],\n",
       "          [ 0.0679,  0.2459, -0.7149,  ..., -1.2675, -0.8314,  0.7616]],\n",
       "\n",
       "         [[-0.3996,  0.3726,  0.3744,  ...,  0.7137,  0.0272, -0.0685],\n",
       "          [-0.8139, -0.7286, -0.5300,  ..., -0.0492, -0.6075,  0.1416],\n",
       "          [-0.9272,  0.1227, -0.8550,  ...,  0.9735, -0.1080,  1.0084],\n",
       "          ...,\n",
       "          [ 0.8984,  0.2794,  1.3908,  ...,  1.6326, -0.7893,  0.0865],\n",
       "          [ 0.1226,  1.0486, -0.4724,  ...,  0.6937,  0.0671, -0.6165],\n",
       "          [-0.1770,  1.1722, -1.1036,  ..., -0.5695, -0.1889, -0.7764]],\n",
       "\n",
       "         [[-0.7251, -0.0239,  0.4534,  ..., -0.0827,  0.0438, -0.0685],\n",
       "          [ 0.3537, -0.7039,  2.1484,  ...,  0.0240, -1.5127, -0.1816],\n",
       "          [ 0.7433, -0.6278,  0.4807,  ..., -0.5895, -0.5164, -0.7777],\n",
       "          ...,\n",
       "          [-0.4031, -1.3088,  1.4730,  ..., -0.3691, -1.1987, -1.0612],\n",
       "          [-0.6611, -0.1354,  1.1895,  ..., -0.6464,  0.1038, -0.4723],\n",
       "          [-0.2698, -1.3207,  2.5769,  ...,  0.4837,  0.8374,  0.7836]]]]), tensor([[[[ 5.3307e-02, -1.3870e-01, -1.8239e-01,  ..., -2.8280e-01,\n",
       "            2.4972e-01, -1.3259e-01],\n",
       "          [-3.0732e-01, -6.8941e-01,  3.8066e-01,  ...,  1.1015e+00,\n",
       "           -1.5945e+00,  1.5051e+00],\n",
       "          [ 1.4721e+00, -4.0715e-01, -1.8804e-01,  ...,  2.1422e+00,\n",
       "           -2.2970e+00,  1.4606e+00],\n",
       "          ...,\n",
       "          [-1.3774e+00,  2.7431e-01,  3.1597e-01,  ..., -8.0166e-01,\n",
       "           -1.9228e+00,  2.7449e+00],\n",
       "          [-8.6936e-01,  4.1157e-02,  1.6834e+00,  ...,  1.5008e+00,\n",
       "           -1.8652e+00,  1.2752e+00],\n",
       "          [-4.0954e-01, -9.8570e-01,  1.1791e+00,  ...,  7.6007e-01,\n",
       "            3.4196e-01,  4.7368e-01]],\n",
       "\n",
       "         [[ 9.3445e-02, -3.2838e-02,  2.1770e-02,  ..., -2.8990e-02,\n",
       "           -1.2303e-01,  1.7575e-01],\n",
       "          [-9.9773e-01,  2.9351e-02, -4.9488e-01,  ...,  1.7975e+00,\n",
       "            4.0943e-01, -7.2222e-01],\n",
       "          [-1.7551e+00, -3.4705e-01,  9.3335e-02,  ...,  4.1428e-01,\n",
       "            1.3474e+00, -1.2023e+00],\n",
       "          ...,\n",
       "          [-2.2930e+00, -7.5587e-01, -2.1194e+00,  ...,  1.2106e+00,\n",
       "           -1.2598e+00,  1.4764e+00],\n",
       "          [-1.0188e+00, -8.6036e-01, -1.0219e+00,  ...,  6.0533e-01,\n",
       "            7.7538e-01,  8.2124e-01],\n",
       "          [-4.9284e-01,  5.9002e-01, -1.0873e+00,  ..., -8.7858e-01,\n",
       "           -2.7505e-02, -2.4797e+00]],\n",
       "\n",
       "         [[ 2.5387e-03,  5.0171e-02, -4.1852e-02,  ...,  8.5355e-03,\n",
       "            2.1758e-02,  5.9575e-02],\n",
       "          [-4.4277e-01,  1.5427e-01,  8.1839e-01,  ..., -3.0281e-01,\n",
       "           -1.3338e-01,  3.9485e-02],\n",
       "          [-8.6464e-02,  7.8931e-01, -5.0545e-01,  ..., -4.2838e-01,\n",
       "           -5.6663e-01,  5.7063e-01],\n",
       "          ...,\n",
       "          [-9.1658e-01,  2.1173e+00,  2.8726e+00,  ..., -2.0357e-02,\n",
       "            2.5085e+00,  2.2784e+00],\n",
       "          [-9.7302e-01, -7.8470e-01,  1.6288e+00,  ...,  6.7887e-01,\n",
       "            6.7672e-01,  6.3044e-01],\n",
       "          [-4.8441e-01, -1.4953e-01, -5.2694e-02,  ..., -4.4231e-01,\n",
       "            1.4669e-01,  5.3709e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0588e-02, -2.4530e-02,  9.7473e-02,  ...,  5.2948e-02,\n",
       "            3.3507e-03,  2.4940e-02],\n",
       "          [-2.9148e-01,  3.8747e-01,  5.7529e-02,  ...,  1.2992e+00,\n",
       "            1.7059e-01,  1.6713e+00],\n",
       "          [ 7.2824e-02,  1.6716e+00, -1.5746e-01,  ...,  9.7559e-03,\n",
       "           -1.7362e-01,  2.2366e-01],\n",
       "          ...,\n",
       "          [ 7.0916e-01, -1.4625e-02,  7.4598e-01,  ..., -1.6705e+00,\n",
       "           -1.2385e+00,  2.1544e+00],\n",
       "          [-2.0015e-01,  1.1510e+00, -7.4652e-01,  ..., -5.9240e-02,\n",
       "           -7.8431e-01,  2.4275e+00],\n",
       "          [-7.7298e-02, -1.4568e+00, -8.9433e-01,  ..., -1.1782e+00,\n",
       "           -7.9351e-02,  4.0793e-01]],\n",
       "\n",
       "         [[-1.9021e-01, -5.6177e-02,  7.9363e-02,  ..., -5.2859e-02,\n",
       "            5.0050e-02, -9.7191e-02],\n",
       "          [-1.7501e-01, -1.9604e+00, -4.4796e-01,  ..., -1.3041e+00,\n",
       "            1.9128e-01,  1.3637e+00],\n",
       "          [-3.2884e-01, -1.1144e-02, -1.4260e-01,  ..., -1.5014e+00,\n",
       "           -1.1554e-01,  6.8806e-01],\n",
       "          ...,\n",
       "          [-1.7338e+00, -7.7367e-01, -2.2717e+00,  ..., -1.0501e+00,\n",
       "            1.5728e+00,  4.9123e-01],\n",
       "          [-6.8415e-01,  4.7328e-01, -7.5791e-02,  ..., -1.1961e+00,\n",
       "            1.8457e+00,  1.7648e+00],\n",
       "          [-9.4137e-01,  1.5187e+00, -1.2870e-01,  ..., -9.2731e-01,\n",
       "            1.1312e+00, -6.7441e-01]],\n",
       "\n",
       "         [[ 1.0597e-01, -1.2691e-01,  1.4986e-01,  ..., -1.2794e-01,\n",
       "           -3.6332e-03, -1.7525e-01],\n",
       "          [-5.7885e-02,  7.3887e-01, -1.5973e+00,  ...,  3.3754e-01,\n",
       "            4.4965e-01,  1.4482e-01],\n",
       "          [-6.8314e-01,  3.3686e-02, -8.7468e-01,  ...,  1.0161e+00,\n",
       "            1.3054e+00, -6.3579e-01],\n",
       "          ...,\n",
       "          [-3.0872e+00,  1.9579e+00, -1.3831e+00,  ...,  2.4112e+00,\n",
       "           -2.7882e+00, -5.6751e-01],\n",
       "          [-9.0665e-01, -6.1307e-01, -5.3815e-01,  ..., -1.5097e+00,\n",
       "           -1.4270e+00, -6.9970e-02],\n",
       "          [-4.7578e-01, -1.4416e+00, -1.4733e+00,  ...,  3.6007e-01,\n",
       "            1.3218e+00,  9.1428e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.3307e-02, -1.3870e-01, -1.8239e-01,  ..., -2.8280e-01,\n",
       "            2.4972e-01, -1.3259e-01],\n",
       "          [-3.0732e-01, -6.8941e-01,  3.8066e-01,  ...,  1.1015e+00,\n",
       "           -1.5945e+00,  1.5051e+00],\n",
       "          [ 1.4721e+00, -4.0715e-01, -1.8804e-01,  ...,  2.1422e+00,\n",
       "           -2.2970e+00,  1.4606e+00],\n",
       "          ...,\n",
       "          [-2.6525e-01,  8.3245e-01,  1.6951e+00,  ...,  1.9200e-01,\n",
       "           -2.1441e+00, -2.4237e-01],\n",
       "          [-8.9089e-02, -7.7707e-01,  1.6501e+00,  ...,  8.2217e-01,\n",
       "            3.2094e-01,  1.6440e-01],\n",
       "          [-1.5775e+00,  5.7529e-01, -7.4172e-02,  ...,  8.9408e-01,\n",
       "           -1.4582e+00,  1.9706e+00]],\n",
       "\n",
       "         [[ 9.3445e-02, -3.2838e-02,  2.1770e-02,  ..., -2.8990e-02,\n",
       "           -1.2303e-01,  1.7575e-01],\n",
       "          [-9.9773e-01,  2.9351e-02, -4.9488e-01,  ...,  1.7975e+00,\n",
       "            4.0943e-01, -7.2222e-01],\n",
       "          [-1.7551e+00, -3.4705e-01,  9.3335e-02,  ...,  4.1428e-01,\n",
       "            1.3474e+00, -1.2023e+00],\n",
       "          ...,\n",
       "          [-6.1971e-01, -3.7463e-01,  1.2174e+00,  ...,  1.4259e+00,\n",
       "            6.6122e-01,  2.3482e-01],\n",
       "          [-6.6640e-01,  5.8230e-01, -1.2729e+00,  ..., -7.1409e-01,\n",
       "            1.8421e-01, -2.2553e+00],\n",
       "          [ 5.3336e-01,  8.4854e-01, -2.2351e-01,  ...,  3.6770e-01,\n",
       "            1.6320e-01,  3.7293e-01]],\n",
       "\n",
       "         [[ 2.5387e-03,  5.0171e-02, -4.1852e-02,  ...,  8.5355e-03,\n",
       "            2.1758e-02,  5.9575e-02],\n",
       "          [-4.4277e-01,  1.5427e-01,  8.1839e-01,  ..., -3.0281e-01,\n",
       "           -1.3338e-01,  3.9485e-02],\n",
       "          [-8.6464e-02,  7.8931e-01, -5.0545e-01,  ..., -4.2838e-01,\n",
       "           -5.6663e-01,  5.7063e-01],\n",
       "          ...,\n",
       "          [-1.5891e+00, -7.2162e-01,  3.8507e-01,  ..., -9.4434e-01,\n",
       "           -1.1034e+00,  4.5148e-01],\n",
       "          [-4.6808e-01, -5.8115e-02, -3.3405e-01,  ..., -6.0903e-01,\n",
       "            2.4462e-01, -1.4277e-02],\n",
       "          [ 1.0225e+00, -6.8323e-02, -9.1117e-01,  ..., -4.3332e-01,\n",
       "            1.5608e+00,  1.8888e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0588e-02, -2.4530e-02,  9.7473e-02,  ...,  5.2948e-02,\n",
       "            3.3507e-03,  2.4940e-02],\n",
       "          [-2.9148e-01,  3.8747e-01,  5.7529e-02,  ...,  1.2992e+00,\n",
       "            1.7059e-01,  1.6713e+00],\n",
       "          [ 7.2824e-02,  1.6716e+00, -1.5746e-01,  ...,  9.7559e-03,\n",
       "           -1.7362e-01,  2.2366e-01],\n",
       "          ...,\n",
       "          [ 3.0997e+00, -8.6603e-01, -2.3600e-01,  ..., -1.5881e+00,\n",
       "            9.3589e-01, -5.6955e-01],\n",
       "          [ 3.3597e-01, -1.4718e+00, -5.6765e-01,  ..., -1.2353e+00,\n",
       "            3.9987e-01, -2.8491e-01],\n",
       "          [-2.9821e-01, -2.4276e-01,  4.7431e-02,  ...,  6.7291e-02,\n",
       "            2.2286e-01,  2.9149e-01]],\n",
       "\n",
       "         [[-1.9021e-01, -5.6177e-02,  7.9363e-02,  ..., -5.2859e-02,\n",
       "            5.0050e-02, -9.7191e-02],\n",
       "          [-1.7501e-01, -1.9604e+00, -4.4796e-01,  ..., -1.3041e+00,\n",
       "            1.9128e-01,  1.3637e+00],\n",
       "          [-3.2884e-01, -1.1144e-02, -1.4260e-01,  ..., -1.5014e+00,\n",
       "           -1.1554e-01,  6.8806e-01],\n",
       "          ...,\n",
       "          [ 1.3997e+00, -1.0940e-01,  1.0504e-02,  ..., -1.9737e+00,\n",
       "            1.9656e+00,  1.7486e+00],\n",
       "          [-1.1777e+00,  1.3575e+00,  1.5207e-01,  ..., -5.3274e-01,\n",
       "            1.2082e+00, -5.0386e-01],\n",
       "          [-1.4976e+00,  9.9867e-02,  5.1457e-01,  ..., -3.8642e-01,\n",
       "            1.9347e-01, -7.6948e-01]],\n",
       "\n",
       "         [[ 1.0597e-01, -1.2691e-01,  1.4986e-01,  ..., -1.2794e-01,\n",
       "           -3.6332e-03, -1.7525e-01],\n",
       "          [-5.7885e-02,  7.3887e-01, -1.5973e+00,  ...,  3.3754e-01,\n",
       "            4.4965e-01,  1.4482e-01],\n",
       "          [-6.8314e-01,  3.3686e-02, -8.7468e-01,  ...,  1.0161e+00,\n",
       "            1.3054e+00, -6.3579e-01],\n",
       "          ...,\n",
       "          [ 1.2763e+00, -1.9526e+00,  2.2545e-01,  ..., -2.2670e+00,\n",
       "            1.8127e-01,  2.3320e-01],\n",
       "          [-7.4166e-01, -1.2194e+00, -1.5792e+00,  ..., -3.6882e-01,\n",
       "            1.4959e+00,  1.3978e+00],\n",
       "          [-3.4169e-01,  1.5963e+00, -9.8543e-01,  ...,  1.4118e+00,\n",
       "            7.6314e-01,  1.0879e+00]]]]))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # dummy - rand tensor\n",
    "# batch_size = 8\n",
    "# text_len = 25\n",
    "# texts = torch.randint(1, 300, size=(batch_size, text_len))\n",
    "# texts = texts\n",
    "# attention_mask = torch.ones(batch_size, text_len)\n",
    "# text_batch = {\n",
    "#     'input_ids': texts,\n",
    "#     'attention_mask': attention_mask,\n",
    "# }\n",
    "\n",
    "# text_embeddings = base_model_text(text_batch['input_ids'])\n",
    "# text_embeddings\n",
    "\n",
    "# text - tokenized tensor\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '!'})\n",
    "seq_max_len = 200\n",
    "sample_text_lst = [\n",
    "                 'this sound is of the bladed weapon',\n",
    "                 'this sound is of the blunt weapon',\n",
    "                 ]\n",
    "encoded_captions = tokenizer(\n",
    "    list(sample_text_lst), padding=True, truncation=True, max_length=seq_max_len, return_tensors='pt',\n",
    ")\n",
    "\n",
    "print(encoded_captions)\n",
    "\n",
    "text_embeddings = base_model_text(encoded_captions['input_ids'])\n",
    "text_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on GPT2TokenizerFast in module transformers.models.gpt2.tokenization_gpt2_fast object:\n",
      "\n",
      "class GPT2TokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
      " |  GPT2TokenizerFast(vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |  \n",
      " |  Construct a \"fast\" GPT-2 tokenizer (backed by HuggingFace's *tokenizers* library). Based on byte-level\n",
      " |  Byte-Pair-Encoding.\n",
      " |  \n",
      " |  This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
      " |  be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
      " |  \n",
      " |  ```python\n",
      " |  >>> from transformers import GPT2TokenizerFast\n",
      " |  \n",
      " |  >>> tokenizer = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n",
      " |  >>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
      " |  [15496, 995]\n",
      " |  \n",
      " |  >>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
      " |  [18435, 995]\n",
      " |  ```\n",
      " |  \n",
      " |  You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer, but since\n",
      " |  the model was not pretrained this way, it might yield a decrease in performance.\n",
      " |  \n",
      " |  <Tip>\n",
      " |  \n",
      " |  When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n",
      " |  \n",
      " |  </Tip>\n",
      " |  \n",
      " |  This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      " |  refer to this superclass for more information regarding those methods.\n",
      " |  \n",
      " |  Args:\n",
      " |      vocab_file (`str`, *optional*):\n",
      " |          Path to the vocabulary file.\n",
      " |      merges_file (`str`, *optional*):\n",
      " |          Path to the merges file.\n",
      " |      tokenizer_file (`str`, *optional*):\n",
      " |          Path to [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n",
      " |          contains everything needed to load the tokenizer.\n",
      " |      unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |          The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |          token instead.\n",
      " |      bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |          The beginning of sequence token.\n",
      " |      eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |          The end of sequence token.\n",
      " |      add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
      " |          other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GPT2TokenizerFast\n",
      " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file=None, merges_file=None, tokenizer_file=None, unk_token='<|endoftext|>', bos_token='<|endoftext|>', eos_token='<|endoftext|>', add_prefix_space=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'attention_mask']\n",
      " |  \n",
      " |  slow_tokenizer_class = <class 'transformers.models.gpt2.tokenization_g...\n",
      " |      Construct a GPT-2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
      " |      \n",
      " |      This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
      " |      be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from transformers import GPT2Tokenizer\n",
      " |      \n",
      " |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      " |      >>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
      " |      [15496, 995]\n",
      " |      \n",
      " |      >>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
      " |      [18435, 995]\n",
      " |      ```\n",
      " |      \n",
      " |      You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
      " |      call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      When used with `is_split_into_words=True`, this tokenizer will add a space before each word (even the first one).\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to\n",
      " |      this superclass for more information regarding those methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_file (`str`):\n",
      " |              Path to the vocabulary file.\n",
      " |          merges_file (`str`):\n",
      " |              Path to the merges file.\n",
      " |          errors (`str`, *optional*, defaults to `\"replace\"`):\n",
      " |              Paradigm to follow when decoding bytes to UTF-8. See\n",
      " |              [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
      " |          unk_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |              The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      " |              token instead.\n",
      " |          bos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |              The beginning of sequence token.\n",
      " |          eos_token (`str`, *optional*, defaults to `\"<|endoftext|>\"`):\n",
      " |              The end of sequence token.\n",
      " |          pad_token (`str`, *optional*):\n",
      " |              The token used for padding, for example when batching sequences of different lengths.\n",
      " |          add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
      " |              other word. (GPT2 tokenizer detect beginning of words by the preceding space).\n",
      " |          add_bos_token (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add an initial beginning of sentence token to the input. This allows to treat the leading\n",
      " |              word just as any other word.\n",
      " |  \n",
      " |  \n",
      " |  vocab_files_names = {'merges_file': 'merges.txt', 'tokenizer_file': 't...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`int` or `List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or `List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, Iterable[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a Iterable of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`str` or `Iterable[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int` or `List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
      " |      often want to remove sub-word tokenization artifacts at the same time.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`List[str]`): The token to join in a string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
      " |      vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
      " |      this inside your training loop.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int], padding_side: Optional[bool])\n",
      " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
      " |      library) and restore the tokenizer settings afterwards.\n",
      " |      \n",
      " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
      " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
      " |      section.\n",
      " |      \n",
      " |      Args:\n",
      " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
      " |              The kind of padding that will be applied to the input\n",
      " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
      " |              The kind of truncation that will be applied to the input\n",
      " |          max_length (`int`):\n",
      " |              The maximum size of a sequence.\n",
      " |          stride (`int`):\n",
      " |              The stride to use when handling overflow.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |  \n",
      " |  tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
      " |      Converts a string into a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          pair (`str`, *optional*):\n",
      " |              A second sequence to be encoded with the first.\n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add the special tokens associated with the corresponding model.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific encode method. See details in\n",
      " |              [`~PreTrainedTokenizerBase.__call__`]\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of tokens.\n",
      " |  \n",
      " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
      " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
      " |      as the current one.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_iterator (generator of `List[str]`):\n",
      " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
      " |              if you have everything in memory.\n",
      " |          vocab_size (`int`):\n",
      " |              The size of the vocabulary you want for your tokenizer.\n",
      " |          length (`int`, *optional*):\n",
      " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
      " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
      " |              A list of new special tokens to add to the tokenizer you are training.\n",
      " |          special_tokens_map (`Dict[str, str]`, *optional*):\n",
      " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
      " |              token name to new special token name in this argument.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
      " |          `text_iterator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  added_tokens_decoder\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  added_tokens_encoder\n",
      " |      Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\n",
      " |      optimisation in `self._added_tokens_encoder` for the slow tokenizers.\n",
      " |  \n",
      " |  backend_tokenizer\n",
      " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
      " |  \n",
      " |  can_save_slow_tokenizer\n",
      " |      `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this\n",
      " |      can only be `True` if the original `\"sentencepiece.model\"` was not deleted.\n",
      " |  \n",
      " |  decoder\n",
      " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  vocab_size\n",
      " |      `int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply_chat_template(self, conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], tools: Optional[List[Union[Dict, Callable]]] = None, documents: Optional[List[Dict[str, str]]] = None, chat_template: Optional[str] = None, add_generation_prompt: bool = False, continue_final_message: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_dict: bool = False, return_assistant_tokens_mask: bool = False, tokenizer_kwargs: Optional[Dict[str, Any]] = None, **kwargs) -> Union[str, List[int], List[str], List[List[int]], transformers.tokenization_utils_base.BatchEncoding]\n",
      " |      Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
      " |      ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
      " |      determine the format and control tokens to use when converting.\n",
      " |      \n",
      " |      Args:\n",
      " |          conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts\n",
      " |              with \"role\" and \"content\" keys, representing the chat history so far.\n",
      " |          tools (`List[Dict]`, *optional*):\n",
      " |              A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
      " |              support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
      " |              giving the name, description and argument types for the tool. See our\n",
      " |              [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
      " |              for more information.\n",
      " |          documents (`List[Dict[str, str]]`, *optional*):\n",
      " |              A list of dicts representing documents that will be accessible to the model if it is performing RAG\n",
      " |              (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n",
      " |              effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys. Please\n",
      " |              see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)\n",
      " |              for examples of passing documents with chat templates.\n",
      " |          chat_template (`str`, *optional*):\n",
      " |              A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n",
      " |              argument, as the model's template will be used by default.\n",
      " |          add_generation_prompt (bool, *optional*):\n",
      " |              If this is set, a prompt with the token(s) that indicate\n",
      " |              the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n",
      " |              Note that this argument will be passed to the chat template, and so it must be supported in the\n",
      " |              template for this argument to have any effect.\n",
      " |          continue_final_message (bool, *optional*):\n",
      " |              If this is set, the chat will be formatted so that the final\n",
      " |              message in the chat is open-ended, without any EOS tokens. The model will continue this message\n",
      " |              rather than starting a new one. This allows you to \"prefill\" part of\n",
      " |              the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n",
      " |          tokenize (`bool`, defaults to `True`):\n",
      " |              Whether to tokenize the output. If `False`, the output will be a string.\n",
      " |          padding (`bool`, defaults to `False`):\n",
      " |              Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
      " |          truncation (`bool`, defaults to `False`):\n",
      " |              Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
      " |              not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
      " |              values are:\n",
      " |              - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return NumPy `np.ndarray` objects.\n",
      " |              - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
      " |          return_dict (`bool`, defaults to `False`):\n",
      " |              Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
      " |          tokenizer_kwargs (`Dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\n",
      " |          return_assistant_tokens_mask (`bool`, defaults to `False`):\n",
      " |              Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n",
      " |              the mask will contain 1. For user and system tokens, the mask will contain 0.\n",
      " |              This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n",
      " |          **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Union[List[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
      " |          output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\n",
      " |          set, will return a dict of tokenizer outputs instead.\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
      " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      " |              `self.clean_up_tokenization_spaces`.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, split_special_tokens: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in `encode_plus`).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens.\n",
      " |      \n",
      " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The model input with special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed. [What are token type\n",
      " |      IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |      Should be overridden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The token type ids.\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
      " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      " |              `self.clean_up_tokenization_spaces`.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, padding_side: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or (for non-fast tokenizers) `List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  get_chat_template(self, chat_template: Optional[str] = None, tools: Optional[List[Dict]] = None) -> str\n",
      " |      Retrieve the chat template string used for tokenizing chat messages. This template is used\n",
      " |      internally by the `apply_chat_template` method and can also be used externally to retrieve the model's chat\n",
      " |      template for better generation tracking.\n",
      " |      \n",
      " |      Args:\n",
      " |          chat_template (`str`, *optional*):\n",
      " |              A Jinja template or the name of a template to use for this conversion.\n",
      " |              It is usually not necessary to pass anything to this argument,\n",
      " |              as the model's template will be used by default.\n",
      " |          tools (`List[Dict]`, *optional*):\n",
      " |              A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
      " |              support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
      " |              giving the name, description and argument types for the tool. See our\n",
      " |              [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
      " |              for more information.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The chat template string.\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
      " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
      " |      \n",
      " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
      " |      text followed by a call to the `pad` method to get a padded encoding.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
      " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
      " |              tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
      " |              List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
      " |              collate function.\n",
      " |      \n",
      " |              Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
      " |              the note above for the return type.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Optional[List[int]] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, padding_side: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
      " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          padding_side (`str`, *optional*):\n",
      " |              The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      " |              Default value is picked from the class attribute of the same name.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (`list`, *optional*):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
      " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
      " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (`int`, *optional*):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to `None`, this will use the max_length value.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to `self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the tokenizer files to the 🤗 Model Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`List[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoTokenizer\n",
      " |      \n",
      " |      tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
      " |      \n",
      " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
      " |      \n",
      " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
      " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (`bool`, *optional*):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
      " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
      " |              loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
      " |              error is raised.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of `str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Optional[List[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `'longest_first'`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
      " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
      " |                batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
      " |                than the model maximum admissible input size).\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
      " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
      " |          of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  convert_added_tokens(obj: Union[tokenizers.AddedToken, Any], save=False, add_type_field=True)\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', trust_remote_code=False, **kwargs)\n",
      " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
      " |      tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
      " |                `./my_model_directory/`.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                `./my_model_directory/vocab.txt`.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download:\n",
      " |              Deprecated and ignored. All downloads are now resumed by default when possible.\n",
      " |              Will be removed in v5 of Transformers.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__` method.\n",
      " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      " |              should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      " |              execute code present on the Hub on your local machine.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
      " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
      " |      # Download vocabulary from huggingface.co and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |      \n",
      " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
      " |      \n",
      " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
      " |      \n",
      " |      # You can link tokens to special vocabulary when instantiating\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", unk_token=\"<unk>\")\n",
      " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |      assert tokenizer.unk_token == \"<unk>\"\n",
      " |      ```\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoTokenizer')\n",
      " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
      " |      library are already mapped with `AutoTokenizer`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
      " |              The auto class to register this new tokenizer with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  padding_side = 'right'\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {}\n",
      " |  \n",
      " |  truncation_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __getattr__(self, key)\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]], replace_additional_special_tokens=True) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the\n",
      " |      model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens can be skipped when decoding using `skip_special_tokens = True`.\n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`.\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
      " |      `'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
      " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the `unk_token` to them).\n",
      " |          replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
      " |              If `True`, the existing list of additional special tokens will be replaced by the list provided in\n",
      " |              `special_tokens_dict`. Otherwise, `self._special_tokens_map[\"additional_special_tokens\"]` is just extended. In the former\n",
      " |              case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged\n",
      " |              as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the\n",
      " |              `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous\n",
      " |              `additional_special_tokens` are still added tokens, and will not be split by the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to add a new classification token to GPT-2\n",
      " |      tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      " |      model = GPT2Model.from_pretrained(\"openai-community/gpt2\")\n",
      " |      \n",
      " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |      assert tokenizer.cls_token == \"<CLS>\"\n",
      " |      ```\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary and will be isolated before the tokenization\n",
      " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
      " |      not treated in the same way.\n",
      " |      \n",
      " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
      " |      of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
      " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
      " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
      " |              strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |      tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |      model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      ```\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      The `sanitize_special_tokens` is now deprecated kept for backward compatibility and will be removed in\n",
      " |      transformers v5.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\n",
      " |      \n",
      " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.), the order has\n",
      " |      nothing to do with the index of each tokens. If you want to know the correct indices, check\n",
      " |      `self.added_tokens_encoder`. We can't create an order anymore as the keys are `AddedTokens` and not `Strings`.\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      `int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
      " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
      " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5661, 2128, 318, 286, 262, 698, 5286, 4282], [5661, 2128, 318, 286, 262, 19861, 4282, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-826c0df11302>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcaption_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclap_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mcaption_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/Practice/DeepLearning/T2A/CLAP/MSCLAP/msclap/models/clap.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m'gpt'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (batch_size=4, seq_len, 768)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0msequence_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# tensor([13, 14, 18, 17])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# sample_tensor = [\n",
    "#                  'this sound is of the bladed weapon',\n",
    "#                  'this sound is of the blunt weapon',\n",
    "#                  ]\n",
    "batch_size = 8\n",
    "text_len = 25\n",
    "texts = torch.randint(1, 300, size=(batch_size, text_len))\n",
    "texts = texts.to(0)\n",
    "attention_mask = torch.ones(batch_size, text_len)\n",
    "text_batch = {\n",
    "    'input_ids': texts,\n",
    "    'attention_mask': attention_mask,\n",
    "}\n",
    "\n",
    "caption_embed = clap_model.clap.caption_encoder(text_batch)\n",
    "caption_embed.shape\n",
    "\n",
    "# print(clap_model.clap.caption_encoder(sample_tensor)[0].shape) # (B, embed_size)\n",
    "# print(clap_model.clap.caption_encoder(sample_tensor)[1].shape) # (B, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6d5ec9f30e77>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_sample_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "text_sample_tensor['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) audio encoder config\n",
    "\n",
    "- enable repeat mode = True로 하면 알아서 반복 채워넣나? -> 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.48.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## config\n",
    "# audio encoder config\n",
    "audio_encoder_config = clap_model.clap.audio_encoder.base.htsat.config\n",
    "audio_encoder_config.enable_repeat_mode\n",
    "# help(audio_encoder_config)\n",
    "\n",
    "# caption encoder config\n",
    "caption_encoder_config = clap_model.clap.caption_encoder.base.config\n",
    "caption_encoder_config\n",
    "# caption_encoder_config.enable_repeat_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
